Title,Introduction,Methods,Results,Discussion,Conclusion
"Chemically enhanced bioremediation of 1,2-cis-dichloroethylene in a limestone aquifer","Chemically Enhanced Biodegradation of
cisâ€1,2 Dichloroethylene in a Limestone
Aquifer
Remediation using iron sulphide and microbes
through batch tests
Sithara Dhinethi Weeratunga
School of Engineering and Design
Department of Civil and Environmental Engineering
Master of Sciences in Environmental Engineering
Kongens Lyngby , June 2025

Chemically Enhanced Biodegradation of
cisâ€1,2 Dichloroethylene in a Limestone
Aquifer
Remediation using iron sulphide and microbes
through batch tests
Sithara Dhinethi Weeratunga
Supervisor: Mette Martina Broholm
Associate Professor, T echnical University of Denmark
Supervisor: Thomas Baumann
Professor, T echnical University of Munich
Coâ€supervisors (DTU): Annika Sidelmann FjordbÃ¸ge
Senior Researcher
Coâ€supervisors (DTU): Cecilie Fisker Ottosen
Assistant Professor
School of Engineering and Design
Department of Civil and Environmental Engineering
Master of Sciences in Environmental Engineering
Master's Thesis
Kongens Lyngby , June 2025

This page intentionally left blank.","1. Introduction 3
BiRD, and ERD do not require excavation and backfill or trenching to be put into the sub-
surface and can be injected, a significant advantage for remediating deep limestone aquifers
such as the one at the Naverland site [ Vidic, 2001 ; Thiruvenkatachari et al., 2008 ]. The dis-
tinction lies in their delivery mechanisms: while micro-scale ZVI (mZVI) and nano-scale
ZVI (nZVI) are particle injections, BiRD and ERD are injected as dissolved compounds,
which could affect the distribution .

2
OBJECTIVES
The overall purpose of this thesis is to provide specific knowledge on the potential for bio-
geochemical remediation of chlorinated solvents in chalk aquifers using the plume origi-
nating from Naverland 26AB, Albertslund by:
â€¢ Assessing the feasibility of using iron-bearing minerals to remediate limestone aquifers
contaminated with cis-1,2-dichloroethylene cDCE.
â€¢ Assessing the feasibility of using ERD to remediate limestone aquifers contaminated
with cis-1,2-dichloroethylene cDCE.
â€¢ Developing and performing batch tests quantifying the cDCE and VCdegradation
rates (Figure 2.1 ).
Figure 2.1: Batch test design
â€¢ Analyse the results of the batch experiments in relation to existing literature and site-
specific conditions to identify key factors influencing DCE degradation.
â€¢ Evaluate the effectiveness of BiRD or a combination of BiRD and ERD based on the
experimental results and reflect on a pilot field project for the implementation of the
most promising approach.
4

3
DCE DEGRADATION MECHANISMS AND THEIR
APPLICATION TO NAVERLAND
This chapter establishes the environmental risks of cDCE by examining its properties, trans-
port behaviour and toxicity Section 3.1 . This is followed by the possible remediation meth-
ods Section 3.2 before focusing on the Naverland siteâ€™s specific conditions to develop a site-
specific strategy Chapter 4 .
3.1 DCE properties, degradation and toxicity
3.1.1 Properties
cDCE is a colourless, sweet-smelling light liquid that is not naturally occurring and yet can
be found in both groundwater and the atmosphere [ Toxic Substances et al., 1996 ; Dreher
et al., 2014 ]. Key properties relevant to the plume are summarised in Table 3.1 .
Table 3.1: Physical and chemical properties of TCE, cDCE, and VC.
Property TCE cDCE VC
Boiling point [ â—¦C] 1 86.7 60.0 -13.4
Density , 20 â—¦C [ ğ‘”/ ğ‘ğ‘š 3] 2 1.478 1.284 0.910
Vapour pressure, 20 â—¦C
[kPa] 1
5.8 24.0 333.0
Water Solubility , 25 â—¦C
[mg/L] 3,4
1280 3500 2700
Viscosity [Pa Â· s]1 0.58 Â· 10âˆ’ 3 0.47 Â· 10âˆ’ 3 0.19 Â· 10âˆ’ 3
log ğ¾ğ‘‚ğ‘Š 5,3 2.61 1.86 1.46
log ğ¾ğ‘‚ğ¶ 4,6 1.69 - 2.66 1.61 - 1.69 2.38â€“2.95
Henryâ€™s Law Constant,
24.8 â—¦C [atm Â· m3/mol] 7
0.00958 0.00408 0.0278
1 [Dreher et al., 2014 ] 2 [ Haynes, 2014 ] 3 [ Horvath et al., 1999 ] 4 [ U.S. Environmental Protection
Agency , 2011] 5 [ Hansch et al., 1995 ; Sakuratani et al., 2007 ] 6[Brigmon et al., 1998 ; Chiou et al.,
1998; Garbarini et al., 1986 ; Mouvet et al., 1993 ; Rathbun, 1998 ; Sahoo et al., 1997 ; Lu et al., 2011 ] 7
[Gossett, 1987 ]
5

6 3. DCE Degradation Mechanisms and Their Application to Naverland
Henryâ€™s law constant quantifies the equilibrium partitioning of a compound between
aqueous and gaseous phases, is fundamentally temperature-dependent. It may be defined
either as the liquid-to-gas phase abundance ratio (reflecting solubility) or its inverse (re-
flecting volatility) [ Sander et al., 2022 ]. For cDCE, Henryâ€™s law constant increases from
0.00178 ğ‘ğ‘¡ğ‘šğ‘š 3ğ‘šğ‘œğ‘™ 1 at 8 â—¦C to 0.00396 ğ‘ğ‘¡ğ‘šğ‘š 3ğ‘šğ‘œğ‘™ 1 at 24.0 â—¦C [ F. Chen et al., 2012 ]. The as-
sociated aqueous solubility rises from 6,954 to 7,026 mg/L when temperature is increased
from 8 to 21 â—¦C [ F. Chen et al., 2012 ]. A Henryâ€™s Law constant of 0.00408 ğ‘ğ‘¡ğ‘š Â· ğ‘š3/mol
(24.8 â—¦C) indicates that cDCE might volatilize from moist soil surfaces and is expected to
volatilize from dry soil due to its high vapour pressure [ PubChem, 2025 ]. With an esti-
mated ğ¾oc of 36-49 mL/g, cDCE is highly mobile in soil and can leach into groundwater if
the DCE did not directly enter the groundwater network [ Chu et al., 2000 ; U.S. Environ-
mental Protection Agency , 2011 ]. Its solubility of 3500 mg/L in water and low adsorption
to soil particles (log ğ¾ow of 1.86) explain its high mobility in groundwater, contributing to
the large plume at Naverland.
3.1.2 Natural attenuation
Natural attenuation in the groundwater aquifer is caused by naturally occurring physical,
chemical and biological processes such as volatilisation, sorption, dispersion, and biodegra-
dation [ Popek, 2018 ]. Sorption is a process by which a substance (sorbate) is sorbed (ad-
sorbed or absorbed) on or in another substance (sorbent)[ Brusseau et al., 2019 ]. It involves
adsorption, absorption and precipitation [ Brusseau et al., 2019 ]. Sorption is quantified us-
ing the relationship between the concentration of contaminant in the sorbed state and the
concentration in the aqueous phase, called the sorption isotherm [ Brusseau et al., 2019 ].
The simplest is the linear coeï¬€icient ğ¾d; the larger this value, the more the contaminant is
sorbed by the sorbent [ Brusseau et al., 2019 ]. The ğ¾d for chlorinated solvents in a limestone
aquifer is low [ MiljÃ¸styrelsen, 1998 ]. According to the calculations performed by Salzer,
2013 in their masterâ€™s thesis, it was 0.03 ğ¿ğ¾ ğ‘”âˆ’ 1 for DCE [ Salzer, 2013 ]. The degree of sorp-
tion is primarily governed by the intrinsic properties of the chlorinated solvent (e.g., solu-
bility and polarity) and the characteristics of the aquifer material itself (e.g., organic carbon
content and clay-fraction content) [ Cwiertny et al., 2010 ]. To a lesser extent, the geochem-
ical conditions of the aquifer, such as pH, temperature, ionic strength, and the presence of
other dissolved chemicals, can also modulate sorption processes [ Cwiertny et al., 2010 ]. As
a consequence of these interactions, CE often migrate at velocities slower than that of the
surrounding groundwater, a process termed retardation [ Cwiertny et al., 2010 ]. The retar-
dation in the matrix, expressed by the retardation factor ğ‘…ğ‘š, is moderate for cDCE, 1.13 in
limestone aquifers calculated where the matrix porosity ( ğœ™ğ‘š) is 0.4 and bulk density ( ğœŒğ‘)
is 1.75 [ Salzer, 2013 ; Broholm et al., 2016 .]

3.1. DCE properties, degradation and toxicity 7
As previously mentioned, long-term sources of dissolved-phase groundwater pollution
are created in fractured aquifers [ Stroo et al., 2003 ; Christ et al., 2005 ; Popek, 2018 ] because
as chlorinated ethene concentrations decrease in the fractures, contaminants stored within
the low-permeability matrix can diffuse back into the fractures due to the establishment of a
concentration gradient, a phenomenon known as back diffusion (as illustrated in Figure 3.1 )
[Broholm et al., 2016 ; Ding et al., 2025 ]. Lateral advection and hydrodynamic dispersion
dominate in the fissure system of chalk aquifers [ Little et al., 1996 ]. The contaminant in
the microporous matrix will be attenuated as a result of sorption and diffusion of dissolved
chlorinated solvents from the fractures [ Jacobsen, 1991 ; WitthÃ¼ser et al., 2003 ]. When flow
is high, sorption processes predominate at the fracture surfaces because matrix diffusion is
a comparatively slow mechanism [Rosenbom, 2005 ]. On the other hand, sorption processes
mainly occur within the matrix for considerable mean resident periods because the matrixâ€™s
accessible exchange surfaces are far more significant than the fractureâ€™s [ Rosenbom, 2005 ].
Figure 3.1: Conceptual illustration for back diffusion of reactive contaminant from a stratified low-
permeability aquitard with internal contaminant sources [ Ding et al., 2025 ].
Supplementary to the matrix, there may be areas of immobile fluid in â€dead-endâ€ frac-
tures for a network and within individual fractures (assuming flow through the fracture
is channelled) [ Rosenbom, 2005 ]. Since the impact of dispersion that may occur along the
fracture walls is negligible, dispersion in the fractures is frequently ignored in pollutant
transport in chalk aquifers [ Reilly et al., 1994 ]. However, Schulze-Makuch, 2005 â€™s assess-
ment of dispersion in aquifers with varying geology revealed significant variances in the
longitudinal dispersivity for carbonates and, hence, significant on an aquifer scale.

8 3. DCE Degradation Mechanisms and Their Application to Naverland
Similar to volatilisation, biodegradation rates are governed by temperature. Temper-
ature fundamentally controls reaction kinetics, with biological and chemical degradation
typically increasing at higher temperatures within the microbial tolerance ranges; hence,
microcosm studies often accelerate biodegradation by incubating at elevated temperatures
with agitation, though such artificial conditions may limit direct field applicability [ Henry ,
2010]. pH similarly constrains biodegradation, as it must remain within species-specific
thresholds for microbial activity , Dehalococcoides dechlorinate CE within a fairly narrow pH
range of 6.5â€“8 [ LÃ¶ffler et al., 2013 ; Y. Yang et al., 2017 . Additionally , aquifer minerals such
as oxides exhibit pH-dependent surface charges, which influence whether the surface is
favourable for bacterial sorption [ Scholl et al., 1992 ]. In contaminated groundwater, bac-
terial sorption was lower at low pH and higher at high pH compared to uncontaminated
systems [Scholl et al., 1992 ].
3.1.3 Toxicology and ecotoxicology
Due to its volatility ( Table 3.1 ), human exposure to chlorinated ethylenes is assumed to
be inhalation from ambient and urban air or groundwater [ Cichocki et al., 2016 ; B. Huang
et al., 2014 ]. Chlorinated ethylenes have been studied in environmental toxicology since
the mid- to late 1970s and mid-1980s and, therefore, are not classified as emerging contam-
inants [ Lash, 2019 ]. Inhaled DCE has the acute effects of eye irritation, nausea, narcosis
and death and unknown long-term effects in humans [ Galizia et al., 2010 ]. VC and TCE
are classified as a human carcinogen with the target tissue being the kidney for TCE while
PCE is a probable human carcinogen [ Cichocki et al., 2016 ]. P . T. McCauley et al., 1995 â€™s
study showed toxicity at subacute and subchronic oral exposure levels as low as 31,988.88
Î¼g/kg/day. Liver abnormalities were observed at 96,936 Î¼g/kg/day , and kidney abnormal-
ities at 31,988.88 Î¼g/kg/day [ P . T. McCauley et al., 1995 ]. Tests on cDCE for genetic activity
in yeast (Saccharomyces cerevisiae) showed that it was toxic but not genetically active, even
with metabolic activation [ Bronzetti et al., 1984 ].
Fortunately , as the bioconcentration factor of DCE is estimated to be 15 to 22 and there-
fore does not tend to bioconcentrate in aquatic organisms based on the linear regression
equations involving low ğ¾ğ‘‚ğ‘Š and water solubility[ U.S. Environmental Protection Agency ,
2011]. Hence, the potential of DCE biomagnifying in aquatic food chains is low [ U.S. En-
vironmental Protection Agency , 2011 ].
Table 3.2: Soil and groundwater quality criteria in Denmark
CE Soil quality criterion [mg/kg] Groundwater quality criteria [ ğœ‡ğ‘”/ ğ¿]
TCE 5 1
DCE 85 1
VC 0.4 0.2

3.2. Remediation methods based on reductive dechlorination 9
3.2 Remediation methods based on reductive dechlorination
3.2.1 Bioremediation
As noted in Section 3.1.2 , natural attenuation encompasses biodegradation, including re-
ductive dechlorination. However, because natural attenuation is inherently inconsistent
and frequently results in contaminants being only partially degraded, it is crucial to better
understand the underlying degradation pathways and explore strategies for targeted im-
provement. Among these processes, reductive dechlorination is of particular importance
for chlorinated ethenes. As a distinct anaerobic pathway , reductive dechlorination involves
the stepwise removal of chlorine (Cl) atoms from the contaminant, replacing them with
hydrogen (H) as shown in Figure 3.2 . In this process, chlorinated ethylenes act as respira-
tion aids and electron acceptors rather than as a food source [ Popek, 2018 ]. Consequently ,
an external carbon source is often required to generate electron donors (H) (through fer-
mentation) to facilitate reductive dechlorination [ Popek, 2018 ]. There must be a suitable
ratio of electron acceptors to donors for reductive dechlorination to occur. Furthermore,
the reductive dehalogenation of PCE and TCE typically occurs more readily than that of
DCE and VC [ Han et al., 2012 ; Schmidt et al., 2008 ]. This means that the rate of reduction
is higher for PCE and TCE, and it can occur under mildly reducing conditions such as ni-
trate or iron-reducing conditions [ Spormann, 2006 ]. In contrast, the degradation of DCE
and VC requires stronger reducing conditions, such as sulfate-reducing or methanogenic
conditions [ Bradley et al., 2010 ; Spormann, 2006 ; MaymÃ³-Gatell et al., 1999 ; Ballapragada
et al., 1997 ; T. M. Vogel et al., 1985 ].
Figure 3.2: Biotic pathway under anaerobic conditions [ C. B. Ottosen, 2020 ]

10 3. DCE Degradation Mechanisms and Their Application to Naverland
Bioremediation typically can be categorised into:
1. Biostimulation: enrichment of indigenous microorganisms. This is done by supple-
menting micronutrients and electron acceptors [ Mohamed et al., 2023 ].
2. Bioaugmentation: enrichment with non-indigenous microorganisms [ H. Huang et al.,
2020].
There are several Organohalide-respiring Bacteria (OHRB), known to be able to degrade
PCE and TCE [ Hug et al., 2013 ]. Until fairly recently , Dehalococcoides was the only known
genus that could break down cDCE and VC but then Y. Yang et al., 2017 discovered the
VC reductase gene, cerA, in a Dehalogenimonas species. Additionally , the vcrA and bvcA
genes in Dehalococcoides (different strains) are crucial for the complete reductive dechlori-
nation of VC to ethene [ MÃ¼ller et al., 2004 ; Krajmalnik-Brown et al., 2004 ]. The presence
of vrcA gene is vital for cDCE and/or VC dechlorination while bvcA gene encodes a vinyl
chloride reductase that is highly effective in converting VC to ethene [ MÃ¼ller et al., 2004 ;
Krajmalnik-Brown et al., 2004 ]. The fact that most known OHRB genomes have numerous
putative reductive dehalogenase genes, but only a small number have been described, sup-
ports this discovery and suggests that there are likely further unidentified and potentially
significant biomarkers [ Molenda et al., 2016 ].
The micro-nutrients Dehalococcoides require is vitamin B12 (cobalamins), which are pro-
duced by other species hence, in commercially produced cultures, they consist of a com-
munity rather than a pure culture; the Dehalococcoides use it as an enzymatic cofactor [ J. He
et al., 2007 ]. Interestingly , using titanium citrate as a reductant and vitamin B12 (a natural
cobalt-containing porphyrin) catalyses the dechlorination of CE. This is an abiotic vitamin-
mediated degradation, as the heat killed the microorganisms, but it is 4 to 5 times lower
than the corresponding microbiological degradation [ Guerrero-Barajas et al., 2005 ].
Major et al., 2002 conducted a pilot-scale field test in south central Texas, United States,
consisting of gravel, sand, silt, and clay , which demonstrated the dechlorination of cDCE to
ethene through bioaugmentation with KB-1, a microbial consortium containing the phy-
logenetic relatives of Dehalococcoides ethenogenes . Within 200 days, PCE, TCE and cDCE
concentrations in the test zone dropped below 5 Î¼g/L, with ethene production accounting
for the mass loss. A more recent study found similar results in the urban area of Suzhou,
Jiangsu, China, in a site primarily consisting of silty sand, R. Wu et al., 2023 reported 99.7%
of VC dechlorination to ethene within 3 months. It should be noted that this study em-
ployed thermal pretreatment, maintaining groundwater temperatures at âˆ¼30Â°C - a condi-
tion known to accelerate microbial dechlorination rates, as discussed in Section 3.1.2 .

3.2. Remediation methods based on reductive dechlorination 11
3.2.2 Biogeochemical reductive dechlorination
A handful of iron-bearing minerals have been used in kinetic batch tests studied for their
abiotic dechlorination include iron sulphides (mackinawite and pyrite) [ Lee et al., 2002 ;
Weerasooriya et al., 2001 ], iron oxides (magnetite) [ Lee et al., 2002 ], Green Rust (GR)
(fougerite)[Lee et al., 2002 ; Jeong et al., 2011 ], and phyllosilicate clays (vermiculite and
biotite) [ Lee et al., 2004 ]. Among these, mackinawite (FeS) under anoxic conditions can
effectively reduce PCE and TCE to acetylene as the main product [ Butler et al., 1999 ], while
pyrite degrades more slowly [ Tobiszewski et al., 2012 ]. Figure 3.3 shows the possible path-
ways for reductive dechlorination. According to Liang et al., 2009 , chloride GR and pyrite
are more effective at PCE and TCE degradation than sulphate GR, magnetite and goethite.
Therefore, this work focuses on iron sulphides, as they demonstrate superior degradation
eï¬€iciency compared to other iron-bearing minerals. However, the field study conducted
by Darlington, Lehmicke, et al., 2013 in southern California, United States, reported that
pyrite is not likely the mineral involved in transforming cDCE, although it was abiotically
under anaerobic conditions in a sandstone aquifer. Furthermore, magnetite was identified
to remove cDCE in a sandy aquifer in Minnesota; the first-order rate of attenuation over 13
years of monitoring was 0.52 Â± 0.195 per year for cDCE [ Ferrey et al., 2004 ].
Figure 3.3: Possible pathways for the reductive dechlorination of chlorinated ethenes by iron sulphides [ Lee
et al., 2002 ; Han et al., 2012 ]

12 3. DCE Degradation Mechanisms and Their Application to Naverland
The pathway cDCE â†’ acetylene (by Î²-dichloroelimination) â†’ ethylene (by hydrogena-
tion) â†’ ethane (hydrogenation) is the preferred pathway as it avoids the formation of VC
and only possible abiotically [ Hyun et al., 2015 ; Han et al., 2012 ]. Furthermore, acetylene
is energetically favourable and therefore quickly used to support microbial growth after its
formation [ Darlington and Rectanus, 2015 ].
Iron sulphide
FeS plays a crucial role in biogeochemical processes are brought about from microbial in-
teractions [PÃ³sfai et al., 2006 ] serving as an energy source in the absence of solar irradiation
and acting as a â€naturally occurring electrical wire, bridging spatially discrete environments
and mediates long distance extracellular electron transferâ€ [ Kondo et al., 2015 ]. Further-
more, FeS is able to enter the periplasm of Sulfate-Reducing Bacterium (SRB) to accelerate
electron transport [ Y. Zhang et al., 2025 ]. FeS also act as an electrocatalyst because of its
semi-conductive and/or metallic qualities, enabling electron-transfer processes and micro-
bial metabolism [ Nakamura et al., 2010 ; Yamamoto et al., 2013 ; Yamaguchi et al., 2014 ].
Effective ISBGT requires a balance of high sulfate loading, presence of iron oxides and
suï¬€icient organic carbon [ Darlington and Rectanus, 2015 ; Y. T. He et al., 2015 ]. High load-
ing of sulfates is reduced into hydrogen sulphide (HS âˆ’ ) by SRB [ Darlington and Rectanus,
2015], who uses organic carbon as an electron donor for sulphide production. Ideally , the
site is under sulphate-reducing conditions and avoids supporting competitive methanogenic
bacteria [ Darlington and Rectanus, 2015 ]. The aforementioned iron oxides present in the
sediments then react with HS âˆ’ to form iron sulphides, which then dechlorinate chlorinated
ethenes [ Kennedy et al., 2006 ]. A column experiment conducted by C. Sun et al., 2025 was
able to effectively suppress the competition with methanogenic bacteria through the high
flux of ğ‘†ğ‘‚ 2âˆ’
4 while also forming ğ¹ğ‘’ğ‘†, ğ¹ğ‘’ğ‘† 2, and ğ¹ğ‘’ 3ğ‘†4, which were capable of Î²-eliminating
contaminants (ISBGT)
Enhanced ISBGT is the injection of amendments into the contaminated zone to promote
optimal conditions for the preferred abiotic mechanism [ Y. T. He et al., 2015 ]. BiRD is stimu-
lated by the addition of sulphate and organic carbon [ Kennedy et al., 2006 ]. Iron, a mineral
typically present in the aquifer, might also be supplemented [ Kennedy et al., 2006 ]. The
possible amendments for ISBGT are shown in Table 3.3 . Additionally , without iron oxides,
the high levels of sulphides are toxic to microorganisms. For example, according to Mao
et al., 2017 , environments with sulphate concentration of 5 mM, had no effect on the cell
yield of Dehalococcoides (strain 195) but decreased by about 65% in sulphide environments.

3.2. Remediation methods based on reductive dechlorination 13
Table 3.3: Common amendments used in ISBGT engineered systems [ Darlington and Rectanus, 2015 ]
Component Bioreactors and Trench biowalls Liquid injections
Iron Sand with natural high iron content,
Iron oxide
Iron sulfate, Iron chloride, Ferrous
lactate
Sulfate Calcium sulfate Iron sulfate, Magnesium sulfate,
Sodium sulfate, Calcium sulfate
Electron donor Tree mulch, Cotton gin compost,
Mulch coated with vegetable oil
Sodium lactate, Emulsified
vegetable oil, Lecithin, Soybean oil
Additional amendments Sand and/gravel for permeability ,
Buffer
Buffer
The recommended hydraulic retention time is 15 to 30 days to provide suï¬€icient time
for the FeS formation as well as the dechlorination [ Darlington and Rectanus, 2015 ]. The
engineered hydraulic residence time of the target treatment zone is controlled by the hy-
drogeology [Darlington and Rectanus, 2015 ]. The specific time period is shown in Table 3.4
[Darlington and Rectanus, 2015 ].
Table 3.4: Time period for the mechanism of ISBGT [ Darlington and Rectanus, 2015 ]
Biotic sulfide formation FeS formation (precipitation) Abiotic dechlorination
ğ‘†ğ‘‚ 2âˆ’
4 + 9ğ»+ + 8ğ‘’âˆ’ = 8ğ»ğ‘† âˆ’ + 4ğ»2ğ‘‚ ğ»ğ‘† âˆ’ + ğ¹ğ‘’ 2+ = ğ¹ğ‘’ğ‘† + ğ»+ ğ¶2ğ¶ğ‘™ 4 âˆ— + ğ¹ğ‘’ğ‘† + 4ğ»2ğ‘‚ = ğ¶2ğ»2
ğ‘†ğ‘‚ 2âˆ’
4 + 2ğ¶ğ»2ğ‘‚ â†’ ğ»2ğ‘† + 2ğ»ğ¶ğ‘‚ 3âˆ’ 2ğ¹ğ‘’ğ‘‚ğ‘‚ğ» + 3ğ»ğ‘† âˆ’ = 2ğ¹ğ‘’ğ‘† + ğ‘† + ğ»2ğ‘‚ + 3ğ‘‚ğ» âˆ’ + ğ¹ğ‘’ 3+ + ğ‘†ğ‘‚ 2âˆ’
4 + 4ğ¶ğ‘™ âˆ’ + 6ğ»+
Days Instantaneous ğ‘¡1/ 2 = 30 Â± 15 days
*For other CE, please refer to Figure 3.3
ISBGT seems to work best when a recirculating injection method is used (shown in Fig-
ure 3.4) [Darlington and Rectanus, 2015 ]. Installing injection wells to add amendments and
using extraction wells or points to gather groundwater, combined with the amendments,
and then reinjecting them into the injection wells are both recirculation parts [ Darlington
and Rectanus, 2015 ]. Doing this continuously creates a mixing zone in the treatment area
where the contaminant can come into contact with the reactive amendments [ Darlington
and Rectanus, 2015 ]. Most effective in unconsolidated geology and in groundwater plumes
with high hydraulic conductivity , as excessive flow rates would prevent the reaction and
restrict hydraulic retention time [ Darlington and Rectanus, 2015 ].

14 3. DCE Degradation Mechanisms and Their Application to Naverland
Figure 3.4: Recirculation system configuration for engineering ISBGT [ Darlington and Rectanus, 2015 ]
To differentiate between abiotic and biotic degradation, Figure 3.5 shows that biologi-
cal degradation is sequential dechlorination of parent followed by daughter products com-
pared to abiotic degradation, where it is simultaneous degradation of all the CE present
[Darlington and Rectanus, 2015 ].
Figure 3.5: Degradation pattern for CE; biotic vs abiotic [ Darlington and Rectanus, 2015 ]
There have been various laboratory tests simulating environmental conditions to fig-
ure out the optimal conditions for mackinawite synthesis and dechlorination. In Hyun
et al., 2015 â€™s study , including sediments and mixing had no impact, while the higher the
pH, the faster the degradation rate and the presence of citrate accelerated the dechlorina-
tion of cDCE. As framboidal FeS has a higher surface area, hence, higher reactivity com-
pared to crystalline FeS, citrate probably acts as an inhibitor for crystal growth [ Darling-
ton and Rectanus, 2015 ; Hyun et al., 2015 ]. Therefore, nucleation being dominant is more
desired than crystal growth [ Whiting et al., 2014 ]. Additionally , freeze-dried mackinaw-
ite is unreactive and therefore, fresh mackinawite or pyrite should be formed in the site
[Hyun et al., 2015 ]. The cause could be aggregation development or the loss of reactiv-

3.2. Remediation methods based on reductive dechlorination 15
ity in nano-sized mackinawite suspensions during the rinse and freeze-drying processes
[Y. Chen et al., 2019 ]. While non-stabilized FeS could only degrade 25% of lindane (Î³-
hexachlorocyclohexane) in the same amount of time, it has been discovered that FeS nanopar-
ticles stabilised by fungal polymer, which is produced from a basidiomycetous fungus called
Itajahia sp., could catalyse a rapid reductive dehalogenation reaction of lindane (5 mg/L)
with an eï¬€iciency of 94% in 8 hours [ K.M. Paknikar et al., 2005 ]. When compared to bare
nanoparticles, stabilised FeS nanoparticles have been shown to degrade chlorinated hydro-
carbons far more effectively [ Y. Chen et al., 2019 ].
Whiting et al., 2014 conducted a geochemical model to forecast which mineral phases
go into solution (undersaturated), be in equilibrium (saturated) and precipitate out of so-
lution (supersaturated) and found that FeS e.g. mackinawite were near equilibrium while
ğ¹ğ‘’ğ‘† 2 e.g. pyrite precipitated out of solution in all sites. Furthermore, the previously men-
tioned nucleation is supported in this environmental condition [ Whiting et al., 2014 ]. The
study used electron microprobe analyses to determine the composition and grain size of Fe-
bearing species (iron monosulfides, iron disulfides, GR, etc) as well as itâ€™s dechlorinating
potential.
3.2.3 Combination of biotic and abiotic remediation
The effect of FeS on OHRB-mediated (i.e., D. mccartyi CG1, G. lovleyi LYY) dechlorination
through batch experiments was studied by Liu et al., 2023 in soil (brown, laterite, black,
and paddy soil, river sediment and SiO2). Acetate was used as a carbon source and the pH
was 7.2. The results were interesting; Pure cultures of Dehalococcoides mccartyi CG1 and
Geobacter lovleyi LYY showed a substantial inhibition of the PCE-to-DCE dechlorination
process when 25 g/L FeS was added, but both tolerated 2.5 g/L of FeS. Nevertheless, these
cultures showed the greatest resistance to FeSâ€™s inhibitory effects when they were mixed
together in a microcosm. The development of flocs or biofilms, which may protect Geobac-
ter and Dehalococcoides from direct exposure to FeS, may be the cause of this enhanced
tolerance. FeS may hinder OHRB-mediated dechlorination, but OHRB can also affect FeS-
mediated abiotic dechlorination by changing the products of dechlorination. To elaborate,
biotic PCE-to-DCE dechlorination had greater percentages of acetylene than abiotic con-
trols, suggesting that OHRB-mediated dechlorination may help FeS-mediated acetylene
synthesis by supplying intermediates such as DCEs and/or TCE. Furthermore, by regen-
erating FeS, the enrichment of Desulfovibrio, a SRB that can reduce ğ‘†ğ‘‚ 2âˆ’
4 and ğ¹ğ‘’ 3+ may
enhance abiotic PCE-to-acetylene dechlorination. By blocking direct interaction between
FeS and OHRB cells, organic matter (e.g. humic substance) and the formation of biofilms
or flocs may also lessen the inhibition of OHRB caused by FeS. Therefore, these biotic and
abiotic mechanisms most likely work together to dechlorinate CE in soil overall. Likewise,
a study by Yaru Li et al., 2021 demonstrated that in-situ formed FeS nanoparticles (0.2 mM
ğ¹ğ‘’ 2+ + ğ‘†2âˆ’ ) enhanced TCE dechlorination by Dehalococcoides mccartyi strain 195, increasing
rates from 25.46 Â± 1.15 to 37.84 ğœ‡ğ‘šğ‘œğ‘™ Â· ğ¿âˆ’ 1 Â· ğ‘‘ğ‘ ğ‘¦âˆ’ 1 as well as the abundance of key genes.
These results suggest low FeS concentrations may stimulate Dhc 195â€™s electron transport.","16 3. DCE Degradation Mechanisms and Their Application to Naverland
Similar to Liu et al., 2023 â€™s observation, dechlorination rate of TCE was inhibited slightly
at 0.6 mM (higher concentrations). This study also showed that the ratios of Fe 2+/S 2â€“ (i.e.,
0.5, 1, and 2) had negligible effects on TCE dechlorination rates, possibly due to the sim-
ilar electrical conductivity of each ironâ€“sulfur compounds. Yoshikawa et al., 2021 studied
reductive dechlorination of PCE with a groundwater sample Dehalococcoides and 2 mg/L of
iron. When 28 mg/L of ferrous iron was supplied, the time required for complete dechlori-
nation of 1 mg/L of PCE decreased 84 to 49 days.
In summary , whilst biotic degradation carries the inherent risk of VC accumulation, abi-
otic or abiotic + biotic degradation doesnâ€™t. Though GR shows higher surface-normalised
rate constants in some cases, it is unstable, pH-dependent and cannot be used in freeze-dried
form; therefore, it is not available as a commercial product, limiting its practical application
at this site [ Han et al., 2012 ; Moreno et al., 2007 ; Lee et al., 2002 ]. Granular ZVI is con-
strained by reactivity primarily at the particle surface interface. In contrast, the reductive
mineral zone established through FeS formation can extend a significant distance down-
gradient, facilitating broader contaminant interaction [ EVONIK, n.d.(a) ]. Furthermore,
FeS enhances ZVI performance when combined through sulfidation, improving electron-
mediated dechlorination without the limitations of pure ZVI systems; however, as this site
does not have a high concentration of sulfate, sulfate would need to be added [ N. Wu, Yi Li,
et al., 2025 ; Guo et al., 2023 ; Islam et al., 2021 ]. ZVI can also be combined with lactic acid
to accelerate the initiation of biotic dehalogenation using the intrinsic reductive capabilities
of ZVI [ N. Wu, Yi Li, et al., 2025 ; Herrero et al., 2019 ]. Both ZVI combinations would be
useful for this site if it can be injected. Another combination is composite material Carbo-
Iron, which combines colloidal activated carbon and nano-ZVI (nZVI) to accumulate PCE
for ERD, but the site being studied here is slightly deeper than Katrin et al., 2016 â€™s, and
they had a higher pH. A more detailed discussion of these alternative methodologies can
be found in Section A.1

4
SITE: NAVERLAND
4.1 Geology and Hydrogeology
Naverland 26AB is situated in Vestegnen, HerstedÃ¸sterâ€™s industrial district. With an eleva-
tion of roughly +21.5 m/DVR90, Vestegnen is a topographically flat region [ Hovedstaden,
2007]. The regional geological settings are shown in Figure 4.1 . The moraine clay at Naver-
land 26AB can be divided into two units: a coarse-elastic lower unit and a fine-grained
upper unit [ COWI, 2023 ]. Followed by a broad layer of sand and gravel with the limestone
beneath [COWI, 2023 ]. The upper metre of the Bryozoan limestone is thought to have been
crushed [ COWI, 2023 ]. The transect of this profile is in Appendix B . Bryozoan limestone
consists of skeletal fragments of bryozoans, the size of sand to silt, mixed with subordinate
amounts of lime mud and clay [ Andersen et al., 2018 ]. Additionally , chalk can be found
beyond -30m/DVR90 [ GEUS, n.d. ] (map in Appendix B ). To distinguish, chalk is a soft,
nearly white, fine-grained formation that is poorly or unlithified and pure calcilutite, while
limestone is a more general term [ Bromley , 1979]. To be more specific, on this site, there is
Cretaceous (Maastrichtian) chalk, which is a carbonate mudstone composed primarily of
calcareous nannofossils (especially coccoliths) [ Jakobsen et al., 2017 ].
17

18 4. Site: Naverland
Figure 4.1: Geology of the Naverland site [ COWI, 2023]

4.2. Contamination 19
The limestoneâ€™s bulk density ranges from 1.42 to 2.36 ğ‘”/ ğ‘ğ‘š 3. It has a high porosity ,
which ranges from 13% to 48% (average 33% in boreholes at the source, C1-C3). Larger
flint layers are typically located in the upper 2-4 m but have also been discovered down to
19 metres below surface (mbs) in C3 [ Broholm et al., 2016 ]. According to Broholm et al.,
2016, the hardness of the limestone increases with depth, and cracks are more widespread
in the soft portions of the limestone, near flint layers, or inside thick hard limestone zones.
While the crushed layer is mostly saturated (largely porous groundwater flow) and in hy-
draulic contact with the underlying primary reservoir in the limestone (primarily fracture
transport), the moraine clay is primarily unsaturated.
Naverland is situated on a regional groundwater divide and in a location with a small
water table gradient (1-2%in a northeasterly direction and 2-3%in a southeasterly direction).
North of the source area the flow direction is northeast towards Glostrup Forsyningâ€™s ab-
straction in Vestskoven and near Ejby , and south of the source area the flow direction is
south and southeast [ SÃ¸, 2023 ]. While the flow in the limestone is horizontal, the main
dispersion path of infiltrating groundwater is downward towards its primary reservoir.
The primary reservoirâ€™s predominant flow direction is northeast when local groundwater
subsidence is omitted. According to slug and pump tests, the hydraulic conditions in the
crushed layer at the source site vary significantly , with hydraulic conductivities ranging
from 5 Â· 10âˆ’ 4ğ‘š/ ğ‘  to 3 Â· 10âˆ’ 6ğ‘š/ ğ‘  [COWI, 2023 ; Damgaard et al., 2012 ]. The hydraulic con-
ductivity for the top 10 meters of limestone ranges from 1 Â· 10âˆ’ 4ğ‘š/ ğ‘  to 2.3 Â· 10âˆ’ 4ğ‘š/ ğ‘  [COWI,
2023; Damgaard et al., 2012 ]. When pumped from K11, the flow rate is roughly 150 m/yr
[Damgaard et al., 2012 ; COWI, 2023 ]. According to earlier estimates, the flow rate in the
upper portion of the limestone is between 1 and 16 m/yr [ Hedeselskabet, 2002 ].
The natural seasonal variation in the water table is approximately. 1.2 m [ SÃ¸, 2023 ]. FLUTeÂ®
transmissivity profiles were conducted in C1â€“C3 to further understand the vertical varia-
tion in hydraulic conductivity , revealing potential fracture sites and preferred flow routes
[Damgaard et al., 2012 ; Broholm et al., 2016 ]. The geological layers in the source site are
expected to be the same for the plume [ C. F. Ottosen et al., 2024 ].
4.2 Contamination
From 1965 to 1982, the company Ingolf Jacobsen resold chlorinated solvents, handling an
estimated 500 ton PCE, 1500 ton TCE and 200 ton Trichloroethane(TCA) [ SÃ¸, 2023 ]. PCE
was stored in underground tanks, while TCE and TCA was traded in unbroken drums [ C. F.
Ottosen et al., 2024 ]. In 1977, the original tank was replaced with a new , larger one, which
was used till 1982, when it was emptied and filled with sand. According to Hedeselskabet,
2002, the site has contaminated soil and groundwater around the tank for storing PCE at
levels indicating the presence of a free phase. Hence, it is reasonable to assume there was
improper handling or disposal of PCE, e.g. spills, leaks or incomplete emptying, especially
since the storage and handling practices were probably not as strict as modern standards

20 4. Site: Naverland
[SÃ¸rensen, 2013 ].CE are most frequently used as degreasing solvent in the metal industries
as well as cleaners and adhesives [ Goodman et al., 2022 ]. Nearby , there were dry cleaning
and industrial painting operations where chlorinated solvents were also used [ SÃ¸, 2023 ].
PCE and TCE have degraded to DCE under anaerobic conditions, resulting in a maximum
cDCE concentration of 7100 Âµg/l in the groundwater [ Goodman et al., 2022 ; SÃ¸, 2023 ]. The
contamination has spread over 1.8 km from the source to the primary limestone aquifer [ SÃ¸,
2023]. Additionally , hydraulic contamination control is maintained by remedial pumping
in the source area, which forms a local subsidence funnel that draws water from the crushed
layer and the top 10 meters or so of the limestone [ SÃ¸, 2023 ]. Additionally , there has been
continued extraction since 2017 from Glostrup Forsyningâ€™s borehole 200.4416 for Ejby , this
has led to the cDCE concentration in borehole 200.3705 (Ejby 1), which is 2.5 km away from
the source area, to rapidly increase, currently at 2.2 ğœ‡ğ‘”/ ğ¿, which is above the threshold for
drinking water [SÃ¸, 2023 ]. However, boreholes 200.5308 (K21), 200.5728 (M2) and 200.5730
(M1) are unaffected by the 2 extractions. M2 is located south of the source area, while
boreholes M1 and K21 are located northeast of the source area (K21 furthest from the source
area).
The well M1 is to be sampled because, as shown in Figure 4.2 (in m/DVR90), this well has
the highest concentration of cDCE in the plume between +4.69 and +2.69 m/DVR90 (18
and 20mbs) [ GEUS, n.d. ]. At this depth, the cDCE concentration was 200 ğœ‡ğ‘”/ ğ¿ in October
2021 but has decreased to 150 ğœ‡ğ‘”/ ğ¿ in October 2024 (PCE: <0.02 Âµg/l, TCE 0.054 Âµg/l, VC
2,1 Âµg/l and TCA <0,02 Âµg/l) [ GEUS, n.d. ].
Figure 4.2: cDCE plume in Naverland (in m/DVR90) [ SÃ¸, 2023]
4.3 Remediation strategies
A student project consisting of biostimulation was done by SÃ¸rensen, 2013 using lactate,
groundwater and sediment samples from the Naverland site, but no degradation of cDCE
was observed. This is because Dehalococcoides were below the detection limit due to the
mildly reduced redox conditions [SÃ¸rensen, 2013 ]. When SÃ¸rensen, 2013 carried out bioaug-

4.3. Remediation strategies 21
mentation using the bacteria in KB-1Â® (SiREM) (containing Dehalococcoides ) at 15 mbs and
was successful at reaching complete dechlorination at day 85, showing that there is a po-
tential for using ERD at the site. The study also highlighted the importance of limestone
for the degradation to occur. Fortunately , in SÃ¸rensen, 2013 â€™s experiment, no methane was
produced. This was the case because KB-1Â® (SiREM) cannot use acetate as a donor, and
therefore, methanogenesis (the fermentation of acetate to ğ¶ğ‘‚ 2 and ğ¶ğ» 4) does not occur
[Duhamel et al., 2004 ]. Additionally , methanogens may compete with Dehalococcoides for
the H from the donor. Lack of methane production is desired because the methane will
escape out of the groundwater if it comes into contact with the air [ Chilingar et al., 2005 ].
There is a risk of fire or explosion if this takes place indoors or in a small area and the
amount of methane in the air reaches 5% [ Chilingar et al., 2005 ].
As previously mentioned, injection systems are the only feasible option for the Naver-
land site; liquid amendments (listed in Table 3.3 ) would be injected into permanent wells
[Darlington and Rectanus, 2015 ]. This thesis uses the commercial product available, Ge-
oFormÂ® Soluble, which is a soluble sulfate and ferrous iron mix added to an emulsified
organic carbon substrate (ELSÂ® Microemulsion was made using ELSÂ® Concentrate, a 25%
organic carbon substrate) [ EVONIK, n.d.(b) ]. This amendment, therefore, provides iron,
sulfate, electron donors, pH buffer, and nutrients for ISBGT [ EVONIK, n.d.(b) ]. ELSÂ® prod-
ucts contain approximately 90 % lecithin (common formula for lecithin is: C 42H 80NO 8P)
and approximately 10% surfactant and release volatile fatty acids (VFAs) such as lactic,
propionic and butyric when bacteria ferment it [ EVONIK, n.d.(b) ]. The scope of this thesis
does not include considerations of the contamination in the hot spot, the plume extend-
ing south-east and west of the site, or the low concentration of contamination by 1,1,1-
trichloroethane.

5
METHODOLOGY
This study evaluates GeoFormÂ® and ERD (KB-1Â®) for enhanced biodegradation of cDCE
through batch tests with bioactive groundwater and limestone. The bioactive groundwater
is essential because the system relies on SRB to generate in situ iron sulphide (FeS), which
is critical for abiotic dechlorination. Limestone was included in all the batch tests because
the study by SÃ¸rensen, 2013 indicated that limestone is essential for initiating dechlorination
and possibly influences contaminant availability through matrix diffusion and surface inter-
actions. While Hyun et al., 2015 found no impact of sediment with pre-synthesised FeS, this
system is SRB-driven and relies on in situ FeS precipitation, where limestoneâ€™s dual-porosity
(fracture/matrix) and carbonate buffering may assist microbial FeS formation and degrada-
tion kinetics. Four experimental conditions were established (refer to Figure 2.1 ): Scenario
A serves as the biotic control with only bioactive groundwater to simulate natural attenua-
tion; Scenario B combines bioactive groundwater with GeoFormÂ® (primarily ferrous sulfate
monohydrate, which reduces to FeS) and ELSÂ® liquid concentrate as electron donors for
SRB; Scenario C tests potential interactions between GeoFormÂ® and KB-1Â® to identify syn-
ergistic or inhibitory effects; while Scenario D contains only KB-1Â® with bioactive ground-
water as a reference for comparing stand-alone bioaugmentation performance against the
GeoFormÂ®-amended systems. This design enables the evaluation of abiotic (FeS-mediated)
and biotic (Dehalococcoides-driven) degradation pathways under controlled batch condi-
tions, with limestone present in all scenarios to maintain consistency with prior findings.
The comparison clarifies GeoFormÂ®â€™s individual contribution and its combined effects with
biological treatment approaches.
5.1 Field sampling and storage
Limestone blocks were collected from Faxe Kalkbrud, an active limestone excavation site
where rock samples are openly accessible for collection from exposed quarry walls and
debris piles, eliminating the need for digging. In contrast, obtaining limestone from the
Naverland site would require invasive drilling due to its unexcavated subsurface deposits.
Although Naverlandâ€™s native sediments would be ideal for bioremediation, given their in-
herent anaerobic conditions, organic carbon content, and diverse microbial communities
22

5.1. Field sampling and storage 23
[Himmelheber et al., 2007 ], the logistical constraints made Faxe Quarry a practical alterna-
tive. Geologically , both sites share comparable bryozoan-dominated cool-water carbonate
deposits, with Danian limestone overlying Maastrichtian chalk and containing flint nod-
ules/layers [Hvid et al., 2021 ; C. F. Ottosen et al., 2024 ]. The Faxe limestone thus provides
an adequate surrogate, offering microbial attachment surfaces and carbonate buffering to
maintain neutral pH, preventing metabolic inhibition during fermentation.
Figure 5.1: Limestone blocks in Faxe Kalkbrud
Groundwater was collected from screen 3 of the borehole M1 (200.5730) because it has
the highest concentration of 150 ğœ‡ğ‘”/ ğ¿ cDCE in late 2024 while still being in the plume, the
location of this well and the concentration in each screen in 2023 is shown Figure 5.2 and
the concentration of cDCE and VC in late 2024 for each screen is shown in Table 5.1 .
Table 5.1: CE concentrations from borehole M1 (200.5730) in all screens in October 2024 GEUS, n.d.
Screen cDCE VC
1 35 Âµg/l 0.46 Âµg/l
2 96 Âµg/l 2.9 Âµg/l
3 150 Âµg/l 2.1 Âµg/l
4 63 Âµg/l 0.3 Âµg/l

24 5. Methodology
Figure 5.2: Results from the monitoring of the pollution spread in 2023 shown as sum concentrations of all
chlorinated solvents (in Î¼g/l). Screen 1 is the deepest screen and screen 4 is the uppermost screen. The yellow
line shows the approximate course of the geological section presented in Figure 4.2 [SÃ¸, 2023].

5.1. Field sampling and storage 25
The water table was 8.66 mbs. The water extracted using a submersible MP1 pump and
the first 210 L of it was discarded, and then 14 L of groundwater was collected into 1 L glass
flasks with no headspace and stored in coolers. The oxygen level was 0.01 mg/L, and the
temperature was 13.9 - 14.0ğ¶; however, the temperature of the aquifer might be lower as
the flow cell with electrodes was exposed to the sun. The pH of the water was 6.900, and
its conductivity was 1380ğœ‡ğ‘†/ ğ‘ğ‘š (at 14.5 â—¦C ). The conditions here are generally anaerobic,
Hemdorff, 2013 reported locally close to the source, the conditions are least reduced, while
the majority of the plume area is primarily iron-reducing. There was 2.48 mg/L of nitrate
, 101.09, 1.67 - 2.56 mg/L of total iron analysed by the lab. And 179.80 âˆ’ 192.14ğœ‡ğ‘”ğ¿ cDCE,
0.38 âˆ’ 0.41ğœ‡ğ‘”/ ğ¿ VC and no detectable PCE or TCE levels at this well. Until the chemical
amendments required for this experiment arrived, the groundwater was stored in a 10â—¦C
room.
Figure 5.3: Set up for groundwater extraction

26 5. Methodology
Unfortunately , while sampling, some oxygen exposure occurred, leading to oxidised
iron precipitation shown below ( Figure 5.4 ).
Figure 5.4: Groundwater from the field a few days after extraction
5.2 Preparation of materials
The limestone was crushed using a Laboratory Jaw Crusher Pulverisette and passed through
5 mm and 1 mm sieves. As <1 mm would not settle and would be diï¬€icult during the sam-
pling of water, it was not considered. >5 mm was not considered because larger grains
have less surface area per unit mass than smaller grains, which limits the chemical reac-
tivity (buffering capacity) and improves access to the matrix micropores where diffusion-
controlled processes dominate. More information is found in Section A.2 .
(a) Limestone after passing through lab jaw crusher
 (b) Sieving crushed limestone

5.3. Set up of microcosm 27
5.3 Set up of microcosm
All flasks (approximately 1138 mL in volume) were heated in an oven at 220 â—¦C overnight,
with stoppers and metal screw caps autoclaved beforehand to ensure sterile conditions. The
following set-up took place at room temperature on a sterile flush bench. 300 g of limestone
is transferred to each of the glass flasks, followed by flushing with ğ‘2 for 10 minutes each
as the limestone was stored in aerobic conditions. The addition of groundwater using a dis-
penser and then capped with Teflon-coated rubber stoppers to ensure the conservation of
the anaerobic conditions within the system and the CE does not leak out of the closed sys-
tem. Each scenario contained a variable volume of contaminated groundwater and different
amendments. The microcosms are presented in Table 5.2 . All the microcosms are prepared
in triplicate. Oxygen levels and pH were measured to verify that initial conditions were
suitable for microbial activity prior to inoculation.
After adding all the relevant materials for each scenario, the microcosms were stored
upside down at 10Â° C and frequently gently shaken by hand to allow the system to reach
equilibrium.
Table 5.2: Reagents used in each batch
Scenario Comment Sediment
300g
Groundwater ELSÂ® Micro-
emulsion
GeoFormÂ® KB-1Â®
A Biotic control X X
B GeoForm X X X X
C GeoForm + KB1 X X X X X
D KB1 X X X X

28 5. Methodology
Figure 5.6 shows the steps taken for each scenario.
Figure 5.6: Procedure for experimental set up

5.3. Set up of microcosm 29
This study did not use buffers like other experiments (e.g. Hyun et al., 2015 ) as lime-
stone already buffers, and the GeoFormÂ® mixture already includes of a buffer. Furthermore,
citrate was not be added like Hyun et al., 2015 suggests in order to simulate field conditions.
5.3.1 Biotic control
Scenario A
900 mL of contaminated groundwater and no amendments.
5.3.2 GeoFormÂ® dilution
Scenario B
According to EVONIK, n.d.(b) , the target sulfate concentration ranges from 1000 to 3000
mg/L, with Geoform soluble mix containing approximately 22.2% sulfate. Both EVONIK
and Hyun et al., 2015 recommend an optimal molar Fe:S ratio of 1:1. Based on this stoi-
chiometry , the target iron (Fe) concentration can be estimated at 1000 mg/L.
For preparation, deionised water was made anaerobic by nitrogen purging in a separate
flask. To 50 ml of anaerobic water, 12.16 g of GeoFormÂ® and to 37 ml of anaerobic water,
0.556 g of ELSÂ® liquid concentrate were added to achieve the target concentrations. The
GeoFormÂ® and ELS Â® Microemulsion (concentration 15g/L) were mixed using a stirrer.
This prepared solution was then injected into each of the batch bottles containing 810 ml of
contaminated groundwater.
Scenario C
This scenario maintained identical GeoFormÂ® (12.16 g) and ELSÂ® concentrate (0.556 g)
dosages as Scenario B, as both scenarios address a similar amount of contaminant mass and
thus require equivalent electron donor quantities to support either sulfate-reducing bacteria
(Scenario B) or KB-1Â® dechlorinating cultures (Scenario C). The preparation method mir-
rored Scenario B, with GeoFormÂ® and ELSÂ® liquid concentrate added to nitrogen-purged
deionised water before injection into each batch reactor containing 800 ml of contaminated
groundwater. This parallel treatment enables direct comparison of GeoFormÂ®â€™s perfor-
mance with Dehalococcoides under otherwise identical chemical conditions.
5.3.3 KB-1Â®
KB-1Â® culture has a Dhc cell count equal to approximately 1 Â· 1011 cell/mL. Assuming all is
present in the aqueous phase, the resulting Dhc count in the bioaugmented flasks equals ap-
proximately 108 - 109 cells/mL. This concentration corresponds to a cell density 1 âˆ’ 10 times
higher than the minimum density strived for in the site barrier ( 108 cells/L). To scenario C,
9.0 ml of KB-1Â® culture was added.

30 5. Methodology
Scenario D
0.050g of ELSÂ® liquid concentrate was required for each batch. After consulting EVONIK,
the ELS Microemulsion, with a concentration of 3000mg/L was made, and 17 ml was added
to stimulate KB-1Â®. These batches contain approximately 895 ml of groundwater. When the
oxygen levels were measured a few days after the addition of the donor, the concentration
was 1.08mg/L which higher than 0.2mg/L; the concentration KB-1Â® would be able to toler-
ate Aziz et al., 2013 and therefore, the KB-1 culture was added in two steps: (1) First, 4.5mL
of KB-1 was added to let the fermentative bacteria and sulfate reducers already present in
KB-1 consume the remaining oxygen and reduce the oxidized iron. (2) The second round
of KB-1 (9mL) was added 2 weeks later, once conditions were fully anaerobic (oxygen level
was â‰¤ 0.06ğ‘š ğ‘”/ ğ¿) , to provide enough Dehalococcoides for dechlorination.
Figure 5.7: Addition of KB-1 to batch D1
5.4 Monitoring
Sampling the water phase was done by laying the bottles horizontally and then extracting
using sterile 0.6 x 30 mm needles and syringes. Gas phase samples was taken by extracting
from the headspace.
Aqueous sampling was taken about every 3 days for the chlorinated ethenes and later on for
their degradation products (gas samples, intended for measuring methane, ethene, acety-
lene and ethane concentrations), which were extracted directly from the headspace within
the flasks and manually injected into the GC-FID. The water samples for redox parameters
were taken for all scenarios and were used to support the interpretations attained from the
degradation products. The infusion flasks were weighed before and after each extraction
for mass balance. Nitrogen was added into the infusion flasks before extractions over 1 mL
to avoid making a vacuum that sucks air so the system remains anaerobic.

5.4. Monitoring 31
The sampling frequency is presented in Table 5.3 and is the same for all scenarios. A
detailed description of the sampling frequency and the procedure for each sampling round
is given in Section A.5 . The protocol for each parameter is available in
Table 5.3: Sampling frequency for each parameter
Exact day CE Gases VFAs NVOC Anions Cations pH
0 X X X X X X X
1 X X X X X X
3 X X X X
7 X X X X X X
10 X
13 X X X X X
17 X X X X X
21 X X X X X X X
24 X X
29 X X X X X X X
34 X X X X
38 X X X X X X X
51 X X
5.4.1 Chlorinated ethenes
The initial concentration of cDCE in the water phase is 150ğœ‡ğ‘”/ ğ¿, and the desired concentra-
tion in water phase would be 1ğœ‡ğ‘”/ ğ¿ because then the concentration reaches below the limit
of detection which is less than 1% of the original concentration and it is the drinking wa-
ter threshold. 10 mL water sample was extracted and preserved with a drop sulphuric acid
and storing at 10â—¦ C. Then when possible, chlorinated aliphatic hydrocarbons were analysed
from water samples by headspace- Gas Chromatograph (GC) -Mass Spectrometry (MS) ;
the standard/calibration curve needs to be done every time. The detection limit for VC and
cDCE is 0.200ğœ‡ğ‘”/ ğ¿.
The internal standard area (ISTD Area) double the rest of the data for the first 16 samples,
this is shown in Figure A.9 (appendix) and therefore the cDCE concentration of the first 16
samples should be doubled from approximately 60 to 120ğœ‡ğ‘”/ ğ¿. This would also need to be
done for the VC concentration but they were below the detection limit.
The dip seen on day 1 for A2 and the data points on day 34 and 38 for all microcosms were
measured on the same batch however when checked with the lab, no calibration or internal
standard issues were mentioned ( Figure A.10 in the appendix).
Most samples from the GeoForm-containing batches had an interference for the VC mea-
surement (could be the precipitation/limestone/white powder in the vials).

32 5. Methodology
Ethene, Ethane and Acetylene - Headspace analysis
For later samples, ethene, ethane, and acetylene are valuable as they are degradation prod-
ucts, allowing for conclusions regarding the remediation. 1.2mL is taken using a gas-tight
sterile syringe from the headspace of the microcosms into 5.9mL gas vials that were flushed
with nitrogen. Unfortunately , these samples could not be analysed due to a lack of stan-
dards and therefore 10mL water samples taken on the 51st day and sent to an external
lab for analysis in order to reduce costs. Because the ethene concentration was not known
throughout the experiment as planned, the molar concentration of CE over time is in the
appendix ( Figure A.14 ,Figure A.15 , Figure A.16 and Figure A.17 )
5.4.2 Volatile fatty acids
VFAs (acetic acid, propionic acid , iso-butyric acid , butyric acid, iso-valeric acid, valeric
acid, hexanoic acid) were measured to monitor the fermentation of ELSÂ® microemulsion
as well as give information regarding the consumption of electron donors. 1 mL of the water
sample is filtered through 0.45 Âµm nylon filters and added into 2 mL brown glass vials. A
high-performance liquid chromatography (HPLC) from the brand Agilent 1100, combined
with a Viable Wavelength Detector, is used. With a detection limit of 5ğœ‡ğ‘”/ ğ¿ . 1mL of water
sample was passed through 0.45ğœ‡ğ‘š filters and acidified.
5.4.3 Redox parameters
Oxygen - headspace analysis
Oxygen is measured because the limestone was aerobic, hence important at the start, 0.2
mL gas samples were taken directly from the headspace of the batch bottles with a 1 mL
gas-tight sterile syringe as direct on-column injections into the gas chromatograph (Thermo
Scientific TRACE 1310 gas Chromatograph) with a flame ionisation detector (GC-FID) con-
nected to a CBM-102 Communication Bus Module. The extracted amount is substituted by
nitrogen. Lowest concentration in the calibration curve is 1%.
Methane- headspace analysis
Unfortunately methane and carbon dioxide could not be measured as there was too long
time between the sample being extracted into the vials and measured so the samples came
out as below the detection limit
Non-volatile organic compounds
NVOC is measured to help assess the natural capacity of the system to support dechlori-
nation and is done using a TOC 5000A Shimadzu analyzer equipped with an ASI-5000 au-
tosampler. The samples was be filtered through 0.45 Âµm nylon filters, and then preserved
by refrigeration at 4â—¦ğ¶. The NVOC quantification limit was 0.1 mg/L based on C 8H 5KO 4","5.4. Monitoring 33
standard solutions [ Broholm et al., 2016 ].
Most of the samplesâ€™ concentrations were too high (>100 mg/L) for the method used by
the analytical lab for NVOC for all microcosms. The data is available in Table A.3 and Fig-
ure A.6.4 (appendix). Additionally , the data exhibited no discernible pattern. The fact that
the NVOC levels are high indicates there is organic matter that the microbes can consume
and therefore explains why VFAs was not consumed.
Anions
Sulfate gives information about the GeoForm, using mass balance, the amount of sulfate
converted to sulphide can be estimated. At the same time, sulfate and nitrate provide in-
formation regarding the redox potential. A 1 mL water sample is filtered through a 0.2 Âµm
filter into 2mL plastic vial and then analysed by ion-chromatography (IC) â€“ suppressed con-
ductivity detection using and anion-exchange column under isocratic conditions with a car-
bonate/bicarbonate eluents system. The quantification limit is 7.5 mg/L. As mentioned in
Section 5.3.2 , the target concentration for sulphate is 1000 mg/L but the results first showed
the sulphate in the GeoForm-containing batches are â‰ˆ 140 mg/L at its maximum and there-
fore the sulphide concentration was measured using test kits for the field and the levels
measured on day 57. However then there was a confirmed calibration issue so sulphate
concentration in the GeoForm-containing batches were measured again.
Cations
Iron gives information about the GeoForm, at the same time iron and manganese provide
information regarding the redox potential. Fe and Mn were measured using an inductively
coupled plasma device equipped with an optical emission spectrophotometer. For this, 10
mL of the water sample is passed through 0.2 Âµm filters and added to a plastic vial and
stored at 10â—¦C. The detection limit is 0.5mg/L.
5.4.4 pH
The pH was measured by adding 0.5 mL of the water sample to pH strips (MERCK pH
indicator strips 6.5-10 and later Cytiva Whatman indicator paper pH 4.5 - 10). The pH
measured using the pH strips seems to fluctuate when it shouldnâ€™t, especially at the start,
where the pH was 6 for the control, lower than the pH measured at the M1, F3 well (pH 7)
and then rose to 7.5 on day 17, therefore the pH was simply measured on day 57 using a pH
electrode instead.

34 5. Methodology
5.5 Data treatment
To account for the decrease in CE concentration due to sampling and therefore removal
from the microcosms, a balance of the CE should be applied. This theoretical curve con-
firmed that the controlâ€™s trend aligned with sampling losses, providing a critical baseline
to distinguish between passive mass removal and active degradation. Without this correc-
tion, the concentration drop could misleadingly imply natural attenuation, complicating
the interpretation of treatment eï¬€icacy. The theoretical curve also acts as a quality control
check, revealing any deviations that might indicate measurement errors such as volatilisa-
tion losses of CE (refer to Table 3.1 ) or calibration issues.
The theoretical CE concentration curves was calculated using the following formula:
ğ¶ğ‘¤[ ğ‘š ğ‘”/ ğ¿] = ğ‘€ğ‘¡ [ ğ‘š ğ‘”]/( ğ‘‰ğ‘Š [ ğ¿] + ğ‘˜ğ‘‘[ 1/ ğ‘”] Â· ğ‘€ğ‘ [ ğ‘”] + ğ¾ â„ Â· ğ‘‰ğ‘)[ ğ¿] where:
ğ¶ğ‘¤ is the aqueous concentration of the chlorinated ethene
ğ‘‰ğ‘¡ğ‘œğ‘¡ is the total volume of glass flasks, 1.138L
ğ‘‰ğ‘¤ is the water volume in the system
ğ¾d is assumed to be 0.00003 1/g [ Hemdorff, 2013 ]
ğ‘€ğ‘  is sediment mass in the system, 300g
ğ¾ â„ is assumed as 0.0741 for cDCE and 0.631 for VC [ Friis, 2006 ; Gossett, 1987 ]
ğ‘‰ğ‘ is the volume of headspace in the system (determined by subtracting the volume
of sediment and water from the total volume)
The initial total mass in the system, Mt [mg] is taken as 0.150[ ğ‘š ğ‘”/ ğ¿] Â· ğ‘‰ğ‘¡ğ‘œğ‘¡ [ ğ¿] instead of
0.184[ ğ‘š ğ‘”/ ğ¿] Â· ğ‘‰ğ‘¡ğ‘œğ‘¡ [ ğ¿] to account for the fact that the groundwater was added to the control
two weeks later. Given limestoneâ€™s negligible sorption, the CE concentration before and
after limestone was not measured. ğ¾ â„ is is critical for accurate mass accounting because
cDCE continuously re-equilibrates between phases after sampling. Each removal of water
or headspace gas disturbs the systemâ€™s equilibrium, causing dissolved cDCE to partition
anew into the headspace. Without ğ¾ â„, repeated sampling would artificially depress aque-
ous concentrations - not from degradation, but from this dynamic redistribution.
For the degradation rate, the initial concentration of cDCE was assumed to be 421.19
ğœ‡ğ‘šğ‘œğ‘™ / ğ¿ (averaged for day -12) and 38.94 ğœ‡ğ‘šğ‘œğ‘™ / ğ¿ (averaged for day -0) for VC. The first-
order kinetic rate is calculated by taken by plotting the ln(C/Co) vs time and calculating
the gradient of this graph.
ğ‘Ÿ = ğ¾1,ğ‘– Â· ğ¶ğ‘– where:
r is the rate of dechlorination [ ğœ‡ğ‘šğ‘œğ‘™ğ¿ âˆ’ 1ğ‘‘ğ‘ ğ‘¦âˆ’ 1]
ğ¾1,ğ‘– is the first-order kinetic rate constant [ ğ‘‘ğ‘ ğ‘¦âˆ’ 1]
ğ¶ğ‘– is the initial CE concentration in ğœ‡ğ‘šğ‘œğ‘™ğ¿ âˆ’ 1
For cDCE day -12 to day 29 was considered and for VC day 10 to 51 was considered which
excludes the production of VC.

6
RESULTS
6.1 Physical observations
Upon mixing, all microcosms initially appeared milky or pastel-colored due to suspended
limestone particles (all figures shown immediately after mixing).
Figure 6.2 shows the GeoForm-containing batches (B and C) on the 3rd day of the ex-
periment. As predicted, GeoForm-containing batches (B and C) appear yellowish-green in
colour (shown in Figure 6.2 ), likely from dissolved ferrous sulfate monohydrate (greenish
characteristic of ğ¹ğ‘’ 2+ ) and the lecithinâ€™s yellowish-brown hue . These batches maintained
greenish tones under iron-reducing conditions; any orange/red-brown precipitates would
indicate oxidation of ferrous iron or other redox disturbances, similar to what was seen in
Figure 5.4 .
Figure 6.3a shows that by the 13th day , the GeoForm (B) and GEoForm + KB-1 (C)
batches look different to each other. Compared to GeoForm batch (B), GeoForm + KB-1 (C)
became much darker as the microcosm amended with both GeoForm and KB-1 (C) should
undergo more rapid darkening since the KB-1 culture contains SRB at at optimal concentra-
tions for iron sulphide formation compared to the GeoForm-only batch (B) which would
have a lower SRB abundance. Figure 6.4 shows the darkening of B1 by Day 38. B3 also fol-
lowed this pattern of darkening on day 57 (shown in Figure A.8 , appendix). Figure 6.4 also
shows further darkening of batch C3 (compared to day 23, Figure 6.3 ) this is confirmation
for the formation of iron sulphide. It was expected that the precipitation of iron sulphide
would occur and, this black colour of the water would then start clearing up, but this not
observed within the timeframe of the thesis.
Both the control and KB-1 only batches had colourless water ( Figure 6.1b ). Figure 6.1b
and Figure 6.3b illustrates that due to the characteristic black pigmentation of KB-1 (shown
in Figure 5.7 ), the KB-1 containing batches have darker-looking limestone, more greyish,
as that limestone provides an attachment surface for microbial colonisation. Furthermore,
both batches B and C were diï¬€icult to filter compared to control and KB-1-only batches (A
and D), but on the 13th day onwards, batch C became much easier to filter.
The control and KB-1-only batches (A and D) exhibited a more pronounced stagnant
water odour compared to GeoForm-containing batches (B and C). While GeoFormâ€™s high
35

36 6.
Results
iro
n content generated a metallic odor, the expected â€rotten eggâ€ smell from ğ»ğ‘† formation
(via SRB activity) was not observed in the first few days ( Table 3.4), suggesting rapid iron
sulphide precipitation.
(a) G
eoForm (B) compared to Control(A) (A3 is darker than A1 and A2)
(b) K
B-1 (D) compared to Control(A)
Figure 6.1: Appearance of all the microcosms on at the start of the experiment
B3                 B2                    B1              A3                 A2                A1
D3                    D2                  D1               A3               A2                 A1","6.
1. Physical observations 37
Fi
gure 6.2: On day 3 GeoForm (B) and GeoForm + KB1 (C) look similar
     B1                             B2                            B3
  C1                          C2                            C3

38 6. R
esults
(a) Ge
oForm (B) and GeoForm + KB1 (C) look different
(b) KB
-1 (D) compared to Geoform + KB-1 (C)
Figure 6.3: Comparison of the microcosms on Day 23
Figu
re 6.4: GeoForm (B) and GeoForm + KB1 (C) on day 43, B1 darkened
B3               B2              B1
C1            C2           C3
D3               D2              D1
C1            C2            C3
                    B2              B3
B1                                               C1            C2          C3

6.2. Data analysis 39
6.2 Data analysis
6.2.1 Chlorinated ethenes
The expected outcome was once the black precipitate forms, FeS should react with CE,
resulting in concurrent decreases in both cDCE and the small amount of VC seen in the
groundwater from the well. These microcosms would also exhibit greater VC formation
compared to GeoForm-only batches. As depicted in Figure 3.5 , KB-1-amended microcosms
should demonstrate sequential degradation of CE, with cDCE degradation preceding VC
degradation. However, in microcosms containing both GeoForm and KB-1, the CE degra-
dation pattern would depend on the dominant processes. If Dehalococcoides remain uninhib-
ited, the pattern would resemble KB-1-only systems. Conversely , if Dehalococcoides activity
is suppressed by elevated iron, sulphate, or sulphide concentrations, the degradation pro-
file would mirror GeoForm-only microcosms, exhibiting simultaneous decreases in cDCE
and VC.
The observed concentration profiles revealed distinct degradation behaviours across ex-
perimental batches. In GeoForm-amended microcosms (B and C), the initial and final cDCE
concentrations showed minimal difference, with fluctuations exceeding those observed in
the control (about 40ğœ‡ğ‘”/ ğ¿) Figure 6.5 . While GeoForm+KB-1 batch (C) demonstrated some
degradation capacity , their eï¬€icacy remained substantially lower than KB-1-only systems
(Batch D) ( Figure 6.5 ). Notably , the GeoForm+KB-1 batch (C) exhibited VC formation fol-
lowed by partial degradation and ethene production, though at significantly reduced levels
compared to KB-1-only systems (D) ( Figure 6.6 , Figure A.21 ). This suggests inhibitory
effects from elevated iron, sulphate, and/or sulphide concentrations on Dehalococcoides ac-
tivity in combined-treatment microcosms. As anticipated, GeoForm-only batches (B) had
very little VC (around 0.2ğœ‡ğ‘”/ ğ¿), while control systems (A) maintained stable cDCE con-
centrations throughout the experiment, confirming the lack of Dehalococcoides in Naverland.
The peak seen in the cDCE concentration at Day 0 for KB-1 batches (D) is likely because of
the second addition of KB-1( Figure 6.5 ). Figure 6.7 demonstrates the expected stepwise
degradation sequence: first the decrease in cDCE concentration and then an accumulation
of VC and finally degradation of VC to ethene in the KB-1 only batches (D). The formation
of VC on Day 7 ( Figure 6.6 ) and its degradation after day 38 observed in GeoForm +KB-1
microcosms (C) confirms biotic degradation as the dominant pathway albeit operating un-
der partial inhibition from GeoForm as shown by previous studies ( Section 3.2.3 ) . The VC
concentrations in the GeoForm +KB-1 microcosm (C) remained below 1 Î¼g/L throughout
the experiment ( Figure 6.6 . However, this limited VC accumulation occurred without an
obvious corresponding decrease in cDCE concentrations.

40 6. Results
Figure 6.5: Average cDCE concentration of all the batches over time. On day -22, ELS was added and day
-12, KB-1 was added only to batch D (KB-1) and then on day 0, GeoForm was added to batch B and GeoForm
and KB-1 was added to batch C
Figure 6.6: Average VC levels and ethene level in the control, GeoForm and GeoForm+KB-1 batches. The
ethene level was only for day 51

6.2. Data analysis 41
Figure 6.7: Average chlorinated ethenes and ethene concentration in KB-1 batch. ELS was added on day -22,
and KB-1 was added -12 and day 0
The only batch that showed degradation is KB-1, and itâ€™s average first-order rate constant
is 0.1252 day âˆ’ 1 for cDCE ( Figure 6.8 ) and 0.1085 day âˆ’ 1 for VC ( Figure 6.9 ). The rate of
dechlorination is 52.73 ğœ‡ğ‘šğ‘œğ‘™ğ¿ âˆ’ 1ğ‘‘ğ‘ ğ‘¦âˆ’ 1 which is faster than the enhanced dechlorination seen
by Yaru Li et al., 2021 who reported that the addition of 0.2 mM ğ¹ğ‘’ 2+ and ğ‘†2 enhanced
the dechlorination rate of TCE from 25.46 Â± 1.15 to 37.84 Â± 1.89 Î¼molÂ· ğ¿1 Â· ğ‘‘ğ‘ ğ‘¦1 in a batch
experiment. While the VC dechlorination observed is slower, the dechlorination rate is 4.22
ğœ‡ğ‘šğ‘œğ‘™ğ¿ âˆ’ 1ğ‘‘ğ‘ ğ‘¦âˆ’ 1.

42 6. Results
Figure 6.8: cDCE degradation rate in the KB-1 batch

6.2. Data analysis 43
Figure 6.9: VC degradation rate in the KB-1 batch

44 6. Results
6.2.2 Volatile fatty acids
Figure 6.10 shows a large difference in acetate concentration in A3 compared to A1 and
A2 (control), which explains its much darker colour (shown in Figure 6.1a ), suggesting
the presence of organic matter in batch A3 however as there is no Dehalococcoides, the VFAs
served as electron donor for indigenous bacteria and its the decreasing acetate concentration
from Day 29 to 38 indicate it is being consumed.
Although all the batches excluding the controls contained equal masses of ELS, the
GeoForm-containing batches (B and C) exhibited acetate levels nearly ten times higher than
the KB-1-only batches (D), indicating that GeoForm may contribute to this increase ( Fig-
ure 6.11, Figure 6.12 , and Figure 6.13 ).
Furthermore, amongst the Geoform-containing batch, the batch with KB-1 produced
slightly more acetate at a higher rate than batch B, likely due to the presence of KB-1 culture,
which could enhance ELS fermentation. C3 has the fastest VFA production and had the
fastest VC production ( Figure A.20 ) However, as there is continuous production of VFA, it
is hard to say if it is being consumed or accumulated. If there is accumulation of VFA in the
GeoForm-containing batches, it would indicate ELS Microemulsion is being broken down
to acetate but not being consumed by bacteria ( Figure 6.11 , Figure 6.12 , and Figure 6.13 ).
if the experiment could have been run for longer the VFA level of C3 (GeoForm + KB-1)
could be observed to figure out whether VFA production stopped, if it is just a fluctuation
or whether it is being consumed ( Figure 6.12 ). This batch (C3) also has the Batch B3 (Ge-
oForm) showed lower VFAs levels than B1 and B2 ( Figure 6.11 ).
And as stated in Section 5.4.3 , all the microcosms have organic matter. Amongst the
KB-1 only batches, D1 produced acetate faster than D2 and D3 and these batches exhib-
ited acetate consumption after day 20 ( Figure 6.13 ). The consumption of acetate indicates
acetate served as an electron donor for the Dehalococcoides and stimulated reductive dechlo-
rination of cDCE and VC and cooperative interaction between the microbes fermenting ELS
(and the natural organic matter), and the Dehalococcoides .
Finally , when lactic acid (a non-volatile fatty acid) was measured in all batches starting
from day 38 (unintentionally), none exceeded the detection limit of 0.050 g/L. As the pH of
the system is >5.5, the VFA is acetate instead of acetic acid. It is to be noted that other than
acetate, no VFAs were detected despite being measured.

6.2. Data analysis 45
Figure 6.10: VFA levels in the controls over the 38 days
Figure 6.11: acetate levels in the GeoForm batches over the 38 days
Figure 6.12: acetate levels in the GeoForm + KB-1 batches over the 38 days

46 6. Results
Figure 6.13: acetate levels in the KB-1 batches over the 38 days
6.2.3 Iron
The dissolved iron concentration in the control and KB-1 (A and D) batches was close to the
detection limit and therefore, there is no pattern seen. Batches containing GeoForm only
(B) started at 900 âˆ’ 940 mg/L and then decreased over time, except for B2, which spiked
after two weeks. This indicates that the GeoForm-containing microcosms are at least iron-
reducing conditions and once again, suitable for the microbes. Like batch B, batches with
both GeoForm and KB-1 (batch C) started with an Fe concentration of 820 âˆ’ 900 mg/L and
then decreased over time, which could be the formation of iron sulphide since, as previously
mentioned, the colour darkens after day 13 (Figure 6.3a ). As there is 2.68 g of Ferrous sulfate
monohydrate, there should be 1.36 g of iron but only 810 mg and 738 mg (B and C respec-
tively) was found the remaining 553 mg and 623 mg is because Ferrous sulfate monohydrate
which is not as immediately soluble as the ferrous sulfate heptahydrate. Therefore, the fer-
rous iron and sulfate are more slowly released and tend to provide a continuous source
of both sulfate and iron for maybe a couple of months depending on the water movement
and rate of diffusion (which is somewhat controlled by the rate of sulfate reduction). The
spike since on day 1 for C1 is not because the conditions were not reduced enough but the
dissolving of Fe ( Figure 6.15 ).

6.2. Data analysis 47
Figure 6.14: Average dissolved iron in the biotic control and KB-1 batches over time
Figure 6.15: Dissolved iron in the GeoForm containing batches over time (note the axis)

48 6. Results
6.2.4 Sulphate
The increasing concentration of sulphate in B2 (GeoForm) (shown in Figure 6.17 ) as well as
the spike of dissolved iron concentration after two weeks ( Figure 6.15 ), illustrates that this
microcosm is not in sulphate reducing conditions and that the Ferrous Sulphate monohy-
drate is dissolving over time. The clear decreasing sulphate concentration of A3 (control),
C3 (GeoForm +KB-1) and D (KB-1) batches shows that these microcosms are in sulphate
reducing conditions ( Figure 6.16 , Figure 6.18 , Figure 6.19 ). As mentioned in Figure 6.4 ,
batch B1 darkened and at the same time as when the sulfate concentration started to dip,
proving the microcosm has sulphate-reducing conditions. As expected, the GeoForm+KB-
1 batches reached sulphate-reducing conditions faster than the batches with only GeoForm
(Figure 6.18 ). C1 (GeoForm +KB-1) also shows a steep decrease in sulfate concentration
from the 29th to 38th day but once again, it is diï¬€icult to assess whether it is a fluctuation
or not ( Figure 6.18 ).
The results also from KB-1 batches confirm that KB-1 has SRB and sulphate reducing con-
ditions ( Figure 6.19 ). The sulphide concentration on day 57 is shown in Table 6.1 . Despite
evidence of sulphate reduction across multiple batches, sulphide concentrations remained
low in most, except for C3. This disparity cannot be attributed to sulphide volatilisation or
measurement limitations, as sulphide is detectable in at least one batch. The more plausible
explanation is that redox conditions in the other microcosms were not suï¬€iciently reducing
to allow for consistent sulphide generation. Competing electron acceptors (Fe) may have
delayed or limited the onset of sulphate-reducing conditions. The decreasing sulphate con-
centration for A3, high sulphide concentration and consumption of acetate indicates there
are SRB and the presence of natural organic matter (as shown by the high NVOC), could be
from the limestone collected. Additionally , the dark colour of C3 confirms the formation of
iron sulphide and therefore indicates potential abiotic cDCE degradation however this was
not observed but the redox conditions are ideal for the Dehalococcoides present in the bottle.
As there is approximately 783 mg of Fe and 2187 mg of sulphate, the ratio is 1:1.6 Fe:SO4
so it is more likely that mackinawite forms rather than pyrite. The measured sulfate con-
centration (â‰ˆ2500 mg/L) substantially exceeded the expected 890 mg/L ( Table A.1 ). Nev-
ertheless, the system did not deplete its electron donor capacity during the experimental
period.
As shown in Figure 6.19 , the triplicates for the KB-1 are similar to each other and therefore
the results are reliable.
Table 6.1: Sulphide concentration [mg/L] in all batches on day 57. â€ndâ€ indicates below detection limit
Replicate A (Biotic control) B (GeoForm) C (GeoForm + KB-1) D (KB-1)
1 nd 0.066 0.381 3.39
2 nd 0.080 0.169 >3.5
3 3.93 0.041 1.43 >3.5
Average 1.31 0.062 0.66 >3.46

6.2. Data analysis 49
Figure 6.16: Sulphate concentration over time in the biotic controls
Figure 6.17: Sulphate concentration over time in the GeoForm batches (note the axis)

50 6. Results
Figure 6.18: Sulphate concentration over time in the GeoForm + KB-1 batches (note the axis)
Figure 6.19: Sulphate concentration over time in the KB-1 batches

6.2. Data analysis 51
6.2.5 Redox parameters
Nitrate
The lowest standard for nitrate was 15 mg/L and therefore there might not be nitrate as the
levels are between 0 and 3 mg/L especially since the microcosms are more reduced than
nitrate-reducing conditions. ( Figure A.23 ,Figure A.24 , Figure A.25 and Figure A.26 in the
appendix) .
Manganese
None of the controls or the batches with only KB-1 had Mn >0.5 mg/L (the detection limit);
therefore, the only detections are for the batches with GeoForm which had Mn that was
decreasing over time as the redox conditions are more than manganese-reducing. (The
mean dissolved Mn concentration is in the appendix, Figure A.22 .)
6.2.6 pH
Figure 6.20 shows the mean pH over time for each scenario, showing that though the pH
fluctuates, it does not go below 6, or above 8 and therefore remained in a favourable range
for microbes.
Figure 6.20: average pH over time

7
DISCUSSION AND PERSPECTIVES
The main objective of this research is to advance the understanding of chlorinated solvent
remediation in chalk aquifers, with particular focus on the cDCE contamination plume at
Naverland 26AB in Albertslund. This investigation pursues four primary goals: first, to
evaluate the feasibility of using iron-bearing minerals (BiRD) and ERD for cDCE remedia-
tion in limestone aquifers; to develop and conduct batch experiments quantifying degrada-
tion rates of both cDCE and VC; to analyse these experimental results in the context of ex-
isting literature and site-specific conditions to identify key factors influencing degradation;
and to assess the effectiveness of BiRD and ERD approaches, culminating in recommenda-
tions for pilot-scale implementation.
7.1 BiRD
BiRD was considered a potentially viable remediation strategy for the Naverland site based
on several site-specific and conceptual factors. cDCE as the primary contaminant in the
plume aligns with the known reactivity of iron minerals, which have been shown to abi-
otically degrade CE without the formation of VC under reducing conditions. As noted by
Darlington and Rectanus, 2015 ; Y. T. He et al., 2015 , effective ISBGT requires a balance of
high sulfate loading, the presence of iron oxides, and suï¬€icient organic carbon, while the
Naverland site contains relatively low natural iron concentrations (1.67 to 2.56 mg/L), its
has sulfate (100 mg/L) and organic matter (fermented to provide electron donors), com-
bined with the potential for iron supplementation (like GeoFormÂ® Soluble), would create
favourable conditions for implementing remediation strategies. Moreover PÃ³sfai et al., 2006
reported that FeS plays a crucial role in biogeochemical processes by mediating long dis-
tance extracellular electron transfer.
In contrast, based on the experimental results, BiRD would show limited effectiveness of
cDCE remediation in the Naverland site. BiRD would be not supported under the available
time-frame and test conditions. Clearly demonstrating that the recommended hydraulic
retention time of 15 to 30 days for FeS formation and dechlorination by Darlington and
52

7.2. BiRD + ERD 53
Rectanus, 2015 is too short even with SRB supplementation. The critical constraint emerged
from delayed FeS formation, which only occurred on the day 38 (for GeoForm) which led
to insuï¬€icient time to measurably contribute to cDCE degradation. This delay likely reflects
inadequate reducing conditions in the GeoForm-only microcosms. While these findings do
not invalidate BiRDâ€™s theoretical basis, they underscore its dependence on sustained redox
control, necessitating more aggressive engineering of reducing conditions for itâ€™s applica-
tion. While it would have been methodologically advantageous to bypass the iron sulphide
formation step (a key rate-limiting process in this system), the approach of using powdered
FeS nor freeze-dried mackinawite according to Jeong et al., 2011 wouldnâ€™t have exhibited
reactivity with CE, necessitating the in situ formation of reactive iron sulphides .
As previous studies that demonstrate iron sulphide minerals can dechlorinate CE (e.g. But-
ler et al., 1999) involved well-established FeS from the the start which makes comparing this
experiment with literature diï¬€icult .
7.2 BiRD + ERD
In the batches with KB-1 and GeoForm, there was VC formation, confirming that biotic
dechlorination by Dehalococcoides was possible rather than abiotic degradation, where it is
simultaneous degradation of all the CE present [ Darlington and Rectanus, 2015 ] . Though
as KB-1 contains SRB and there was faster formation of FeS, no significant decrease in cDCE
was observed within the time-frame, especially when compared to the KB-1-only batch. re-
generating FeS, the enrichment of Desulfovibrio, a SRB that can reduce ğ‘†ğ‘‚ 2
4 and ğ¹ğ‘’ 3+ may
enhance abiotic PCE-to-acetylene dechlorination[ Liu et al., 2023 ]. This suggests potential
inhibition from competing processes, such as high iron or sulphate levels which is sup-
ported by prior studies [ Yaru Li et al., 2021 and Liu et al., 2023 ] which indicated that high
FeS concentrations can inhibit Dehalococcoides. However, direct comparison for Dehalococ-
coides inhibition before the FeS formation is diï¬€icult because this system contained much
higher iron levels (800 mg/L) than those tested by Yoshikawa et al., 2021 (28 mg/L) which
observed that ferrous iron shortened the time required for complete dechlorination. So this
experiment did not show synergistic effects of the combination of BiRD and ERD. Addition-
ally , in the batches where FeS should have mediated degradation, the expected sequence
involves: (1) initial blackening of the water due to FeS formation, followed by (2) precipi-
tation and settling of FeS, resulting in clarified water. In this experiment, the batches with
GeoForm + KB-1, only showed the first step, blackening of the water.
7.3 ERD
ERD was also considered a promising approach, particularly due to the potential for com-
plete dechlorination of cDCE and VC in the presence of Dehalococcoides. Unlike ZVI, which
is particulate and dependent on surface area for reactivity (often leading to distribution
challenges and potential aquifer clogging), ERD relies on dissolved-phase amendments

54 7. Discussion and Perspectives
(dissolved electron donors and microbial culture), allowing for more effective delivery in
fractured limestone systems.
The ERD trials using KB-1 alone demonstrated the best results, achieving complete
cDCE reduction to VC within 46 days, followed by near-total VC degradation to trace levels
(0.1â€“0.2 Âµg/L) and ethene formation within 73 days. The observed dechlorination indicate
the presence of the vcrA and bvcA genes in Dehalococcoides [MÃ¼ller et al., 2004 ; Krajmalnik-
Brown et al., 2004 ]. This effective dechlorination likely benefited from the limestone aquiferâ€™s
natural buffering capacity , which maintained a stable pH of 6.99â€”optimal for sustaining
both SRB and Dehalococcoides activity in the KB-1 consortium. The siteâ€™s existing sulphate
and organic carbon levels suggest that only minimal supplementation ( Dehalococcoides )
would be required to stimulate microbial activity. According to Mao et al., 2017 , environ-
ments with high sulphate had no effect on Dehalococcoides (strain 195) but inhibited its yield
by about 65% in sulphide environments, indicating the addition of iron to form FeS could
help regulate sulphide levels and create redox conditions more favourable for Dehalococ-
coides. In low concentrations, FeS has even been shown to enhance ERD by facilitating elec-
tron transfer and supporting microbial metabolism.
One of the aims of this thesis is to calculate the degradation rate for all the microcosms.
However degradation was seen only in the batch with KB-1, which was 0.1251 day âˆ’ 1 for
cDCE and 0.1302 day âˆ’ 1 for VC. Even if the experiment with GeoForm and GeoForm + KB-1
was extended, the potential abiotic degradation rates would not compete with the rapid
biotic dechlorination demonstrated by KB-1 culture. The superior performance of ERD was
evident in these experiments. This suggests ERD implementation would be more reliably
effective for field applications.
7.4 Pilot-scale implementation
For pilot-scale implementation in limestone aquifers, ERD using KB-1 or similar cultures
represents the most promising approach. Field deployment should incorporate measures
such as supplying enough KB-1 and electron donors (e.g., lactate or ELS Microemulsion)
should be injected directly into the aquifer, with recirculation to enhance contact between
amendments and contaminants. This is particularly critical in limestone aquifers, where
fracture-dominated flow may limit mixing. Monitoring protocols would need to be estab-
lished to assess both short-term performance and long-term sustainability of the remedi-
ation effort. Parameters would include redox-sensitive species such as sulfate and ferrous
iron, concentrations of CE, and VFAs to ensure enough electron donors are supplied if the
NVOC levels in the Naverland site are different to the microcosm test done. The specific
conditions at Naverland, including the chalk aquiferâ€™s depth and carbonate buffering ca-
pacity , appear generally favorable for ERD implementation. Previous failures of BiRD in

7.5. Perspectives 55
field applications [ Darlington, Lehmicke, et al., 2013 ], further support the recommenda-
tion to prioritize ERD for this specific site.
7.5 Perspectives
Future iterations of this study should incorporate sediment material from the Naverland
site into microcosms prepared under strictly anaerobic conditions (e.g., within an anaerobic
glovebox) to better replicate the iron-reducing to sulfate-reducing conditions relevant for
both ERD and BiRD processes. Although methanogenic conditions are theoretically ideal
for ERD, SÃ¸rensen, 2013 did not observe methane production, suggesting SRB outcompet-
ing methanogenic archaea for hydrogen and acetate [ Stams, 1994 ; Muyzer et al., 2008 ]. The
inclusion of site-specific sediments would enhance the representativeness of redox dynam-
ics by preserving in situ microbial communities and geochemical profiles. Particular care
must also be taken to prevent oxygen introduction during groundwater collection to main-
tain optimal redox conditions from the start of the experiment.
A more comprehensive characterisation of solid phases would significantly improve
process understanding. This could involve: (1) quantitative analysis of sediment samples
for total iron and sulfide content to verify precipitation dynamics; (2) selective sampling
of precipitates through centrifugation or filtration of the aqueous phase; and (3) detailed
characterization of collected solids using stoichiometric analysis to differentiate between
iron sulphide phases (e.g., mackinawite (FeS) versus pyrite (FeS2)) or the use of X-ray
diffraction to confirm the formation of these minerals.
The experimental duration should be extended beyond the current 38-day time-frame,
with a minimum 60-day period recommended to allow for complete establishment of re-
ducing conditions and subsequent degradation processes. This modification is particularly
warranted given the delayed onset of iron sulphide precipitation observed in the current
study. Additional analytical improvements would include: (1) measuring the CE before
adding to the microcosm bottles to establish more accurate baseline conditions and enable
better kinetic modelling; and (2) measure the NVOC concentration at the start of the exper-
iment to see if it follows the VFAs levels and if not conclude that there is enough organic
matter instead of measuring NVOC throughout the experiment. Conducting the experi-
ment at room temperature (approximately 20 to 25 â—¦ğ¶) rather than 10 â—¦ğ¶ would likely have
accelerated key processes. In the GeoForm + KB-1 batch, the established iron sulphide
(FeS) would have promoted faster dechlorination through both biotic (Dehalococcoides)
and abiotic (FeS-mediated) pathways due to enhanced reaction kinetics at elevated temper-
atures. Similarly , the lag phase observed in GeoForm-only batches, which was attributable
to slow microbial acclimation at 10 â—¦ğ¶, would probably have been shorter given the known
temperature dependence of microbial metabolic rates.

8
CONCLUSION
This thesis investigated the potential for Biogeochemical Reductive Dechlorination (BiRD)
and Enhanced Reductive Dechlorination (ERD) to decontaminant the primarily cis-1,2-
Dichloroethylene (cDCE) plume in a fractured limestone aquifer, using the Naverland site
in Denmark as a cast study. Through a a comprehensive evaluation of available literature
and laboratory-scale batch experiments, this study compared the effectiveness of Moni-
tored Natural Attenuation (MNA), GeoFormÂ® Soluble, a ferrous sulphate-based amend-
ment, KB-1Â®, a consortium containing Dehalococcoides, and a combination of KB-1Â® and
GeoformÂ®, to degrade cDCE and Vinyl Chloride (VC) under under site-relevant condi-
tions. The results indicate that ERD using KB-1Â® was the most effective remediation strat-
egy , achieving complete dechlorination of cDCE to ethene with high degradation rates of
52.73 ğœ‡ğ‘šğ‘œğ‘™ğ¿ âˆ’ 1ğ‘‘ğ‘ ğ‘¦âˆ’ 1 on average while BiRD despite the theoretical promise, showed limited
effectiveness within the given time-frame, mainly due to the delayed formation of reactive
iron sulphide (FeS) minerals as the microcosms were not suï¬€iciently reduced. When KB-1Â®
and GeoformÂ® was combined, the Sulfate-Reducing Bacterium (SRB) in KB-1Â® accelerated
the production of FeS , but insuï¬€icient precipitation occurred to enable abiotic degradation.
Consistent with previous studies, the resulting high iron sulphide concentration inhibited
Dehalococcoides activity , slowing reductive dechlorination rather than showing synergistic
effects. Nevertheless, the formation of VC and ethene confirmed that biotic degradation
persisted, albeit at reduced rates. Therefore given the redox conditions, buffering capacity
of limestone, and other site-specific factors, ERD using KB-1 and recirculation to enhance
amendment delivery in the fracture for field-scale applications. Future work could focus on
incorporating native sediments, avoiding oxygen introduction during groundwater collec-
tion to maintain optimal redox conditions from the start of the experiment and extending
monitoring periods from 38 days to 60 days to capture long-term degradation dynamics.
Ultimately , this study contributes valuable insights into the practical challenges and op-
portunities of treating chlorinated solvent plumes in limestone aquifers and supports the
broader application of ERD as a robust remediation approach.
56

This page intentionally left blank.

BIBLIOGRAPHY
Andersen, Thomas Breum and Mads Robenhagen MÃ¸lgaard (2018). Copenhagen Area Overview of
the geological conditions in the Copenhagen area and surrounding areas . Tech. rep. GeoAtlas Live. URL:
https://wgn.geo.dk/geodata/modeldokumentation/Copenhagen%20_25m_2018-07-12.pdf .
Aziz, Carol E., Ryan A. Wymore, and Robert J. Steffan (2013). â€œBioaugmentation Considerationsâ€.
In: Bioaugmentation for Groundwater Remediation . Ed. by Hans F. Stroo, Andrea Leeson, and C. Herb
Ward. New York, NY: Springer New York, pp. 141â€“169. ISBN: 978-1-4614-4115-1. DOI: 10.1007/978-
1-4614-4115-1_5 .
Ballapragada, Bhaskar S., H. David Stensel, J. A. Puhakka, and John F. Ferguson (1997). â€œEffect of
Hydrogen on Reductive Dechlorination of Chlorinated Ethenesâ€. In: Environmental Science & Tech-
nology 31.6, pp. 1728â€“1734. DOI: 10.1021/es9606539.
Bradley , Paul and Francis Chapelle (May 2010). â€œBiodegradation of Chlorinated Ethenesâ€. In: DOI:
10.1007/978-1-4419-1401-9_3 .
Brigmon, Robin, Nathan Bell, David Freedman, and Christopher Berry (July 1998). â€œNatural At-
tenuation of Trichloroethylene in Rhizosphere Soils at the Savannah River Siteâ€. In: Journal of Soil
Contamination - J SOIL CONTAM 7, pp. 433â€“453. DOI: 10.1080/10588339891334429.
Broholm, Mette M., Gry S. Janniche, Klaus Mosthaf, Annika S. FjordbÃ¸ge, Anders G. Binning Philip
J.and Christensen, Bernt Grosen, Torben H. JÃ¸rgensen, Carl Keller, Gary Wealthall, and . Henriette
Kerrn-Jespersen (2016). â€œCharacterization of chlorinated solvent contamination in limestone using
innovative FLUTeÂ® technologies in combination with other methods in a line of evidence approachâ€.
In: Journal of Contaminant Hydrology 189, pp. 68â€“85. ISSN: 0169-7722. DOI: https : / / doi . org / 10 .
1016/j.jconhyd.2016.03.007 . URL: https://www.sciencedirect.com/science/article/pii/
S0169772216300444.
Bromley , Richard G. (1979). â€œChalk and Bryozoan limestone: facies, sediments and depositional en-
vironmentsâ€. In: Institute of Historical Geology and Palaeontology, Ã˜ster Voldgade 1 0 , DK-13 50 Copen-
hagen K . URL: https : / / paleoarchive . com / literature / Birkelund & Bromley1979 - 03 - Bromley -
ChalkBryozoanLimestone.pdf.
Bronzetti, Giorgio, Carlo Bauer, Claudio Corsi, Renata Del Carratore, Alvaro Galli, Riccardo Nieri,
Moreno Paolini, Enrico Cundari, Giorgio Cantelli Forti, and John Crenshaw (1984). â€œComparative
genetic activity of cis- and trans-1,2-dichloroethylene in yeastâ€. In: Teratogenesis, Carcinogenesis, and
Mutagenesis 4.4, pp. 365â€“375. DOI: https://doi.org/10.1002/tcm.1770040406.
Brusseau, M.L. and J. Chorover (2019). â€œChapter 8 - Chemical Processes Affecting Contaminant
Transport and Fateâ€. In: Environmental and Pollution Science (Third Edition) . Ed. by Mark L. Brusseau,
59

60 BIBLIOGRAPHY
Ian L. Pepper, and Charles P . Gerba. Third Edition. Academic Press, pp. 113â€“130. ISBN: 978-0-12-
814719-1. DOI: https://doi.org/10.1016/B978-0-12-814719-1.00008-2 .
Butler, Elizabeth C. and Kim F. Hayes (1999). â€œKinetics of the Transformation of Trichloroethylene
and Tetrachloroethylene by Iron Sulfideâ€. In: Environmental Science & Technology 33.12, pp. 2021â€“2027.
DOI: 10.1021/es9809455.
Chen, Fei, David L. Freedman, Ronald W. Falta, and Lawrence C. Murdoch (2012). â€œHenryâ€™s law
constants of chlorinated solvents at elevated temperaturesâ€. In: Chemosphere 86.2, pp. 156â€“165. ISSN:
0045-6535. DOI: https://doi.org/10.1016/ j.chemosphere.2011.10.004 . URL: https://www.
sciencedirect.com/science/article/pii/S0045653511011453.
Chen, Yaoning, Weiyu Liang, Yuanping Li, Yanxin Wu, Yanrong Chen, Wei Xiao, Li Zhao, Jiachao
Zhang, and Hui Li (2019). â€œModification, application and reaction mechanisms of nano-sized iron
sulfide particles for pollutant removal from soil and water: A reviewâ€. In: Chemical Engineering Jour-
nal 362, pp. 144â€“159. ISSN: 1385-8947. DOI: https://doi.org/10.1016/j.cej.2018.12.175 . URL:
https://www.sciencedirect.com/science/article/pii/S1385894718326615.
Chilingar, G. V . and B. Endres (Jan. 1, 2005). â€œEnvironmental hazards posed by the Los Angeles
Basin urban oilfields: an historical perspective of lessons learnedâ€. In: Environmental Geology 47.2,
pp. 302â€“317. ISSN: 1432-0495. URL: https://doi.org/10.1007/s00254-004-1159-0 .
Chiou, Cary T. and Daniel E. Kile (1998). â€œDeviations from Sorption Linearity on Soils of Polar
and Nonpolar Organic Compounds at Low Relative Concentrationsâ€. In: Environmental Science &
Technology 32.3, pp. 338â€“343. DOI: 10.1021/es970608g.
Christ, John A., C. Andrew Ramsburg, Linda M. Abriola, Kurt D. Pennell, and Frank E. LÃ¶ffler
(2005). â€œCoupling Aggressive Mass Removal with Microbial Reductive Dechlorination for Reme-
diation of DNAPL Source Zones: A Review and Assessmentâ€. In: Environmental Health Perspectives
113.4, pp. 465â€“477. DOI: 10.1289/ehp.6932.
Chu, Wei and Kwai-Hing Chan (2000). â€œThe prediction of partitioning coeï¬€icients for chemicals
causing environmental concernâ€. In: Science of The Total Environment 248.1, pp. 1â€“10. ISSN: 0048-9697.
DOI: https://doi.org/10.1016/S0048-9697(99)00472-6 .
Cichocki, Joseph A., Kathryn Z. Guyton, Neela Guha, Weihsueh A. Chiu, Ivan Rusyn, and Lawrence
H. Lash (Oct. 1, 2016). â€œTarget Organ Metabolism, Toxicity , and Mechanisms of Trichloroethylene
and Perchloroethylene: Key Similarities, Differences, and Data Gapsâ€. In: The Journal of Pharmacology
and Experimental Therapeutics 359.1. Publisher: Elsevier, pp. 110â€“123. ISSN: 0022-3565. DOI: 10.1124/
jpet.116.232629. URL: https://doi.org/10.1124/jpet.116.232629 (visited on 01/21/2025).
COWI (2023). VideregÃ¥ende forureningsundersÃ¸gelse - Naverland 26AB. Rapport, 118 sider. Tech. rep.
COWI.
COWI (n.d.). â€œForslag til undersÃ¸gelser i Naverland fanen â€“ oplÃ¦g til mÃ¸de den 1 oktober. Udkast
(25. september 2024)â€. 2024.
Cwiertny , David M. and Michelle M. Scherer (2010). â€œAbiotic Processes Affecting the Remediation of
Chlorinated Solventsâ€. In: In Situ Remediation of Chlorinated Solvent Plumes . Ed. by H.F. Stroo and C.H.
Ward. New York, NY: Springer New York, pp. 69â€“108. ISBN: 978-1-4419-1401-9. DOI: 10.1007/978-1-
4419-1401-9_4.

BIBLIOGRAPHY 61
Damgaard, Jesper, Bernt Grosen, Kerim Martinez, Nielsen Ole Frits, Torben H. JÃ¸rgensen, Gry Sander
Janniche, Mette Broholm, Henriette Kerrn-Jespersen, and Gary P . Wealthall (2012). Geologisk, Geofy-
sisk og Hydrologisk Karakterisering pÃ¥ Naverland 26; Opstilling af Hydrogeologisk Konceptuel Model . Tech.
rep. COWI.
Darlington, Ramona, Leo G. Lehmicke, Richard G. Andrachek, and David L. Freedman (2013).
â€œAnaerobic abiotic transformations of cis-1,2-dichloroethene in fractured sandstoneâ€. In: Chemo-
sphere 90.8, pp. 2226â€“2232. ISSN: 0045-6535. DOI: https : / / doi . org / 10 . 1016 / j . chemosphere .
2012.09.084. URL: https://www.sciencedirect.com/science/article/pii/S0045653512012258.
Darlington, Ramona and Heather Rectanus (2015). Biogeochemical transformation handbook . Tech. rep.
Naval facilities Engineering command NA VFAC. URL: https://www.enviro.wiki/images/7/78/
Darlington-2015-Biogeochem_Transformation_Handbook.pdf .
Ding, Xiang-Hong, Shi-Jin Feng, and Zhang-Wen Zhu (2025). â€œAnalytical model for reactive con-
taminant back-diffusion from low-permeability aquitards with internal source zonesâ€. In: Journal of
Hydrology 646, p. 132306. ISSN: 0022-1694. DOI: https://doi.org/10.1016/j.jhydrol.2024.132306.
URL: https://www.sciencedirect.com/science/article/pii/S0022169424017025.
Dong, Haoran, Long Li, Yue Lu, Yujun Cheng, Yaoyao Wang, Qin Ning, Bin Wang, Lihua Zhang,
and Guangming Zeng (2019). â€œIntegration of nanoscale zero-valent iron and functional anaerobic
bacteria for groundwater remediation: A reviewâ€. In: Environment International 124, pp. 265â€“277.
DOI: 10 . 1016 / j . envint . 2019 . 01 . 030. URL: https : / / www . scopus . com / inward / record . uri ?
eid = 2 - s2 . 0 - 85060051787 & doi = 10 . 1016 % 2fj . envint . 2019 . 01 . 030 & partnerID = 40 & md5 =
6f60411cf8b793ef5bcf0aa112580478.
Dreher, Eberhard-Ludwig, Klaus K. Beutel, John D. Myers, Thomas LÃ¼bbe, Shannon Krieger, and
Lynn H. Pottenger (2014). â€œChloroethanes and Chloroethylenesâ€. In: Ullmannâ€™s Encyclopedia of In-
dustrial Chemistry . John Wiley & Sons, Ltd, pp. 1â€“81. ISBN: 9783527306732. DOI: https://doi.org/
10.1002/14356007.o06_o01.pub2.
Duhamel, Melanie, Kaiguo Mo, and Elizabeth A. Edwards (2004). â€œCharacterization of a Highly
Enriched Dehalococcoides Containing Culture That Grows on Vinyl Chloride and Trichloroetheneâ€.
In: Applied and Environmental Microbiology 70.9, pp. 5538â€“5545. DOI: 10 . 1128 / AEM . 70 . 9 . 5538 -
5545.2004.
EVONIK, Active Oxygens (n.d.[a]). GeoFormÂ®: Biogeochemical Reagent for Soil and Groundwater Reme-
diation. urlhttps://regenesis.com/en/geoform/.
EVONIK, Active Oxygens (n.d.[b]). accessed 12.03.2025. URL: https://active- oxygens.evonik.
com/en/products-and-services/soil-and-groundwater-remediation/geoform-reagents .
Ferrey , Mark L., Richard T. Wilkin, Robert G. Ford, and John T. Wilson (2004). â€œNonbiological
Removal of cis-Dichloroethylene and 1,1-Dichloroethylene in Aquifer Sediment Containing Mag-
netiteâ€. In: Environmental Science & Technology 38.6. PMID: 15074684, pp. 1746â€“1752. DOI: 10.1021/
es0305609.
Friis, Anne Kirketerp (2006). â€œThe potential for reductive dechlorination after thermal treatment of
TCE-contaminated aquifersâ€. PhD thesis. Technical University of Denmark.
Galizia, D. Audrey and Charles Thompson (2010). Toxicological review of cis-1,2-dichloroethylene and
trans-1,2-dichloroethylene. Tech. rep. Integrated Risk Information System (IRIS) U.S. Environmental
Protection Agency. eprint: https://iris.epa.gov/static/pdfs/0418tr.pdf.

62 BIBLIOGRAPHY
Garbarini, Doug R. and Leonard W. Lion (1986). â€œInfluence of the nature of soil organics on the sorp-
tion of toluene and trichloroethyleneâ€. In: Environmental Science & Technology 20.12. PMID: 22229435,
pp. 1263â€“1269. DOI: 10.1021/es00154a013.
Gejl, R.N., M. Rygaard, H.J. Henriksen, J. Rasmussen, and P .L. Bjerg (2019). â€œUnderstanding the
impacts of groundwater abstraction through long-term trends in water qualityâ€. In: Water Research
156, pp. 241â€“251. ISSN: 0043-1354. DOI: https://doi.org/10.1016/j.watres.2019.02.026 . URL:
https://www.sciencedirect.com/science/article/pii/S0043135419301514.
GEUS (n.d.). The Geological Survey of Denmark and Greenland, The GEUS Jupiter database, url =
geus.dk.
Goodman, Julie E., Rebecca C. Ticknor, and Jean Zhou (2022). â€œSystematic review of perchloroethy-
lene and non-Hodgkinâ€™s lymphomaâ€. In: Global Epidemiology 4, p. 100077. ISSN: 2590-1133. DOI: https:
//doi.org/10.1016/j.gloepi.2022.100077 . URL: https://www.sciencedirect.com/science/
article/pii/S2590113322000074.
Gossett, James M. (1987). â€œMeasurement of Henryâ€™s law constants for C1 and C2 chlorinated hydro-
carbonsâ€. In: Environmental Science & Technology 21.2. PMID: 22242591, pp. 202â€“208. DOI: 10.1021/
es00156a012.
Guan, Xiaohong, Yuankui Sun, Hejie Qin, Jinxiang Li, Irene M.C. Lo, Di He, and Haoran Dong
(2015). â€œThe limitations of applying zero-valent iron technology in contaminants sequestration and
the corresponding countermeasures: The development in zero-valent iron technology in the last two
decades (1994â€“2014)â€. In: Water Research 75, pp. 224â€“248. ISSN: 0043-1354. DOI: https://doi.org/
10.1016/j.watres.2015.02.034 . URL: https://www.sciencedirect.com/science/article/pii/
S0043135415001074.
Guerrero-Barajas, Claudia and Jim A. Field (June 1, 2005). â€œEnhancement of anaerobic carbon tetra-
chloride biotransformation in methanogenic sludge with redox active vitaminsâ€. In: Biodegradation
16.3, pp. 215â€“228. ISSN: 1572-9729. DOI: 10.1007/s10532-004-0638-z .
Guo, Jiaming, Feilong Gao, Chengfang Zhang, Shakeel Ahmad, and Jingchun Tang (2023). â€œSul-
fidation of zero-valent iron for enhanced reduction of chlorinated contaminants: A review on the
reactivity , selectivity , and interference resistanceâ€. In: Chemical Engineering Journal 477, p. 147049.
ISSN: 1385-8947. DOI: https : / / doi . org / 10 . 1016 / j . cej . 2023 . 147049. URL: https : / / www .
sciencedirect.com/science/article/pii/S1385894723057807.
Han, Young-Soo, Sung Pil Hyun, Hoon Y. Jeong, and Kim F. Hayes (2012). â€œKinetic study of cis-
dichloroethylene (cis-DCE) and vinyl chloride (VC) dechlorination using green rusts formed under
varying conditionsâ€. In: Water Research 46.19, pp. 6339â€“6350. ISSN: 0043-1354. DOI: https://doi.org/
10.1016/j.watres.2012.08.041 . URL: https://www.sciencedirect.com/science/article/pii/
S0043135412006410.
Hansch, C, A Leo, and D Hoekman (1995). Exploring QSAR: Fundamentals and applications in chemistry
and biology . American Chemical Society.
Haynes, William M. (2014). CRC Handbook of Chemistry and Physics . CRC Press. URL: https://doi.
org/10.1201/b17118.
He, Jianzhong, Victor F. Holmes, Patrick K. H. Lee, and Lisa Alvarez-Cohen (2007). â€œInfluence of
Vitamin B12 and Cocultures on the Growth of Dehalococcoides Isolates in Defined Mediumâ€. In:
Applied and Environmental Microbiology 73.9, pp. 2847â€“2853. DOI: 10.1128/AEM.02574-06.

BIBLIOGRAPHY 63
He, Y. T., J. T. Wilson, C. Su, and R. T. Wilkin (2015). â€œReview of Abiotic Degradation of Chlo-
rinated Solvents by Reactive Iron Minerals in Aquifersâ€. In: Groundwater Monitoring & Remediation
35.3, pp. 57â€“75. DOI: https://doi.org/10.1111/gwmr.12111.
Hedeselskabet (2002). Omfattende undersÃ¸gelser - Naverland 26AB, Albertslund . Tech. rep. KÃ¸benhavns
Amt.
Hemdorff, Anne Schouby (2013). â€œNatural attenuation of a chlorinated solvent plume in a chalk
aquifer: Processes and modeling. Case study: Naverland 26AB, Albertslundâ€. MA thesis. Technical
University of Denmark.
Henry , Bruce M. (2010). â€œBiostimulation for Anaerobic Bioremediation of Chlorinated Solventsâ€. In:
In Situ Remediation of Chlorinated Solvent Plumes . Ed. by H.F. Stroo and C.H. Ward. New York, NY:
Springer New York, pp. 357â€“423. ISBN: 978-1-4419-1401-9. DOI: 10.1007/978-1-4419-1401-9_12 .
Herrero, Jofre, Diana Puigserver, Ivonne Nijenhuis, Kevin Kuntze, and JosÃ© M. Carmona (2019).
â€œCombined use of ISCR and biostimulation techniques in incomplete processes of reductive de-
halogenation of chlorinated solventsâ€. In: Science of The Total Environment 648, pp. 819â€“829. ISSN:
0048-9697. DOI: https : / / doi . org / 10 . 1016 / j . scitotenv . 2018 . 08 . 184. URL: https : / / www .
sciencedirect.com/science/article/pii/S0048969718331723.
Himmelheber, David W., Kurt D. Pennell, and Joseph B. Hughes (2007). â€œNatural Attenuation Pro-
cesses during In Situ Cappingâ€. In: Environmental Science & Technology 41.15. PMID: 17822095, pp. 5306â€“
5313. DOI: 10.1021/es0700909.
Horvath, Ari L., Forrest W. Getzen, and Z. Maczynska (Mar. 1999). â€œIUP AC-NIST Solubility Data
Series 67. Halogenated Ethanes and Ethenes with Waterâ€. In: Journal of Physical and Chemical Reference
Data 28.2, pp. 395â€“627. ISSN: 0047-2689. DOI: 10.1063/1.556039 . eprint: https://pubs.aip.org/
aip/jpr/article-pdf/28/2/395/8183483/395\_1\_online.pdf.
Hovedstaden, Region (2007). Naverland 26AB, Albertslund. Monitering af punktkilde 2007. Rapport udar-
bejdet. Tech. rep. Orbicon.
Huang, Binbin, Chao Lei, Chaohai Wei, and Guangming Zeng (2014). â€œChlorinated volatile organic
compounds (Cl-VOCs) in environment â€” sources, potential human health impacts, and current
remediation technologiesâ€. In: Environment International 71, pp. 118â€“138. ISSN: 0160-4120. DOI: https:
//doi.org/10.1016/j.envint.2014.06.013 . URL: https://www.sciencedirect.com/science/
article/pii/S0160412014001974.
Huang, Hui and Lin Ye (2020). â€œChapter 9 - Biological technologies for cHRPs and risk controlâ€. In:
High-Risk Pollutants in Wastewater. Ed. by Hongqiang Ren and Xuxiang Zhang. Elsevier, pp. 209â€“236.
ISBN: 978-0-12-816448-8. DOI: https://doi.org/10.1016/B978-0-12-816448-8.00009-5 .
Hug, Laura A., Farai Maphosa, David Leys, Frank E. LÃ¶ffler, Hauke Smidt, Elizabeth A. Edwards,
and Lorenz Adrian (2013). â€œOverview of organohalide-respiring bacteria and a proposal for a clas-
sification system for reductive dehalogenasesâ€. In: Philosophical T ransactions of the Royal Society B:
Biological Sciences 368.1616, p. 20120322. DOI: 10.1098/rstb.2012.0322.
Hvid, Jens Martin, Frans Van Buchem, Frank Aandreasen, Emma Sheldon, and Ida Lykke Fabricius
(2021). â€œStratigraphy and petrophysical characteristics of Lower Paleocene cool-water carbonates,
Faxe quarry , Denmarkâ€. In: Bulletin of the Geological Society of Denmark . URL: https://doi.org/10.
37570/bgsd-2021-69-07 .

64 BIBLIOGRAPHY
Hyun, Sung Pil and Kim F. Hayes (2015). â€œAbiotic reductive dechlorination of cis-DCE by ferrous
monosulfide mackinawiteâ€. In: Environmental Science and Pollution Research 22.21, pp. 16463â€“16474.
ISSN: 1614-7499. DOI: 10.1007/s11356-015-5033-2 .
Islam, Syful, Asef Redwan, Kayleigh Millerick, Jan Filip, Lingfei Fan, and Weile Yan (2021). â€œEffect
of Copresence of Zerovalent Iron and Sulfate Reducing Bacteria on Reductive Dechlorination of
Trichloroethyleneâ€. In: Environmental Science & Technology 55.8. PMID: 33787255, pp. 4851â€“4861. DOI:
10.1021/acs.est.0c07702.
Jacobsen, R (1991). Hydraulik og stoftransport i en opsprÃ¦kket kalkbjergart â€“ lossepladsprojektet . Tech. rep.
H9, MiljÃ¸ministeriet, MiljÃ¸styrelsen.
Jakobsen, Peter, Magnus Rohde, and Emma Sheldon (2017). â€œStructures and stratigraphy of Danian
limestone, eastern SjÃ¦lland, Denmarkâ€. In: The Geological Survey of Denmark and Greenland Bulletin .
URL: https://geusbulletin.org/index.php/geusb/article/download/4391/10119/29033.
Jeong, Hoon Y., Karthik Anantharaman, Young-Soo Han, and Kim F. Hayes (2011). â€œAbiotic Reduc-
tive Dechlorination of cis-Dichloroethylene by Fe Species Formed during Iron- or Sulfate-Reductionâ€.
In: Environmental Science & Technology 45.12. PMID: 21595430, pp. 5186â€“5194. DOI: 10.1021/es104387w.
Jiang, Lisi, Yi Yang, Huijuan Jin, Hongyan Wang, Cynthia M. Swift, Yongchao Xie, Torsten Schubert,
Frank E. LÃ¶ffler, and Jun Yan (2022). â€œGeobacter sp. Strain IAE Dihaloeliminates 1,1,2-Trichloroethane
and 1,2-Dichloroethaneâ€. In: Environmental Science & Technology 56.6. PMID: 35239320, pp. 3430â€“3440.
DOI: 10.1021/acs.est.1c05952.
Jorgensen, L. F., L. Troldborg, M. Ondracek, I. K. Seidenfaden, J. Kidmose, C. Vangsgaard, and K.
Hinsby (Sept. 2024). â€œGroundwater Resilience, security , and safety in the four largest cities in Den-
markâ€. In: Acque Sotterranee - Italian Journal of Groundwater 13.3. DOI: 10.7343/as-2024-803 .
K.M. Paknikar V . Nagpal, A.V . Pethkar and J.M. Rajwade (2005). â€œDegradation of lindane from aque-
ous solutions using iron sulfide nanoparticles stabilized by biopolymersâ€. In: Science and Technology
of Advanced Materials 6.3-4, pp. 370â€“374. DOI: 10.1016/j.stam.2005.02.016.
Katrin, Mackenzie, Bleyl Steffen, Kopinke Frank-Dieter, Doose Heidi, and Bruns Johannes (2016).
â€œCarbo-Iron as improvement of the nanoiron technology: From laboratory design to the field testâ€.
In: Science of The Total Environment 563-564, pp. 641â€“648. ISSN: 0048-9697. DOI: https://doi.org/10.
1016/j.scitotenv.2015.07.107 . URL: https://www.sciencedirect.com/science/article/pii/
S0048969715304496.
Kennedy , Lonnie G., Jess W. Everett, Erica Becvar, and Donald DeFeo (2006). â€œField-scale demon-
stration of induced biogeochemical reductive dechlorination at Dover Air Force Base, Dover, Delawareâ€.
In: Journal of Contaminant Hydrology 88.1, pp. 119â€“136. ISSN: 0169-7722. DOI: https://doi.org/10.
1016/j.jconhyd.2006.06.007 . URL: https://www.sciencedirect.com/science/article/pii/
S0169772206001136.
Kocur, Chris M. D., Line Lomheim, Hardiljeet K. Boparai, Ahmed I. A. Chowdhury , Kela P . Weber,
Leanne M. Austrins, Elizabeth A. Edwards, Brent E. Sleep, and Denis M. Oâ€™Carroll (2015). â€œContribu-
tions of Abiotic and Biotic Dechlorination Following Carboxymethyl Cellulose Stabilized Nanoscale
Zero Valent Iron Injectionâ€. In: Environmental Science & Technology 49.14. PMID: 26090687, pp. 8648â€“
8656. DOI: 10.1021/acs.est.5b00719.
Kondo, Katsuhito, Akihiro Okamoto, Kazuhito Hashimoto, and Ryuhei Nakamura (2015). â€œSulfur-
Mediated Electron Shuttling Sustains Microbial Long-Distance Extracellular Electron Transfer with

BIBLIOGRAPHY 65
the Aid of Metallic Iron Sulfidesâ€. In: Langmuir 31.26. PMID: 26070345, pp. 7427â€“7434. DOI: 10.1021/
acs.langmuir.5b01033.
Krajmalnik-Brown, Rosa, Tina HÃ¶lscher, Ivy N. Thomson, F. Michael Saunders, Kirsti M. Ritalahti,
and Frank E. LÃ¶ffler (2004). â€œGenetic Identification of a Putative Vinyl Chloride Reductase in De-
halococcoides sp. Strain BA V1â€. In: Applied and Environmental Microbiology 70.10, pp. 6347â€“6351. DOI:
10.1128/AEM.70.10.6347-6351.2004.
Kueper, Bernard H., Hans F. Stroo, Catherine M. Vogel, and C. Herb Ward (2014). â€œSource Zone
Remediation: The State of the Practiceâ€. In: Chlorinated Solvent Source Zone Remediation . New York,
NY: Springer New York, pp. 1â€“27. ISBN: 978-1-4614-6922-3. URL: https://doi.org/10.1007/978-1-
4614-6922-3_1.
Lash, Lawrence H. (2019). â€œEnvironmental and Genetic Factors Influencing Kidney Toxicityâ€. In:
Seminars in Nephrology 39.2. Kidney Safety Science, pp. 132â€“140. ISSN: 0270-9295. DOI: https://doi.
org / 10 . 1016 / j . semnephrol . 2018 . 12 . 003. URL: https : / / www . sciencedirect . com / science /
article/pii/S0270929518301827.
Lee, Woojin and Bill Batchelor (2002). â€œAbiotic reductive dechlorination of chlorinated ethylenes by
iron-bearing soil minerals. 2. Green rustâ€. In: Environmental Science and Technology 36.24, pp. 5348â€“
5354. DOI: 10.1021/es0258374.
Lee, Woojin and Bill Batchelor (2004). â€œAbiotic reductive dechlorination of chlorinated ethylenes
by iron-bearing phyllosilicatesâ€. In: Chemosphere 56.10, pp. 999â€“1009. ISSN: 0045-6535. DOI: https :
/ / doi . org / 10 . 1016 / j . chemosphere . 2004 . 05 . 015. URL: https : / / www . sciencedirect . com /
science/article/pii/S004565350400400X.
Lemke, Lawrence D., Linda M. Abriola, and Pierre Goovaerts (2004). â€œDense nonaqueous phase
liquid (DNAPL) source zone characterization: Influence of hydraulic property correlation on pre-
dictions of DNAPL infiltration and entrapmentâ€. In: Water Resources Research 40.1. DOI: https://doi.
org/10.1029/2003WR001980.
Li, Yaru, He-Ping Zhao, and Lizhong Zhu (2021). â€œIron Sulfide Enhanced the Dechlorination of
Trichloroethene by Dehalococcoides mccartyi Strain 195â€. In: Frontiers in Microbiology 12. DOI: 10.
3389/fmicb.2021.665281.
Liang, Xiaoming, R. Paul Philp, and Elizabeth C. Butler (2009). â€œKinetic and isotope analyses of tetra-
chloroethylene and trichloroethylene degradation by model Fe(II)-bearing mineralsâ€. In: Chemo-
sphere 75.1, pp. 63â€“69. ISSN: 0045-6535. DOI: https://doi.org/10.1016/j.chemosphere.2008.11.
042. URL: https://www.sciencedirect.com/science/article/pii/S004565350801446X.
Ling, Liu, Yaqiang Wei, Haobo Niu, Hang Zhao, Yuling Chen, Dan Qu, Miao Gao, and Jian Chen
(2024). â€œLong-term effectiveness and sustainability of EHC remediation in carbon tetrachloride-
contaminated groundwater: Mechanistic understanding and practical applicationsâ€. In: Journal of
Cleaner Production 435, p. 140510.
Little, R., E. Muller, and R. Mackay (1996). â€œModelling of contaminant migration in a chalk aquiferâ€.
In: Journal of Hydrology 175.1, pp. 473â€“509. ISSN: 0022-1694. DOI: https://doi.org/10.1016/S0022-
1694(96)80021-7.
Liu, Xiaokun, Lian Zhang, Rui Shen, Qihong Lu, Qinglu Zeng, Xiaojun Zhang, Zhili He, Simona
Rossetti, and Shanquan Wang (2023). â€œReciprocal Interactions of Abiotic and Biotic Dechlorination

66 BIBLIOGRAPHY
of Chloroethenes in Soilâ€. In: Environmental Science & Technology 57.37. PMID: 37665676, pp. 14036â€“
14045. DOI: 10.1021/acs.est.3c04262.
LÃ¶ffler, Frank E., Jun Yan, Kirsti M. Ritalahti, Lorenz Adrian, Elizabeth A. Edwards, Konstantinos T.
Konstantinidis, Jochen A. MÃ¼ller, Heather Fullerton, Stephen H. Zinder, and Alfred M. Spormann
(2013). â€œDehalococcoides mccartyi gen. nov., sp. nov., obligately organohalide-respiring anaerobic
bacteria relevant to halogen cycling and bioremediation, belong to a novel bacterial class, Dehalo-
coccoidia classis nov., order Dehalococcoidales ord. nov. and family Dehalococcoidaceae fam. nov.,
within the phylum Chloroflexiâ€. In: International Journal of Systematic and Evolutionary Microbiology
63.Pt 2, pp. 625â€“635. ISSN: 1466-5034. DOI: https://doi.org/10.1099/ijs.0.034926-0.
Lu, Cong, Poul L. Bjerg, Fengjun Zhang, and Mette M. Broholm (2011). â€œSorption of chlorinated
solvents and degradation products on natural clayey tillsâ€. In: Chemosphere 83.11, pp. 1467â€“1474.
ISSN: 0045-6535. DOI: https : / / doi . org / 10 . 1016 / j . chemosphere . 2011 . 03 . 007. URL: https :
//www.sciencedirect.com/science/article/pii/S0045653511002761.
Major, David W., Michaye L. McMaster, Evan E. Cox, Elizabeth A. Edwards, Sandra M. Dworatzek,
Edwin R. Hendrickson, Mark G. Starr, Jo Ann Payne, and Lois W. Buonamici (2002). â€œField Demon-
stration of Successful Bioaugmentation To Achieve Dechlorination of Tetrachloroethene To Etheneâ€.
In: Environmental Science & Technology 36.23. PMID: 12523427, pp. 5106â€“5116. DOI: 10.1021/es0255711.
Mao, Xinwei, Alexandra Polasko, and Lisa Alvarez-Cohen (2017). â€œEffects of Sulfate Reduction on
Trichloroethene Dechlorination by Dehalococcoides-Containing Microbial Communitiesâ€. In: Ap-
plied and Environmental Microbiology 83.8, e03384â€“16. DOI: 10.1128/AEM.03384-16.
MaymÃ³-Gatell, Xavier, Timothy Anguish, and Stephen H. Zinder (1999). â€œReductive Dechlorination
of Chlorinated Ethenes and 1,2-Dichloroethane by &#x201c;<i>Dehalococcoides ethenogenes</i>&#x201d;
195â€. In: Applied and Environmental Microbiology 65.7, pp. 3108â€“3113. DOI: 10.1128/AEM.65.7.3108-
3113.1999.
MiljÃ¸styrelsen, MiljÃ¸- og energiministriet (1998). Vejledning nr. 6 og 7. Oprydning pÃ¥ forurenede lokaliteter.
Tech. rep. Hovedbind og Appendikser. MiljÃ¸styrelsen.
Mohamed, Badr Ali and Mohamed Samer (2023). â€œChapter 8 - Biobutanol production from agri-
cultural wastesâ€. In: Valorization of Wastes for Sustainable Development . Ed. by Rangabhashiyam Sel-
vasembian, Nur Izyan Wan Azelee, Saravanan Ramiah Shanmugam, Ponnusami Venkatachalam,
and Ajay Kumar Mishra. Elsevier, pp. 181â€“200. ISBN: 978-0-323-95417-4. DOI: https://doi.org/10.
1016/B978-0-323-95417-4.00008-1 .
Molenda, Olivia, Andrew T. Quaile, and Elizabeth A. Edwards (2016). â€œDehalogenimonas sp. Strain
WBC-2 Genome and Identification of Its <i>trans</i>-Dichloroethene Reductive Dehalogenase,
TdrA â€. In: Applied and Environmental Microbiology 82.1, pp. 40â€“50. DOI: 10.1128/AEM.02017-15.
Moreno, Hector A., David L. Cocke, Jewel A. Gomes, Paul Morkovsky , Jose Parga, Eric Peterson, and
C. Garcia (2007). â€œElectrochemical Generation of Green Rust using Electrocoagulationâ€. In: ECS
T ransactions3.18, p. 67. DOI: 10.1149/1.2753225.
Mortan, Siti Hatijah, LucÃ­a MartÃ­n-GonzÃ¡lez, Teresa Vicent, Gloria Caminal, Ivonne Nijenhuis, Lorenz
Adrian, and Ernest Marco-Urrea (2017). â€œDetoxification of 1,1,2-trichloroethane to ethene in a biore-
actor co-culture of Dehalogenimonas and Dehalococcoides mccartyi strainsâ€. In: Journal of Hazardous
Materials 331, pp. 218â€“225. ISSN: 0304-3894. DOI: https://doi.org/10.1016/j.jhazmat.2017.02.
043. URL: https://www.sciencedirect.com/science/article/pii/S0304389417301310.

BIBLIOGRAPHY 67
Mouvet, Christophe, Delphine Barberis, and Alain C.M. Bourg (1993). â€œAdsorption isotherms of tri-
and tetrachloroethylene by various natural solidsâ€. In: Journal of Hydrology 149.1. Conventry Ground-
water Investigation: Sources and Movement of Chlorinated Hydrocarbon Solvents, pp. 163â€“182. ISSN:
0022-1694. DOI: https://doi.org/10.1016/0022-1694(93)90105-I .
MÃ¼ller, Jochen A., Bettina M. Rosner, Gregory von Abendroth, Galit Meshulam-Simon, Perry L. Mc-
Carty , and Alfred M. Spormann (2004). â€œMolecular Identification of the Catabolic Vinyl Chloride
Reductase from <i>Dehalococcoides</i> sp. Strain VS and Its Environmental Distributionâ€. In: Ap-
plied and Environmental Microbiology 70.8, pp. 4880â€“4888. DOI: 10.1128/AEM.70.8.4880-4888.2004.
Muyzer, Gerard and Alfons J. M. Stams (June 1, 2008). â€œThe ecology and biotechnology of sulphate-
reducing bacteriaâ€. In: Nature Reviews Microbiology 6.6, pp. 441â€“454. ISSN: 1740-1534. DOI: 10.1038/
nrmicro1892. URL: https://doi.org/10.1038/nrmicro1892.
Nakamura, Ryuhei, Toshihiro Takashima, Souichiro Kato, Ken Takai, Masahiro Yamamoto, and Kazuhito
Hashimoto (2010). â€œElectrical Current Generation across a Black Smoker Chimneyâ€. In: Angewandte
Chemie International Edition 49.42, pp. 7692â€“7694. DOI: https://doi.org/10.1002/anie.201003311.
NÄ›meÄek, Jan, KristÃ½na MarkovÃ¡, Roman Å pÃ¡nek, VojtÄ›ch AntoÅ¡, Petr Kozubek, OndÅ™ej LhotskÃ½,
and Miroslav ÄŒernÃ­k (2020). â€œHydrochemical Conditions for Aerobic/Anaerobic Biodegradation
of Chlorinated Ethenesâ€”A Multi-Site Assessmentâ€. In: Water 12.2. ISSN: 2073-4441. DOI: 10.3390/
w12020322. URL: https://www.mdpi.com/2073-4441/12/2/322.
Nilsson, Bertel and Peter Gravesen (2018). â€œKarst Geology and Regional Hydrogeology in Den-
markâ€. In: Karst Groundwater Contamination and Public Health . DOI: 10 . 1007 / 978 - 3 - 319 - 51070 -
5_34.
Ottosen, Cecilie Bang (2020). â€œDocumentation and quantification of in situ natural and enhanced
degradation of chlorinated ethenesâ€. PhD thesis. Technical University of Denmark.
Ottosen, Cecilie Fisker and Mette Martina Broholm (2024). Vidensopsamling Naverland Sammenfatning
af eksisterende viden samt udpegning af videnshuller .
P . T. McCauley M. Robinson, F. B. Daniel and G. R. Olson (1995). â€œThe Effects of Subacute and Sub-
chronic Oral Exposure to Cis-1,2-Dichloroethylene in Sprague-Dawley Ratsâ€. In: Drug and Chemical
Toxicology 18.2-3. PMID: 7497910, pp. 171â€“184. DOI: 10.3109/01480549509014319.
Popek, Emma (2018). â€œChapter 2 - Environmental Chemical Pollutantsâ€. In: Sampling and Analysis
of Environmental Chemical Pollutants (Second Edition) . Second Edition. Elsevier, pp. 13â€“69. ISBN: 978-
0-12-803202-2. DOI: https://doi.org/10.1016/B978-0-12-803202-2.00002-1 .
PÃ³sfai, MihÃ¡ly and Rafal E Dunin-Borkowski (2006). â€œSulfides in biosystemsâ€. In: Reviews in Miner-
alogy and Geochemistry 61.1, pp. 679â€“714.
PubChem (2025). PubChem Annotation Record for cis-1,2-Dichloroethylene, Source: Hazardous Substances
Data Bank (HSDB) . https : / / pubchem . ncbi . nlm . nih . gov. National Center for Biotechnology
Information, National Library of Medicine (US), Retrieved February 4, 2025.
Puri, Shammy (2003). â€œTransboundary Aquifer Resourcesâ€. In: Water International 28, pp. 276â€“279.
URL: https://api.semanticscholar.org/CorpusID:153241299.
Rathbun, R. E. (1998). â€œTransport, behavior, and fate of volatile organic compounds in streamsâ€. In:
U.S. Geological survey . DOI: 10.3133/pp1589.

68 BIBLIOGRAPHY
Reilly , Thomas E., L. Niel Plummer, Patrick J. Phillips, and Eurybiades Busenberg (1994). â€œThe use of
simulation and multiple environmental tracers to quantify groundwater flow in a shallow aquiferâ€.
In: Water Resources Research 30, pp. 421â€“433. URL: https://api.semanticscholar.org/CorpusID:
129050485.
Rosenbom, Annette (2005). â€œPreferential flow and transport in variably saturated fractured mediaâ€.
PhD thesis. Techical University of Denmark.
Sahoo, Dipak and James A. Smith (1997). â€œEnhanced Trichloroethene Desorption from Long-Term
Contaminated Soil Using Triton X-100 and pH Increasesâ€. In: Environmental Science & Technology 31.7,
pp. 1910â€“1915. DOI: 10.1021/es960655t.
Sakuratani, Yuki, Kenji Kasai, Yoshiyuki Noguchi, and Jun Yamada (2007). â€œComparison of Pre-
dictivities of Logâ€…P Calculation Models Based on Experimental Data for 134 Simple Organic Com-
poundsâ€. In: QSAR & Combinatorial Science 26.1, pp. 109â€“116. DOI: https://doi.org/10.1002/qsar.
200630019.
Salzer, Joel S (2013). â€œSorption capacity and governing parameters for transport of chlorinated sol-
vents in chalk aqufiersâ€. MA thesis. Techical University of Denmark.
Sander, Rolf, William E. Acree, Alex De Visscher, Stephen E. Schwartz, and Timothy J. Wallington
(2022). â€œHenryâ€™s law constants (IUP AC Recommendations 2021)â€. In: Pure and Applied Chemistry
94.1, pp. 71â€“85. DOI: doi:10.1515/pac-2020-0302 .
Schmidt, K. R. and A. Tiehm (Sept. 2008). â€œNatural attenuation of chloroethenes: identification of se-
quential reductive/oxidative biodegradation by microcosm studiesâ€. In: Water Science and Technology
58.5, pp. 1137â€“1145. ISSN: 0273-1223. DOI: 10.2166/wst.2008.729. eprint: https://iwaponline.com/
wst/article-pdf/58/5/1137/436253/1137.pdf. URL: https://doi.org/10.2166/wst.2008.729.
Scholl, Martha A. and Ronald W. Harvey (1992). â€œLaboratory investigations on the role of sediment
surface and groundwater chemistry in transport of bacteria through a contaminated sandy aquiferâ€.
In: Environmental Science & Technology 26.7, pp. 1410â€“1417. DOI: 10.1021/es00031a020.
Schulze-Makuch, Dirk (2005). â€œLongitudinal dispersivity data and implications for scaling behav-
iorâ€. In: Groundwater 43.3, pp. 443â€“456. DOI: https://doi.org/10.1111/j.1745-6584.2005.0051.x.
Sheu, Y.T., S.C. Chen, C.C. Chien, C.C. Chen, and C.M. Kao (2015). â€œApplication of a long-lasting col-
loidal substrate with pH and hydrogen sulfide control capabilities to remediate TCE-contaminated
groundwaterâ€. In: Journal of Hazardous Materials 284, pp. 222â€“232. ISSN: 0304-3894. DOI: https://doi.
org/10.1016/j.jhazmat.2014.11.023. URL: https://www.sciencedirect.com/science/article/
pii/S0304389414009303.
Siegel, M.D. and C.R. Bryan (2003). â€œ9.06 - Environmental Geochemistry of Radioactive Contami-
nationâ€. In: T reatise on Geochemistry. Ed. by Heinrich D. Holland and Karl K. T urekian. Oxford: Perg-
amon, pp. 205â€“262. ISBN: 978-0-08-043751-4. DOI: https : / / doi . org / 10 . 1016 / B0 - 08 - 043751 -
6/09049-6.
SÃ¸, Helle Ugilt (2023). Naverland 26A-B, Albertslund Forureningskilder , potentialeforhold og forurening-
sudbredelse - 2023 . Tech. rep. HOFOR Plan Vand.
Sonnenborg, Torben O. (2006). â€œVandressource- og stoftransportmodellering i kalk: status og mu-
ligheder - KALK P Ã… TV Ã†RS ATV Jord og Grundvandâ€. In: Danmarks og GrÃ¸nlands Geologiske Under-

BIBLIOGRAPHY 69
sÃ¸gelser (GEUS) . URL: https://backend.miljoeogressourcer.dk/media/lix/3130/Sonnenborg.
pdf.
SÃ¸rensen, Mie Barrett (2013). â€œRemediation potential for chlorinated ethene contamination by en-
hanced reductive dechlorination in a chalk aquiferâ€. MA thesis. Technical University of Denmark.
Spormann, Alfred M. (2006). Factors Affecting Cis-Dichloroethene and Vinyl Chloride Biological T rans-
formation Under Anaerobic Conditions . Tech. rep. Stanford University.
Stams, Alfons J. M. (Mar. 1, 1994). â€œMetabolic interactions between anaerobic bacteria in methanogenic
environmentsâ€. In: Antonie van Leeuwenhoek 66.1, pp. 271â€“294. ISSN: 1572-9699. DOI: 10.1007/BF00871644.
URL: https://doi.org/10.1007/BF00871644.
Stroo, Hans, Marvin Unger, Herb Ward, Michael Kavanaugh, Catherine Vogel, Andrea Leeson, Jef-
frey Marqusee, and Bradley Smith (2003). â€œPeer Reviewed: Remediating Chlorinated Solvent Source
Zonesâ€. In: Environmental Science & Technology 37.11. PMID: 12831011, 224Aâ€“230A. DOI: 10.1021/
es032488k.
Sun, Chen and Jun Dong (2025). â€œEffects of engineering injection and supplement mode of in-situ
biogeochemical transformation enhancement EVO-FeSO4 on the remediation of tetrachloroethylene
contaminated aquiferâ€. In: Journal of Environmental Sciences 154, pp. 200â€“211. ISSN: 1001-0742. DOI:
https://doi.org/10.1016/j.jes.2024.08.017. URL: https://www.sciencedirect.com/science/
article/pii/S1001074224004224.
Sun, Yuankui, Kaiwei Zheng, Xueying Du, Hejie Qin, and Xiaohong Guan (2024). â€œInsights into
the contrasting effects of sulfidation on dechlorination of chlorinated aliphatic hydrocarbons by
zero-valent ironâ€. In: Water Research 255, p. 121494. ISSN: 0043-1354. DOI: https : / / doi . org / 10 .
1016/j.watres.2024.121494 . URL: https://www.sciencedirect.com/science/article/pii/
S0043135424003968.
Thiruvenkatachari, R., S. Vigneswaran, and R. Naidu (2008). â€œPermeable reactive barrier for ground-
water remediationâ€. In: Journal of Industrial and Engineering Chemistry 14.2, pp. 145â€“156. ISSN: 1226-
086X. DOI: https://doi.org/10.1016/j.jiec.2007.10.001 . URL: https://www.sciencedirect.
com/science/article/pii/S1226086X0700024X.
Thorling, LÃ¦rke, Christian Nyrop Albers, Birgitte Hansen, Jacob Kidmose, Anders R. Johnsen, Jolanta
Kazmierczak, Mette Hilleke Mortensen, and Lars Troldborg (2024). GrundvandsovervÃ¥gning Status og
udvikling 1989 â€“ 2023 . Tech. rep. The Geological Survey of Denmark and Greenland. URL: https://
data.geus.dk/pure-pdf/Grundvandsoverv%C3%A5gning.%20Status%20og%20udvikling%201989-
2023_web.pdf.
Tobiszewski, Marek and Jacek NamieÅ›nik (July 1, 2012). â€œAbiotic degradation of chlorinated ethanes
and ethenes in waterâ€. In: Environmental Science and Pollution Research 19.6, pp. 1994â€“2006. ISSN: 1614-
7499. DOI: 10.1007/s11356-012-0764-9 .
Toxic Substances, Agency for and Disease Registry (1996). Toxicological Profile for 1,2-Dichloroethene .
Tech. rep. Prepared in accordance with guidelines developed by ATSDR and EP A. Atlanta, GA: U.S.
Department of Health and Human Services, Public Health Service. URL: https://hero.epa.gov/
hero/index.cfm/reference/details/reference_id/723873.
U.S. Environmental Protection Agency (2011). Archived Technical Fact Sheet on 1,2-Dichloroethylene .
Technical Report EP A/690/R-11/020F. U.S. Environmental Protection Agency. URL: https://archive.

70 BIBLIOGRAPHY
epa.gov/water/archive/web/pdf/archived-technical-fact-sheet-on-1-2-dichloroethylene.
pdf.
Vidic, Radisav D (2001). â€œPermeable reactive barriers: Case study reviewâ€. In: Technology Evaluation
Report TE01-01 .
Vogel, Maria, Ivonne Nijenhuis, Jonathan Lloyd, Christopher Boothman, MarlÃ©n PÃ¶ritz, and Katrin
Mackenzie (2018). â€œCombined chemical and microbiological degradation of tetrachloroethene dur-
ing the application of Carbo-Iron at a contaminated field siteâ€. In: Science of The Total Environment628-
629, pp. 1027â€“1036. ISSN: 0048-9697. DOI: https://doi.org/10.1016/j.scitotenv.2018.01.310 .
URL: https://www.sciencedirect.com/science/article/pii/S0048969718303528.
Vogel, T M and P L McCarty (1985). â€œBiotransformation of tetrachloroethylene to trichloroethylene,
dichloroethylene, vinyl chloride, and carbon dioxide under methanogenic conditionsâ€. In: Applied
and Environmental Microbiology 49.5, pp. 1080â€“1083. DOI: 10.1128/aem.49.5.1080-1083.1985.
Wang, S.Y., Y.C. Kuo, Y.Z. Huang, C.W. Huang, and C.M. Kao (2015). â€œBioremediation of 1,2-dichloroethane
contaminated groundwater: Microcosm and microbial diversity studiesâ€. In: Environmental Pollution
203, pp. 97â€“106. ISSN: 0269-7491. DOI: https://doi.org/10.1016/j.envpol.2015.03.042 . URL:
https://www.sciencedirect.com/science/article/pii/S0269749115001724.
Wang, Xiaohui, Jia Xin, Mengjiao Yuan, and Fang Zhao (2020). â€œElectron competition and electron
selectivity in abiotic, biotic, and coupled systems for dechlorinating chlorinated aliphatic hydro-
carbons in groundwater: A reviewâ€. In: Water Research 183, p. 116060. ISSN: 0043-1354. DOI: https:
//doi.org/10.1016/j.watres.2020.116060 . URL: https://www.sciencedirect.com/science/
article/pii/S0043135420305972.
Weerasooriya, R. and B. Dharmasena (2001). â€œPyrite-assisted degradation of trichloroethene (TCE)â€.
In: Chemosphere 42.4, pp. 389â€“396. ISSN: 0045-6535. DOI: https://doi.org/10.1016/S0045-6535(00)
00160-0.
Whiting, Kent, Patrick J. Evans, Carmen LebrÃ³n, Bruce Henry , John T. Wilson, and Erica Becvar
(2014). â€œFactors Controlling In Situ Biogeochemical Transformation of Trichloroethene: Field Sur-
veyâ€. In: Groundwater Monitoring & Remediation 34.3, pp. 79â€“94. DOI: https://doi.org/10.1111/
gwmr.12068.
WitthÃ¼ser, K., B. Reichert, and H. Hotzl (2003). â€œContaminant Transport in Fractured Chalk: Labo-
ratory and Field Experimentsâ€. In: Groundwater 41.6, pp. 806â€“815. DOI: https://doi.org/10.1111/
j.1745-6584.2003.tb02421.x.
Wu, Naijin, Yi Li, Yizhou Liu, Yangfan Feng, Wenbo Fei, Tianwen Zheng, Liming Rong, Nan Luo,
Yun Song, Wenxia Wei, and Peizhong Li (2025). â€œReductive dechlorination of 1,1,2-trichloroethane
in groundwater by zero valent iron coupled with biostimulation under sulfate stress: Differences
and potential mechanismsâ€. In: Environmental Research , p. 121574. ISSN: 0013-9351. DOI: https : / /
doi . org / 10 . 1016 / j . envres . 2025 . 121574. URL: https : / / www . sciencedirect . com / science /
article/pii/S0013935125008254.
Wu, Naijin, Wen Zhang, Wenxia Wei, Sucai Yang, Haijian Wang, Zhongping Sun, Yun Song, Peizhong
Li, and Yong Yang (2020). â€œField study of chlorinated aliphatic hydrocarbon degradation in contam-
inated groundwater via micron zero-valent iron coupled with biostimulationâ€. In: Chemical Engineer-
ing Journal 384, p. 123349. ISSN: 1385-8947. DOI: https://doi.org/10.1016/j.cej.2019.123349 .
URL: https://www.sciencedirect.com/science/article/pii/S1385894719327627.

BIBLIOGRAPHY 71
Wu, Rifeng, Rui Shen, Zhiwei Liang, Shengzhi Zheng, Yong Yang, Qihong Lu, Lorenz Adrian, and
Shanquan Wang (2023). â€œImprove Niche Colonization and Microbial Interactions for Organohalide-
Respiring-Bacteria-Mediated Remediation of Chloroethene-Contaminated Sitesâ€. In: Environmental
Science & Technology 57.45. PMID: 37902991, pp. 17338â€“17352. DOI: 10.1021/acs.est.3c05932.
Yamaguchi, Akira, Masahiro Yamamoto, Ken Takai, Takumi Ishii, Kazuhito Hashimoto, and Ryuhei
Nakamura (2014). â€œElectrochemical CO2 Reduction by Ni-containing Iron Sulfides: How Is CO2
Electrochemically Reduced at Bisulfide-Bearing Deep-sea Hydrothermal Precipitates?â€ In: Electrochim-
ica Acta 141, pp. 311â€“318. ISSN: 0013-4686. DOI: https://doi.org/10.1016/j.electacta.2014.07.
078. URL: https://www.sciencedirect.com/science/article/pii/S0013468614014832.
Yamamoto, Masahiro, Ryuhei Nakamura, Kazumasa Oguri, Shinsuke Kawagucci, Katsuhiko Suzuki,
Kazuhito Hashimoto, and Ken Takai (2013). â€œGeneration of Electricity and Illumination by an Envi-
ronmental Fuel Cell in Deep-Sea Hydrothermal Ventsâ€. In: Angewandte Chemie International Edition
52.41, pp. 10758â€“10761. DOI: https://doi.org/10.1002/anie.201302704.
Yang, Yi, Steven A Higgins, Jun Yan, Burcu ÅimÅŸir, Karuna Chourey , Ramsunder Iyer, Robert L Het-
tich, Brett Baldwin, Dora M Ogles, and Frank E LÃ¶ffler (Aug. 2017). â€œGrape pomace compost harbors
organohalide-respiring Dehalogenimonas species with novel reductive dehalogenase genesâ€. In: The
ISME Journal 11.12, pp. 2767â€“2780. ISSN: 1751-7362. DOI: 10.1038/ismej.2017.127.
Yang, Zhilong, Xiao-li Wang, Hui Li, Jie Yang, Li-Yang Zhou, and Yong-di Liu (2017). â€œRe-activation
of aged-ZVI by iron-reducing bacterium Shewanella putrefaciens for enhanced reductive dechlori-
nation of trichloroethyleneâ€. In: Journal of Chemical Technology & Biotechnology 92.10, pp. 2642â€“2649.
DOI: https://doi.org/10.1002/jctb.5284.
Yoshikawa, Miho, Ming Zhang, Yoshishige Kawabe, and Taiki Katayama (May 2021). â€œEffects of fer-
rous iron supplementation on reductive dechlorination of tetrachloroethene and on methanogenic
microbial communityâ€. In: FEMS Microbiology Ecology 97.5, fiab069. ISSN: 0168-6496. DOI: 10.1093/
femsec/fiab069.
Zhang, Yaru, Zhaoyong Bian, Feng Wang, Yiyin Peng, Wenyu Xiao, and Qiang Zhang (2025). â€œIn-
situ synthesis of FeS nanoparticles enhances Sulfamethoxazole degradation via accelerated electron
transfer in anaerobic bacterial communitiesâ€. In: Water Research 273, p. 123025. ISSN: 0043-1354. DOI:
https://doi.org/10.1016/j.watres .2024.123025 . URL: https://www.sciencedirect.com/
science/article/pii/S0043135424019250.

This page intentionally left blank.

APPENDICES

This page intentionally left blank.

A
APPENDIX A
A.1 Other remediation techniques
A.1.1 Green rust (GR)
Green Rust (GR), also called fougerite, is a layered double hydroxides (a family of minerals)
and has â€the general chemical formula of [ ğ¹ğ‘’ ( ğ¼ğ¼ ) ( 1âˆ’ ğ‘¥) ğ¹ğ‘’ ( ğ¼ğ¼ğ¼ ) ğ‘¥ ğ‘‚ğ» 2] ğ‘¥+ [( ğ‘¥/ ğ‘›) ğ´ğ‘›âˆ’ ğ‘šğ»2ğ‘‚] ğ‘¥âˆ’ where
ğ´ğ‘›âˆ’ is an anion intercalated between two brucite-like layers, and x is the fraction of Fe(III)
out of total Feâ€ [ Han et al., 2012 ]. The types of GRs are carbonate ( ğ¶ğ‘‚ 3), ğ¶ğ‘™ , and sulphate
(ğ‘†ğ‘‚ 4) GR [ Han et al., 2012 ]. GRs are highly reactive (and therefore unstable) and are ca-
pable of reducing organic compounds, including DCE [ Moreno et al., 2007 ]. Sulphate GR
has been shown to rapidly and effectively dechlorinate cDCE and VC as well as lead to be-
nign byproducts [ Han et al., 2012 ; Lee et al., 2002 ]. This study also found similar results
to FeS, there is a dependence of pH on the dechlorination rate constant, increasing the pH
from 6.8 to 10.1 increases the dechlorination of TCE and freeze-drying would oxidise the
sulfate GR [ Lee et al., 2002 ]. Like Hyun et al., 2015 â€™s study , the identity and purity of syn-
thesised iron-bearing mineral was confirmed using X-ray diffraction. However, sulfate GR
has a greater surface area-normalised pseudo-first order initial rate constant than ğ¹ğ‘’ğ‘† 2 by
a factor of 3.4 - 8.2. The surface phase of GR is more reactive when there is excess dissolved
Fe(II), increasing the rate of dechlorination of cDCE and VC [ Han et al., 2012 ].
A.1.2 Zero Valent Iron (ZVI)
ZVI degrades chlorinated solvents by oxidation of Fe(0) to Fe(+2), donating itâ€™s electron to
the CE (or other contaminant) as shown below;
ğ¹ğ‘’ + ğ‘…ğ¶ğ‘™ + ğ»+ â†’ ğ¹ğ‘’ 2+ + ğ‘…ğ» + ğ¶ğ‘™ âˆ’
The form of ZVI that requires trenching to be put into the subsurface is granular ZVI; how-
ever, through direct injection and soil mixing, micro ZVI (mZVI) can be added, while nano
ZVI (nZVI) can be added through injection wells (and direct injection). At the same time,
mZI and nZVI have lower longevity as they have smaller particle diameters and higher reac-
77

78 A. Appendix A
tive surface areas. ZVI dechlorinates CEs via the beta elimination pathway ( Figure 3.3 ). Fur-
thermore, ZVI exhibits limited environmental stability due to rapid corrosion in Feï¿¿/Hï¿¿O
systems, which diminishes its reactivity over time as surface passivation occurs, e.g. through
the precipitation of iron hydroxides and carbonates Guan et al., 2015 .
A.1.3 Combined ZVI-Biotic Systems
Herrero et al., 2019 compared i) MNA, ii) biostimulation with lactic acid, iii) In-Situ Chem-
ical Reduction (ISCR) with ZVI and iv) a combined strategy with lactic acid and ZVI and
found that the combined strategy was the fastest and sustained complete dechlorination of
PCE for pools in the transition zone. This was supported by Sheu et al., 2015 ; X. Wang et al.,
2020; Ling et al., 2024 as well. It accelerates the initiation of biotic dehalogenation using the
intrinsic reductive capabilities of ZVI [ N. Wu, Yi Li, et al., 2025 ]. ZVI corrosion reaction
and chemical reduction of naturally occurring electron acceptors result in the redox poten-
tial of the aquifer being lowered [ N. Wu, Yi Li, et al., 2025 ]. Providing the dechlorinating
microorganisms with electron donors, hence a suitable setting for organohalide respiration
which facilitates reductive dehalogenation and depends on OHRB [ Kocur et al., 2015 ; S.
Wang et al., 2015 ; Mortan et al., 2017 ; Dong et al., 2019 ; NÄ›meÄek et al., 2020 ; N. Wu, W.
Zhang, et al., 2020 ; Jiang et al., 2022 ]. Furthermore, the oxidised layer on the surface of Fe0
may be corroded by dissimilatory iron-reducing bacteria, revealing active areas [ Z. Yang
et al., 2017 ].
Islam et al., 2021 used a customised contamination system to perform bench-scale tests
on TCE dechlorination under the combined action of ZVI and SRB. They discovered that the
main mechanism for TCE reduction in the coupled SRB-ZVI system was abiotic dechlorina-
tion. Even in the absence of direct interaction between nZVI and FeS, synthetic FeS particles
can improve ZVIâ€™s dechlorination performance as sulfidation improves electron-mediated
dechlorination [ N. Wu, Yi Li, et al., 2025 ; Guo et al., 2023 ]. And SRB may affect ZVIâ€™s reac-
tivity in a variety of mechanisms depending on the remediation setting [ N. Wu, Yi Li, et al.,
2025]. However, even though S-mZVI (sulfidated micro ZVI) reacts more strongly to chlo-
rinated ethanes than mZVI, it doesnâ€™t improve dechlorination of other chlorinated aliphatic
hydrocarbons unlike S-nZVI (sulfidated nano ZVI) which shows a differential trend [ Y.
Sun et al., 2024 ]. Coupling nZVI and biostimulation shows an improved dechlorination
eï¬€iciency from 66.7% to 80.2% when the ğ‘†ğ‘‚ 2âˆ’
4 reached 400 mg/L, and by the 20th day ,
ethylene and ethane formed from VC (from 1,1,2-TCA ) [ N. Wu, Yi Li, et al., 2025 ]. Simi-
larly , GEOFORMÂ® Extended Release consists of mZVI and FeS and can be applied through
direct push injection, hydraulic or pneumatic fracturing or soil mixing to create a permeable
reactive barrier (PBR) [ EVONIK, n.d.(a) ].
Another combination is composite material Carbo-Iron which combines colloidal acti-
vated carbon ( ğ‘‘50 < 1ğœ‡m) and nZVI whose intention is to accumulate PCE for ERD. It was
tested 4 - 6 mbs in the first aquifer of the site [ Katrin et al., 2016 ]. However, as VC wasnâ€™t
found and when CSIA was conducted, isotope fractionation was found as is known from

A.2. Grain size 79
sorption-dominated processes, microbial dechlorination by OHRB is less likely and there-
fore M. Vogel et al., 2018 suggests microbial oxidation of cDCE.
A.2 Grain size
The following experiment was conducted to determine the appropriate grain size range for
limestone and to identify the range at which fine limestone particles are not extracted when
collecting water samples using a syringe and needle. As shown in Figure A.1 , the cloudiness
in the bottle containing grain sizes <1 mm remained significantly high, indicating that this
range is unsuitable and will not be considered for further use.
Figure A.2 demonstrates that allowing the batch to settle for several hours after mixing
is advisable if the vials sent for analysis are not filtered. This settling period helps prevent
the extraction of fine particles.
Based on the results, the optimal grain size range is 1-5 mm. Within this range, the
3-5 mm fraction produces the clearest water samples, while the 1-2 mm fraction, which is
similar in size to coarse sand, allows for greater exposure of the water to the limestone due
to its larger surface area. Additionally , using 900 mL of water and 300 g of limestone leaves
approximately 100 mL of headspace, which is ideal for the experimental setup.
(a) for 1 hour
 (b) for 15 hours
Figure A.1: Batch test bottles after the limestone settles
(a) for 1 hour
 (b) for 15 hours
Figure A.2: Vials of water extracted after the limestone settles

80 A. Appendix A
A.3 Reagent calculations
A.3.1 Hydrogen consumption
Scenario B and C
Table A.1: The calculated maximum hydrogen consumption in moles for a batch consisting 1000 mL ground-
water.
Reaction Concentration
[mg/L]
Concentration [mol] H consumption [mol]
2ğ»2 + O22ğ»2ğ‘‚ <0.01 3.125E-07 0.000000625
6ğ»2 + 2NOâˆ’
3 1ğ‘2 + 6ğ»2ğ‘‚ <0.3 4.84E-06 1.45E-05
ğ»2 + Mn4+ 2ğ‘€ğ‘› 2 + + 2ğ»+ est 0.1 (no
record)
1.82E-06 1.82E-06
1ğ»2 + 2Fe3+ 2ğ¹ğ‘’ 2+ + 2ğ»+ 0.053 9.49E-07 4.74E-07
1ğ»2 + 2Fe3+ 2ğ¹ğ‘’ 2+ + 2ğ»+ 999.947 0.018 0.00895
9ğ»2 + 2SO2âˆ’
4 2ğ»ğ‘† âˆ’ + 8ğ»2ğ‘‚ 110 0.00115 0.00515
9ğ»2 + 2SO2âˆ’
4 2ğ»ğ‘† âˆ’ + 8ğ»2ğ‘‚ 890 0.009265043 0.0416
C2Cl2H2 + 2ğ»2ğ¶2ğ»4 + 2ğ»+ + 2ğ¶ğ‘™ âˆ’ 0.15 1.55E-06 3.09E-06
Total 0.0558
DGU-NR: 200. 5096 Sample journal number: 835-2016-80391103 from Eurofins Denmark
A/S except for Mn4+
349.2 g H 2 is produced by 1 Kg ELS concentrate and the total organic carbon (TOC) of
ELS concentrate is approximately 58%.
Mass of hydrogen required [g]= H2 consumption [mol] Â· H2 molar mass [g/mol] =
0.056 Â· 2.015 = 0.113 g
ELS concentrate mass required[g] = ( Mass of H2 required [g] /58%) / mass of H2
produced per ELS concentrate kg [g/g] = 0.194 / 0.349 =0.556 g
The concentration of ELS Microemulsion is 15g/L, volume of ELS Microemulsion re-
quired [mL] = ELS concentrate mass [g] / concentration of ELS Microemulsion = 0.556/ 15 =
37.075 mL

A.3. Reagent calculations 81
Scenario D
Table A.2: The calculated maximum hydrogen consumption in moles for a batch consisting 1000 mL ground-
water.
Reaction Concentration
[mg/L]
Concentration [mol] H consumption [mol]
2ğ»2 + O22ğ»2ğ‘‚ <0.01 3.125E-07 0.000000625
6ğ»2 + 2NOâˆ’
3 1ğ‘2 + 6ğ»2ğ‘‚ <0.3 4.84E-06 1.45E-05
ğ»2 + Mn4+ 2ğ‘€ğ‘› 2 + + 2ğ»+ est 0.1 (no
record)
1.82E-06 1.82E-06
1ğ»2 + 2Fe3+ 2ğ¹ğ‘’ 2+ + 2ğ»+ 0.053 9.49E-07 4.74E-07
9ğ»2 + 2SO2âˆ’
4 2ğ»ğ‘† âˆ’ + 8ğ»2ğ‘‚ 110 0.00115 0.00515
C2Cl2H2 + 2ğ»2ğ¶2ğ»4 + 2ğ»+ + 2ğ¶ğ‘™ âˆ’ 0.15 1.55E-06 3.09E-06
Total 0.00517
DGU-NR: 200. 5096 Sample journal number: 835-2016-80391103 from Eurofins Denmark
A/S except for Mn4+
Mass of hydrogen required [g]= H2 consumption [mol] Â· H2 molar mass [g/mol] =
0.005 Â· 2.015 =0.018 g
ELS concentrate mass required[g] = ( Mass of H2 required [g] /58%) / mass of H2
produced per ELS concentrate kg [g/g] = 0.018 / 0.349 =0.052 g
The concentration of ELS Microemulsion is 3g/L, volume of ELS Microemulsion re-
quired [mL] = ELS concentrate mass [g] / concentration of ELS Microemulsion = 0.052/ 3 =
17.368 mL

82 A. Appendix A
A.4 Experimental setup
As shown in Figure A.3 , groundwater was added to the batch bottles using a 60-mL dis-
penser while the headspace was continuously flushed with nitrogen (black tube). Prior to
addition, the first 120 mL of groundwater was discarded.
Figure A.3: Addition of groundwater to the batches
A.4.1 GeoForm to water ratio
As the volume of amendments is lower, the better; the minimal amount of water required to
dissolve the powder GeoForm was tested, Figure A.4 shows the result of dissolving 12.16g
of GeoForm powder in 10, 20 and 50 mL of tap (aerobic) water. In 10 ml, it was thick like
cake batter, while 20 ml was still not fully dissolved; therefore, batches B and C, GeoForm
(12.16g), were dissolved in 50 ml of nitrogen-purged water as it had the appropriate con-
sistency.

A.4. Experimental setup 83
Figure A.4: Results of dissolving GeoForm powder in varying water quantities
Figure A.5 shows GeoForm and ELS Microemulsion prepared, which were mixed to-
gether with a magnetic stirrer and then injected into the microcosms.
Figure A.5: GeoForm and ELS Microemulsion prepared for batches B and C
A.4.2 KB-1
It is important to note that due to unexpected delays by the time the KB-1 was added to the
batches, it was technically expired by 3 weeks. This is probably why the KB-1 changed in
appearance as shown in Figure A.6 ; however, when shaken, it looked the same as before as
seen in Figure 5.7 .

84 A. Appendix A
Figure A.6: KB-1 comparison
A.4.3 Oxygen
The oxygen concentration was given in percentage Concentration of oxygen dissolved in wa-
ter [mg/L] = ğ¶ğ‘[ %]Â· 101325Â· ğ¾ â„[ ğ‘šğ‘œğ‘™ / ğ‘š3ğ‘ƒğ‘ ] [mol/L]* molecular weight of oxygen [mg/mol]
A.5 Sampling frequency and instruments
Figure A.7: Sampling frequency and volume

A.5. Sampling frequency and instruments 85
A.5.1 Chlorinated ethylenes and ethanes
Water samples are collected in containers with no headspace where after a certain volume is
transferred to a GCMS vial containing internal standard (chloroform). Heated and agitated
incubation of the samples is done automatically by the autosampler of the GC-MS prior to
injection. Calibration and control solutions are prepared in the same type of containers as
the samples are collected. T wo levels of calibrations are typically used; low (0.02 â€“ 40 Âµg/l)
and high (20 â€“ 2000 Âµg/l). Analysis time on the GC-MS is 30 min per sample including
GC cool down time. Sample vials are incubated in a rotary shaker at 250 rpm 85 Â°C for 5
min and 250 Âµl headspace is injected in splitless mode for the low range and 2:1 split mode
for the high range at 80 Â°C. Chromatographic separation is achieved on a 30 m x 0.32 mm
I.D x 20.00 Âµm film thickness HP-PLOT/Q capillary column (Agilent Technologies). The
initial column temperature is set to 40 Â°C for 4 min then ramped at 35 Â°C min-1 to 290 Â°C.
The final temperature is held for 7 min and the total run time is 18.1 min with Helium (1.6
ml min-1 ) as carrier gas. Detection is achieved using an Agilent 5975C electron impact (70
eV) triple-axis mass-selective detector. The mass-selective detector temperatures are 230 Â°C
for the EI source and 150 Â°C for the quadropole with the transfer line held at 250 Â°C. The
spectra are monitored in selected ion monitoring (SIM) mode.
A.5.2 Gas analysis
The runtime for each sample was 2.50 min at a temperature of 130â—¦ğ¶. Unfortunately methane
and carbon dioxide could not be measured as there was too long time between the sample
being extracted into the vials and measured so the samples came out as below the detection
limit even though when a sample for batch C3 and D2 directly from the headspace on day
55 showed 4.4% CO2 and 0.7% CO2 respectively.
A.5.3 VFA
Samples are injected automatically by the Thermo AI/AS 1310 Series Autosampler, 1 Âµl are
injected. Chromatographic separation is achieved on Agilent J&W GC capillary Column
(Length 30 meters, Diam. 0,530 mm, Film thickness 1,50 Âµm, Temp limits from 40Â°C to
250Â°C). The initial column temperature is set to 40Â°C for 6 min, then it rises by 20Â°C/min
to 200Â°C. The final temperature is held for 6 min and the total run-time is 20 min, with
Helium as carrier gas. Detection is achieved using a Thermo Scientific TRACEâ„¢ 1300 Gas
Chromatograph, equipped with a Flame Ionization Detector (FID).
A.5.4 Anions
Anions are separated on an anion-exchange column equipped with a guard column (IonPac
AS22, 4 x 250mm, Ionpac AS22 Guard column, 4 x 50 mm, Thermo Scientific). Separation is
achieved at isocratic conditions with 4.5 mM bicarbonate/ 1.4 mM carbonate as eluant. The
flow is kept constant at 1,2 ml/min, the column compartment at 20 degrees and the total

86 A. Appendix A
analysis time is 13 min per sample (Appendix A). A 200 Âµl sample loop is used, enabling
an injection volume up to 100 Âµl. The Dionex AERS 500 is an electrolytically regenerated
suppressor enabeling it to operated continuously with a water source as a regenarent. The
suppressor (Dionex AERS 500 for Carbonate, 4 mm, ThermoScientific) is operated at 41
mA when using the chromatographic conditions above with the compartment held at 20
Â°C. Detection is achieved by a conductivity detector (ICS-5000 Conductivity detector, Ther-
moScientific) held at 20 Â°C.

A.6. Results 87
A.6 Results
Relevant equations:
ğ‘€ğ‘‡ = ğ¶ğ‘¤ Â· ğ‘‰ğ‘¤ + ğ¾ğ‘‘ Â· ğ¶ğ‘¤ Â· ğ‘€ğ‘  + ğ¾ â„ Â· ğ¶ğ‘¤ Â· ğ‘‰ğ‘
ğ‘€ğ‘‡,ğ‘›ğ‘’ğ‘¤ = ğ‘€ğ‘‡,ğ‘ğ‘’ ğ‘“ ğ‘œğ‘Ÿğ‘’ âˆ’ ( ğ¶ğ‘¤ Â· ğ‘‰ğ‘¤,ğ‘œğ‘¢ğ‘¡ ) âˆ’ ( ğ¶ğ‘¤ Â· ğ¾ â„ Â· ğ‘‰ğ‘,ğ‘œğ‘¢ğ‘¡ )
The dry density was 265ğ‘”/ ğ‘ğ‘š 3 therefore the volume of limestone is 0.001132L
Figure A.8: Batch B1 and B3 on day 57, showing darkening of B3 similar to B1 on day 43
As shown in Figure A.9 , the internal standard area (ISTD Area) in yellow is double
the rest of the data and therefore the cDCE concentration of the first 16 samples should be
doubled from approximately 60 to 120ğœ‡ğ‘”/ ğ¿ .

88 A. Appendix A
Figure A.9: ISTD Area is the internal standard

A.6. Results 89
Figure A.10: cDCE levels in the control batches over time
Figure A.11: cDCE levels in GeoForm batches over time, on day 0, ELS Microemulsion and GeoForm was
added

90 A. Appendix A
Figure A.12: cDCE levels in GeoForm + KB-1 batches over time, on day 0, ELS Microemulsion, KB-1 and
GeoForm was added
Figure A.13: cDCE levels in KB-1 batches over time

A.6. Results 91
A.6.1 Molar fraction of CE
Figure A.14: Molar fraction of control over time
Figure A.15: Molar fraction of GeoForm over time

92 A. Appendix A
Figure A.16: Molar fraction of GeoForm+KB-1 over time
Figure A.17: Molar fraction of KB-1 over time, without ethane or ethene except for day 73 (day 51 if. Ethene
probably formed

A.6. Results 93
Figure A.18: VC levels in the control batches

94 A. Appendix A
Figure A.19: VC levels in the GeoForm batches. The outliers seen in B1 and B3 were ignored and not
considered because it was not seen on day 51 and there was no ethene formation for it to be degradation and
in this microcosm would have to be abiotic, if any. The interferences in the VC measurements is also a reason
the error bars are not in all points of the GeoForm-containing batches.

A.6. Results 95
Figure A.20: VC levels in the GeoForm+KB-1 batches

96 A. Appendix A
Figure A.21: VC levels in the KB-1 batches

A.6. Results 97
A.6.2 Manganese
Figure A.22: Average Mn concentration over time in the GeoForm containing batches (batch B and C)
A.6.3 Nitrate
Figure A.23: Nitrate levels over time in the control batches

98 A. Appendix A
Figure A.24: Nitrate levels over time in GeoForm batches

A.6. Results 99
Figure A.25: Nitrate levels over time in the GeoForm + KB-1 batches
Figure A.26: Nitrate levels over time in the KB-1 batches

100 A. Appendix A
A.6.4 NVOC
Table A.3 shows the exact NVOC levels in all 12 batches throughout the experiment. The
values in blue are within the calibration curve, while the empty cells are due to some of the
labels coming off the test tubes and, therefore, are impossible to identify. In Figure A.6.4 ,
the points above the red line are the points beyond the calibration curve and are shown
because samples were measured.
Table A.3: NVOC levels [mg/L] in all 12 batches throughout the experiment
Day A1 A2 A3 B1 B2 B3 C1 C2 C3 D1 D2 D3
0 1810 1828 49.3 81.3 1986 2036 1869 1900 1912 68.9 125.1 81.6
1 48.5 1997 17.4 55.4 1799 55.6 34.3 1900 87.7 41.4
3 63.0 2006 1887 1944 2060 1998 73.5 91.5 1887 90.2
7 66.9 1965 60.8 1877 36.9 1838 18.0 1838 108.1 1891
17 65.7 65.2 64.1 1978 67.5 82.2 2014 1960 1993 1856
21 1976 1818.3 35.7 1953
29 1906 52.5 82.1 1836 7.39 87.7 1879 93.7 68.5
34 1819 1966 99.6 1872 51.8 1903 64.6 66.0 1995
38 61.2 1940 63.3 1797 64.2 1998 121.6 1823 1971 1950
Figure A.27: NVOC levels in batch A, only points below the red line should be considered

A.6. Results 101
Figure A.28: NVOC levels in batch B, only points below the red line should be considered
Figure A.29: NVOC levels in batch C, only points below the red line should be considered

102 A. Appendix A
Figure A.30: NVOC levels in batch D, only points below the red line should be considered

B
APPENDIX B
103

104 B. Appendix B
Figure B.1: Geology of the Naverland site COWI, 2023

B. Appendix B 105
Figure B.2: The transect of the geology cross-section shown in Figure 4.1 from COWI, 2023

106 B. Appendix B
Figure B.3: Smaller scale geology of the site COWI, n.d."
Bayesian Optimization for Hyperparameter Optimization in Linear Gaussian Models,"Bayesian Optimization for Hyperparameter
Optimization in Linear Gaussian Models
Master Thesis
Mathematical Modelling and Computing
DTU Compute",,,"Lastly, we consider the last term in (6.2.2), n
2 log
yâŠ¤[Î¦Kâ„“Î¦âŠ¤+Î»I]
âˆ’1
y
n the numerator becomes:
yâŠ¤
h
Î¦Kâ„“Î¦âŠ¤ + Î»I
iâˆ’1
y = yâŠ¤
h
Î¦LLâŠ¤Î¦âŠ¤ + Î»I
iâˆ’1
y
= yâŠ¤
h
ËœÎ¦ ËœÎ¦âŠ¤ + Î»I
iâˆ’1
y
= yâŠ¤
h
U Î£V âŠ¤V Î£âŠ¤U âŠ¤ + Î»I
iâˆ’1
y
= yâŠ¤
h
U Î£Î£âŠ¤U âŠ¤ + Î»I
iâˆ’1
y
= yâŠ¤U
h
Î£Î£âŠ¤ + Î»I
iâˆ’1
U âŠ¤y
= ËœyâŠ¤
h
Î£Î£âŠ¤ + Î»I
iâˆ’1
Ëœy
(6.3.25)
Recalling that Î£ =
S
0


ËœyâŠ¤
1 ËœyâŠ¤
2
  
S2 + Î»I
âˆ’1 0
0 Î»âˆ’1I
 Ëœy1
Ëœy2

= ËœyâŠ¤
1
 
S2 + Î»I
âˆ’1 Ëœy1 + ËœyâŠ¤
2 Î»âˆ’1I Ëœy2. (6.3.26)
Noting the inverse matrix consist of only diagonal elements, we can rewrite the term as a
sum:
ËœyâŠ¤
1
 
S2 + Î»I
âˆ’1 Ëœy1 =
nX
i=1
Ëœy2
i
s2
i + Î» (6.3.27)
The last terms can then be expressed only in terms of U1 and y using (6.3.16)
ËœyâŠ¤
2 Î»âˆ’1I Ëœy2 = Î»âˆ’1yâŠ¤U2U âŠ¤
2 y = Î»âˆ’1yâŠ¤(I âˆ’ U1U âŠ¤
1 )y. (6.3.28)
In conclusion, using SVD we can write
yâŠ¤
h
Î¦Kâ„“Î¦âŠ¤ + Î»I
iâˆ’1
y =
nX
i=1
Ëœy2
i
s2
i + Î» + Î»âˆ’1yâŠ¤(I âˆ’ U1U âŠ¤
1 )y (6.3.29)
We hereby are able to compute PML using only diag (S), Î», y and U1.
6.4 Measure of Fit
In order to give an assessment of the quality of the estimate Ë†Î¸, we compute the fitÂ­score
as:
fit( Ë†Î¸) = 100 Â· (1 âˆ’ ||Î¸true âˆ’ Ë†Î¸||2
||Î¸true âˆ’ 1Âµ||2
), Âµ = 1
q 1âŠ¤Î¸true, (6.4.1)
where Î¸true âˆˆ Rq. Thus, a good fit will yield a value close to 100.
Bayesian Optimization for Hyperparameter Optimization in Linear Gaussian Models 17

6.5 Implementing the Hyperparameter Selection
As mentioned, this report uses Bayesian optimization for minimizing the costÂ­functions.
As previously unfolded, using SVD allows us to compute the PML and GCV critetion in
terms of diag(S), Î», y and U1. This allows us to eliminate the hyperparameter Î» by simply
selecting it using grid search. This results in a function which only depends on â„“ for which
Bayesian optimization is used for selecting â„“. The target function for the BO is evaluated
as follows:
Algorithm 2 T argetFunction
1: procedure T argetFunction(â„“, y, Î¦, Î»min, Î»max)
2: t â† [0, 1, 2, . . .]
3: Kâ„“ â† K(t, t, â„“)
4: L â† Cholesky(Kâ„“)
5: ËœÎ¦ â† Î¦L
6: U1, S, V âŠ¤ â† SVD( ËœÎ¦)
7: Î» â† [Î»min, . . . , Î»max]
8: for i = 1 . . . p do
9: ci â† PML/GCV(Î»i, S, y, U1) â–· Evaluating PML or GCV for a given â„“ in a grid
for varying Î»Â­values (Figure 6.2).
10: return min(c)
Note that Î» is selected each time the target function is evaluated. Figure 6.2 illustrates
how, for a fixed value of â„“, Î» is chosen via grid search. This approach allows Î» to be
selected while performing the SVD only once, which is a significant computational advanÂ­
tage.
10
 5
 0 5 10
log
1400
1600
1800
2000
2200
2400
2600
PML ( =  0.01)
= 1.67e-06
10
 5
 0 5 10
log
20
40
60
80
100
GCV ( =  0.01)
= 1.67e-06
Figure 6.2: PML and GCV as a function of Î».
When running the Baysian optimization for either PML or GCV we are able to select â„“ as
seen in Figure 6.3 and using the selected hyperparameter we can compute the estimated
impulse response (Figure 6.4).
18 Bayesian Optimization for Hyperparameter Optimization in Linear Gaussian Models","9 Conclusion
In this report the results demonstrate that the Stable Spline kernel is an effective prior
for estimating the impulse response. Furthermore, both the PML and GCV criteria prove
to be useful methods for hyperparameter selection. Here Bayesian optimization is an
advantageous tool for optimizing these costÂ­functions as it is able to arrive at the optimum
in few number of functions evaluations in comparison to grid search. Furthermore, we
have found that we are able to obtain an improvement of fit when combining the two costÂ­
functions. However, there is no clear pattern in how the costÂ­functions are weighed in
relation to each other. Therefore the method is not directly applicable for new unknown
systems. Furthermore, the numerical experiments carried out in this report show relatively
small improvements however given that the result are dependent on the choices made
for the numerical experiments one can assume that these could be improved as there still
remains unexhausted ways to combine the costÂ­functions.
Bayesian Optimization for Hyperparameter Optimization in Linear Gaussian Models 27

Bibliography
[1] Gianluigi Pillonetto and Giuseppe De Nicolao. â€œA new kernelÂ­based approach for
linear system identificationâ€. In: Automatica 46.1 (2010), pp. 81â€“93. issn: 0005Â­1098.
doi: https://doi.org/10.1016/j.automatica.2009.10.031 . url: https://www.sciencedirect.
com/science/article/pii/S0005109809004920.
[2] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for
Machine Learning. The MIT Press, 2006.
[3] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information SciÂ­
ence and Statistics) . 1st ed. Springer, 2007. isbn: 0387310738.
[4] Roman Garnett. Bayesian Optimization. Cambridge University Press, 2023.
[5] Martin S. Andersen and Tianshi Chen. â€œSmoothing Splines and Rank Structured MaÂ­
trices: Revisiting the Spline Kernelâ€. English. In: SIAM Journal on Matrix Analysis and
Applications 41.2 (2020), pp. 389â€“412. issn: 0895Â­4798. doi: 10.1137/19m1267349.
[6] Tianshi Chen and Lennart Ljung. â€œImplementation of algorithms for tuning paramÂ­
eters in regularized least squares problems in system identificationâ€. In: AutomatÂ­
ica 49.7 (2013), pp. 2213â€“2220. issn: 0005Â­1098. doi: https : / / doi . org / 10 . 1016 /
j . automatica . 2013 . 03 . 030. url: https : / / www . sciencedirect . com / science / article / pii /
S0005109813001994.
[7] Giulio Bottegal and Gianluigi Pillonetto. â€œThe generalized cross validation filterâ€. In:
Automatica 90 (2018), pp. 130â€“137. issn: 0005Â­1098. doi: https://doi.org/10.1016/
j . automatica . 2017 . 12 . 054. url: https : / / www . sciencedirect . com / science / article / pii /
S0005109817306416.
28 Bayesian Optimization for Hyperparameter Optimization in Linear Gaussian Models

A Additional Figures
A.1
20 40 60 80
83.85
83.90
83.95
84.00
20 40 60 80
95.10
95.12
95.14
95.16
95.18
20 40 60 80
94.4
94.6
94.8
95.0
95.2
20 40 60 80
93.75
94.00
94.25
94.50
94.75
95.00
20 40 60 80
89.6
89.8
90.0
90.2
20 40 60 80
91.70
91.75
91.80
91.85
91.90
20 40 60 80
90.6
90.7
90.8
90.9
91.0
20 40 60 80
88
89
90
91
92
20 40 60 80
91.4
91.6
91.8
92.0
20 40 60 80
94.55
94.60
94.65
94.70
94.75
Function evaluations
fit
Grid Search Bayesian Optimization (GCV)
Figure A.1: Fit for 10 SSM for varying numbers of function evaluations using GCV.
Bayesian Optimization for Hyperparameter Optimization in Linear Gaussian Models 29

A.2
-7 -5 -3 -1 1 3 5 7
0
5
10
SNR = 5
-7 -5 -3 -1 1 3 5 7
0
5
10
SNR = 10
-7 -5 -3 -1 1 3 5 7
0
5
10
SNR = 15
-7 -5 -3 -1 1 3 5 7
0
5
10
SNR = 20
-7 -5 -3 -1 1 3 5 7
0
5
10
SNR = 25
-7 -5 -3 -1 1 3 5 7
0
5
10
SNR = 30
log
fitcomb fitPML
Figure A.2: Difference in fit relatively to PML for 20 SSM.
30 Bayesian Optimization for Hyperparameter Optimization in Linear Gaussian Models

A.3
20 40 60 80
49.0
49.1
49.2
49.3
49.4
49.5
49.6
20 40 60 80
0.16326
0.16328
0.16330
0.16332
0.16334
20 40 60 80
1.352
1.354
1.356
1.358
1.360
20 40 60 80
0.0298
0.0299
0.0300
0.0301
20 40 60 80
3.50
3.52
3.54
3.56
20 40 60 80
0.08786
0.08787
0.08788
0.08789
0.08790
0.08791
20 40 60 80
0.3746
0.3748
0.3750
0.3752
0.3754
0.3756
20 40 60 80
24.330
24.335
24.340
24.345
24.350
20 40 60 80
92.0
92.5
93.0
93.5
94.0
20 40 60 80
65.390
65.395
65.400
Function evaluations
fit
Grid Search Bayesian Optimization (GCV)
Figure A.3: Minimum target function value for 10 SSM for varying numbers of function
evaluations using PML.
A.4
5
 0 5
2
0
5
 0 5
1.0
0.5
0.0
5
 0 5
0.25
0.00
0.25
5
 0 5
1.0
0.5
0.0
5
 0 5
0.4
0.2
0.0
5
 0 5
2
1
0
5
 0 5
2
0
5
 0 5
4
2
0
5
 0 5
0.2
0.1
0.0
5
 0 5
1.0
0.5
0.0
5
 0 5
1.0
0.5
0.0
5
 0 5
0.2
0.0
0.2
5
 0 5
0.4
0.2
0.0
5
 0 5
2
1
0
5
 0 5
0.5
0.0
5
 0 5
0.050
0.025
0.000
5
 0 5
0.2
0.0
5
 0 5
1
0
5
 0 5
1.0
0.5
0.0
5
 0 5
0.5
0.0
5
 0 5
1
0
5
 0 5
1.0
0.5
0.0
5
 0 5
0.1
0.0
5
 0 5
0.05
0.00
5
 0 5
1.0
0.5
0.0
5
 0 5
0.2
0.0
5
 0 5
1.0
0.5
0.0
5
 0 5
0.05
0.00
0.05
5
 0 5
10
0
5
 0 5
0.2
0.0
log
fitcomb fit*
 *
Figure A.4: Difference in fit as function of Î² for the remaining 30 SSM .
Bayesian Optimization for Hyperparameter Optimization in Linear Gaussian Models 31

T echnical
University of
Denmark
Building 303B
2800 Kgs. Lyngby
Tlf. 4525 1700
www.compute.dtu.dk"
Modelling price volatility in the maritime sector for decarbonization,"Modelling Market Dynamics within the Maritime Sector
T owards Realistic Maritime Predictions: Enhancing NavigaTE with MarketÂ Dynamics
MÃ¦rsk McÂ­Kinney MÃ¸ller Center for Zero Carbon Shipping
Master Thesis
DTU Management
Department of T echnology, Management and Economics","Term Definition
ui(qi) Utility function of agent i
Ï‰i Initial endowment of agent i
p Vector of commodity prices
qi(p) Demand of agent i at price p
Z(p) Excess demand: P
i qi(p) âˆ’ P
i Ï‰i
T able 2.3: Core components of Walrasian equilibrium.
Under regular assumptions such as continuity and convex preferences, this system has a soÂ­
lution. The prices are unique up to a scale, and the allocation is Pareto efficient: no agent can
be made better off without making another worse off [ 35, 36].
2.5.3 The TÃ¢tonnement Adjustment Process
T o describe how equilibrium might be reached in practice, LÃ©on Walras proposed a process
called tÃ¢tonnement, or â€trial and error.â€ Prices adjust over time based on the gap between supply
and demand:
Ë™p = Î³Z (p), Î³ > 0.
If the price is too low and there is excess demand, prices rise. If the price is too high, they fall.
This process helps explain how markets might move toward equilibrium even without a central
planner. Although purely theoretical, the concept gives useful insight into how decentralised
decisions can coordinate through price signals [ 37, 38].
The NavigaTE model applies these ideas to simulate how fuel prices adjust under supply conÂ­
straints and policy rules. Instead of setting prices directly, the model searches for the prices
that clear the market across different ports and vessel types. This is done by embedding the
concept of market equilibrium within an iterative optimisation routine. By doing so, NavigaTE
reflects how prices emerge from the balance of supply and demand, shaped by both physical
limits and regulatory signals.
2.6 Iterative Algorithms and Convergence Methods
In many largeÂ­scale optimisation problems, including those related to energy systems and transÂ­
port models, standard solution methods may struggle to handle nonlinearity, decentralisation,
or dynamic market feedback. T o address these challenges, iterative algorithms, heuristics, and
convergence methods are employed to gradually refine solutions over successive steps. This
section reviews key approaches relevant to fuel pricing and allocation in maritime models, unÂ­
derstanding this is crucial for grasping how the algorithm within NavigaTE operates.
2.6.1 Iterative Algorithms and the Simplex Method
Iterative algorithms approach optimisation problems by progressively refining solutions until
convergence is achieved. A common example is the simplex method, widely applied in linear
programming, which systematically moves between feasible solutions, improving the objective
function at each step. Its robustness and transparency make it particularly wellÂ­suited for appliÂ­
cations involving constrained resource allocation.
2.6.2 Heuristics
Heuristic methods provide practical alternatives in cases where exact optimisation is compuÂ­
tationally infeasible. Instead of guaranteeing perfect optimality, they focus on delivering good
enough solutions within reasonable time frames. These approaches are particularly valuable
in largeÂ­scale or complex systems, such as logistics, scheduling, or market simulations, where
quick decisionÂ­making can be more critical than absolute mathematical precision [ 39, 40].
Modelling Market Dynamics within the Maritime Sector 9

In all cases, the appeal of heuristic methods lies in their flexibility and speed, making them ideal
for realÂ­time or exploratory simulations.
2.6.3 Convergence Methods
Convergence methods are designed to guide iterative processes toward a stable and reliable
solution. In largeÂ­scale applications, both the speed and robustness of convergence play a
critical role in determining whether an algorithm is practical and efficient in realÂ­world settings.
Some types of convergence include global convergence , which guarantees convergence to
the global optimum from any starting point [ 41]Í¾ local convergence that assures convergence
only when the starting point is sufficiently close to a local optimum [ 42]. Linear convergence
indicates that the error between the estimate and the true solution decreases by a constant ratio
with each iteration [ 43]. In contrast, superlinear or quadratic convergence , often observed in
NewtonÂ­type methods,describes an accelerating rate of convergence at each step [ 44].
Stopping criteria usually include a small change in the objective value, minimal updates in
variables, or full satisfaction of constraints.
2.6.4 Bisection Method for Market Clearing
Originally a rootÂ­finding technique, the bisection method narrows an interval [a, b] by evaluating
the midpoint c = a+b
2 , and iteratively reducing the interval based on the sign of the function at
c [45]. In this project, the method is adapted to determine marketÂ­clearing prices: instead of
locating a root, it finds the price at which supply roughly matches demand.
Though the classical requirement f (a) Â· f (b) < 0 does not apply in this work, the principle of
convergence still holds due to the monotonic relationship between price and supplyÂ­demand
balance. High prices cause oversupplyÍ¾ low prices result in unmet demand. By narrowing the
interval between a price floor and ceiling, the algorithm converges to a practical price, close to
the equilibrium that supports market realism in the model.
2.6.5 Method of Successive Averages (MSA)
The Method of Successive Averages (MSA) is another iterative approach often used in traffic
flow and energy systems. It incrementally blends previous and current estimates to smooth out
fluctuations and avoid premature convergence [ 46].
The generic update formula is:
x(n+1) = Î±n Â· x(n) + (1 âˆ’ Î±n) Â· Calculation(x(n)), (2.1)
where:
â€¢ x(n) is the estimate at iteration n,
â€¢ Î±n = 1
n+1 is the averaging factor,
â€¢ Calculation(x(n)) is the updated estimate based on the current state.
Variants of MSA include:
â€¢ Standard MSA: Uses a simple 1/n decay.
â€¢ Weighted MSA: Assigns higher importance to more recent updates.
â€¢ Adaptive Averaging: Adjusts weights dynamically to improve stability.
â€¢ MSA with Memory Reset : Periodically resets the average to prevent stagnation.
â€¢ MSA with Polyak StepÂ­Size : Introduces adaptive step sizes for more controlled converÂ­
gence, using the form Î±k = 1
kp for p > 0.
10 Modelling Market Dynamics within the Maritime Sector

MSA is particularly wellÂ­suited for equilibrium problems involving feedback loops and decenÂ­
tralised decisions, such as dynamic fuel pricing and multiÂ­agent transport systems.
The iterative methods introduced in this section form the practical backbone of NavigaTEâ€™s fuel
pricing and allocation logic. T echniques like the bisection method are used to find marketÂ­
clearing prices by balancing supply and demand under real world constraints and regulatory
signals. While this work focuses on bisection, more advanced methods, such as the Method of
Successive Averages (MSA), are explored for future versions to handle more complex equilibÂ­
rium behaviour.
Modelling Market Dynamics within the Maritime Sector 11

12 Modelling Market Dynamics within the Maritime Sector

3 NavigaTE Framework
3.1 Framework Overview
NavigaTE is a systemÂ­level modelling framework that combines vessel operations, fuel supply,
and environmental regulations into a single optimisation platform. This chapter gives a clear
description of the core structure of the existing bunkering module. It outlines the main purpose
of the model, explains its key components, and describes the stepÂ­byÂ­step sequence the model
follows during each year of simulation.
Having this overview is important to better understand the new developments introduced in a
later stage of this thesis. It also helps distinguish which parts of the model remain as originally
designed, and which have been expanded to include these new market mechanisms.
3.1.1 Bunker Algorithmâ€™s Framework
The Bunker Algorithm is a key component of the NavigaTE framework, responsible for optimisÂ­
ing fuel allocation across vessels and ports while considering market dynamics, emissions, and
regulatory constraints. Its main functionalities include:
â€¢ FairÂ­Share Allocation : Ensures fair distribution across agents of fuel based on supply
and demand constraints.
â€¢ Shadow Prices: Calculates marginal costs of relaxing supply constraints to guide optimal
fuel allocation.
â€¢ Market Dynamics: Adjusts fuel prices dynamically for different market types (e.g., local,
global, emerging).
â€¢ Emission and Policy Integration: Incorporates emissions, levy penalties, and regulatory
constraints into the optimisation process.
â€¢ Optimisation: Uses linear programming (via Gurobi) to iteratively solve the bunkering
problem and achieve convergence.
3.1.2 Algorithm Workflow
The bunkering and market pricing routine follow four phases:
1. Initialisation
Sets up the linear programming model with the required inputs (vessel data, fuel options,
regulatory parameters). Dynamic properties such as policy states, emission factors and
initial fairÂ­share caps are also initialised at this stage.
2. Build & Update
At every simulation step, the model is built (first step) or updated (subsequent steps).
Decision variables, the objective function, and all constraints are created or modified to
reflect the current system state. This process includes:
â€¢ CleanÂ­Up: redundant elements are removed, outdated vessels or fuels, obsolete
constraints, and unused variables, to keep the model compact.
â€¢ Definition of Variables, Objectives and Constraints
â€“ Variables: bunkered fuels, fuel consumption, emissions released or stored, regÂ­
ulation variables.
Modelling Market Dynamics within the Maritime Sector 13

â€“ Objectives: minimise total cost (fuel + emissions storage)Í¾ secondary objectives
enforce interÂ­port fuel consistency and regulatory compliance.
â€“ Constraints: fuel mass balance, energy conservation, tank capacity, emission
caps, regional bunkering limits, and equality constraints for fair distribution.
3. Solve Methods
By default, the model solves using fairÂ­share allocation, iteratively adjusting allocations
and active constraints until convergence. If market dynamics are activated, a twoÂ­stage
algorithm is used: a ceiling iteration identifies binding constraints, followed by a bisection
loop that resolves marketÂ­clearing prices through successive fairÂ­share solves.
4. Transfer Methods
Outputs such as fuel allocations and emissions compliance are moved forward to the
next time step. Vessel and port objects are updated with bunkering decisions and fuel
consumption figures.
3.2 Core Frameworks: FairÂ­Share Allocation and MarketÂ­Clearing
3.2.1 FairÂ­Share in NavigaTEâ€™s Bunkering Algorithm
In the NavigaTE bunkering optimisation framework, the FairÂ­Share Allocation mechanism enÂ­
sures that vessels initially receive fuel supply from ports based on a proportional, equitable
distribution of available resources. This is particularly important in emerging markets where no
realÂ­time market price signal exists yet, and the allocation of fuel must respect supply constraints
while also reflecting fair treatment across vessels.
These are the key characteristics that ensure this algorithm works:
â€¢ Initialisation: Each vessel is allocated a share of the available fuel at each port proportional
to a precomputed fairÂ­share factor and available supply.
â€¢ Update Loop: After solving the optimisation model, unused portions of allocated fairÂ­share
are released and reallocated to other vessels in need, iteratively improving the solution.
â€¢ Convergence: This loop continues until the allocation stabilises (within a defined numeriÂ­
cal tolerance), ensuring supply is not wasted and allocations are as efficient as possible.
â€¢ Adaptivity: FairÂ­share is updated dynamically based on vessel behaviour (actual bunker
use vs. allocation) and the global remaining fuel pool.
This approach ensures a fair distribution of resources while also laying the groundwork for price
formation in interconnected markets. Once constraints become active, the system naturally
moves toward an economically meaningful equilibrium.
This approach uses only the available supply and each vesselâ€™s fair share factor to guide alloÂ­
cation, ensuring that fuel is directed to vessels with greater operational needs. In cases where
policies like emission caps restrict total fuel availability, the same fair share logic applies to the
reduced supply. As a result, the system automatically redistributes fuel across vessels, allowÂ­
ing for a clear understanding of how regulatory limits affect distribution fairness, even before
introducing any price mechanisms.
3.2.2 Market Clearing Mechanism
The market clearing mechanism implemented in this module draws inspiration from classical
Walrasian equilibrium theory, as explained in section 2.5.2, where prices adjust iteratively to
balance supply and demand across a decentralised system. In the context of multiÂ­fuel maritime
logistics, this translates into assigning implicit marginal prices Ï€pf to each portÂ­fuel combination.
14 Modelling Market Dynamics within the Maritime Sector","7 Results & Analysis
Following the scenario description, this chapter presents the outcomes of all the 16 simulaÂ­
tion scenarios. It is divided into four main sections, each covering one regulatory setup: No
Regulation, Regulation with Flexibility , Regulation without Flexibility , and Levy-
Based Regulation .
Within each section, results are structured in three parts. The first part, Performance Analytics,
analyses the computational performance of the model across scenarios. It covers total runtime,
model size, iteration counts, and solver effort required for each regulatory and market setting.
This helps to quantify the computational burden associated with more complex policy designs
and pricing mechanisms.
The second part, Market Dynamics , summarises the aggregated outcomes for each market
configuration (Cost-Only, Global, Local, and Emerging). It focuses on total cost, market price
levels, fuel demand, and fuel supply, allowing us to observe how market design shapes price
formation and allocation across fuels and ports.
The third part, Emissions and Expenses , reviews systemÂ­wide emissions, the adoption of lowÂ­
carbon fuels, and overall cost structure. This includes the breakdown of vessel costs, fuel
costs, and costs related to emissions, providing a clear picture of how compliance costs evolve
under each policy scenario and whether decarbonisation goals are achieved. It is important to
note that the main indicator that is used in the emissions analysis is the WellÂ­toÂ­Wake (WTW),
composed of the sum of wellÂ­toÂ­tank (WTT), measuring upstream emissions, and tankÂ­toÂ­wake
(TTW) emissions, that measures the downstream emissions.
Special attention is given to the Regulation with Flexibility scenario, as it uniquely exÂ­
plores both the official IMO GFS with a remedial cost of 380 USD per tonne of CO 2 equivalent
and a more ambitious variant at 1,200 USD. While the 380 USD case reflects current policy, it
proved insufficient to drive full compliance, fossil fuels remained economically viable. In conÂ­
trast, the 1,200 USD version achieved full compliance by 2050 and revealed a transition from
a demandÂ­constrained system (continued fossil use) to a supplyÂ­constrained one, where limÂ­
ited clean fuel production became the main bottleneck. This stronger price signal expanded
the marginal abatement cost curve and enabled broader uptake of lowÂ­carbon fuels, including
biofuels, blue fuels, and eÂ­fuels. The priceÂ­adjusting mechanism also played a greater role,
reallocating scarce fuels to vessels with the highest willingness to pay. Given these dynamÂ­
ics, the 1,200 USD penalty was extended to the Levy and Regulation with No Flexibility
scenarios to maintain comparability and realistic decarbonisation pressure.
After presenting the individual results of each regulatory scenario, the chapter closes with a
crossÂ­scenario comparison. This final section brings together the key findings across all policies,
analysing Performance Analytics , Market Dynamics , and Emissions and Expenses . The goal
is to clearly illustrate the tradeÂ­offs between different regulatory approaches and provide a solid
basis for the broader discussion developed in the following Chapter 8.
7.1 No Regulation Scenario
In the No Regulation scenario, the system operates without any policy constraints. This serves
as a baseline to observe how the shipping sector would naturally behave if decisions were driven
purely by market forces, where fuel choices and technology adoption depend only on operating
costs and fuel availability.
Modelling Market Dynamics within the Maritime Sector 43

7.1.1 Performance Analytics
Figure 7.1: Runtime evolution in seconds from 2025 to 2050 across the four market configuraÂ­
tions in the No Regulation scenario.
As illustrated in Figure 7.1, simulation runtimes decreases over time, evidenced by the gradiÂ­
ent of the runtime curves tapering off in later years. This trend is due to the modelâ€™s nested
timeÂ­stepping structure, which performs approximately n2
t
2 linear programs, where nt is the total
number of time steps. The majority of these solves occur in the early stages of the simulation,
as each new year requires fewer nested solves than the initial years. Consequently, although
the system becomes more complex over time, the computational effort per year declines.
Among the four market structures, the Local market takes the longest to solve, exceeding 5,200
seconds by 2050. This reflects the additional computational effort required when prices are
cleared separately at each port, resulting in more iterations and solver calls. The Global market
follows a similar trend but remains slightly faster, as prices are coordinated centrally rather than
independently. The difference highlights the extra complexity of decentralised price setting.
Meanwhile, both Cost-Only and Emerging configurations are much faster to compute. A full
disaggregated vision of these metrics, can be found in Appendix L.
7.1.2 Market Dynamics
The table in Appendix M summarises the key market indicators for the No Regulation scenario,
comparing total system costs, market prices, total supply and total demand across the four marÂ­
ket configurations. Cost-Only case yields zero prices, since fuel allocation is performed purely
based on production costs. Once market dynamics are introduced in the Global, Local, and
Emerging configurations, actual price signals emerge as the model balances supply and deÂ­
mand at each time step. These price signals generally lead to higher price levels compared to
the simple costÂ­based allocation, reflecting supply limitations and competition for available fuÂ­
els. Among these market configurations, the Local Market displays the highest prices because
these are determined independently at each port. This localised pricing creates more variabilÂ­
ity across regions driving prices above the levels observed in more centralised or coordinated
market structures.
As shown in Appendix G, total demand remains stable across the four configurations. The
apparent gap between demand and supply arises because unconstrained fuels such as LNG
and LSFO dominate bunker consumption but are not captured in the aggregated supply figures,
as their production is not limited within the model.
Across all configurations, fossil fuels remain dominant due to their lower production costs and
practically unconstrained availability within the model. In the Cost-Only case, allocation is
strictly costÂ­driven, resulting in LSFO and LNG fully covering almost the entire demand. BioÂ­
fuels such as FAME and Bio-oil (Pyrolysis) only enter the mix marginally, limited by their
relatively small available supply capacity. No pricing mechanism exists in this setup, so the
allocation strictly follows production cost hierarchies.
44 Modelling Market Dynamics within the Maritime Sector

When price coordination is introduced in the Global, Local, and Emerging configurations, bioÂ­
fuels gain slightly more market share as vessels adjust willingness to pay in response to price
signals. These pricing mechanisms allow some substitution towards biofuels when market conÂ­
ditions justify higher costs. However, the price premium required to trigger significant biofuel
uptake remains high, which limits their expansion. In the Local configuration, geographically
changing prices result in slightly more biofuel use in certain ports facing localised supply shortÂ­
ages. The Emerging model allows for more gradual shifts at the vessel level, but fossil fuels
remain the dominant energy source throughout the simulation period in all scenarios.
Complete regional results and full supplyÂ­demand and priceÂ­cost plots for all locations are proÂ­
vided in Appendix G.2 (Cost-Only), G.2.3 (Global), G.2.4 (Local), and G.2.2 (Emerging).
7.1.3 Emissions and Expenses
(a) T otal Equivalent Emissions WTW (in tons
CO2eq/year).
(b) T otal Intensity Equivalent Emissions WTW (in
g CO 2eq/MJ).
Figure 7.2: Global WTW emissions across the four different market typologies for the No
Regulation setup
In the absence of regulatory constraints, the overall emissions across all market types remain
very close, this is due to the increase in demand. Since fuel allocation is mainly driven by
production cost and technical feasibility, fossil fuels dominate across the board, leading to similar
WTW emission totals regardless of market structure. In the equivalent WTW emissions it can
be seen that from 2028 the emissions take a shift and start increasing steadily through the
years, this is because the absence of regulatory constraints allows vessels to continue relying
on high emission fuels as demand grows, and no policy mechanisms are in place to redirect
investments toward cleaner alternatives. But, when looking at intensity metrics (g of CO 2/MJ
used of fuel) they follow a different pattern. From year 2025 they tend to decrease, due to the
works of decarbonisation in the year 2032, there is a jump in the decrease, probably due to the
early integration of lowÂ­carbon fuels, gradual technology optimisation and fleet renewal, which
reduces emissions per unit of energy from 93 g of CO 2/MJ to 85 g of CO 2/MJ.
Figure 7.3: Stacked expenses in USD (yÂ­axis) across the four market configurations (xÂ­axis) in
the No Regulation scenario.
Modelling Market Dynamics within the Maritime Sector 45

The expense structure across all markets remains almost identical, seen in detail in T able N.1.
Since there are no penalties or regulatory charges applied, regulation related costs are zero.
The vessel related expenses are the largest contributor to total system costs, reflecting fleet
turnover and investment requirements. Fuel related expenses stay stable across markets, with
minor differences caused by small shifts in fuel allocation depending on the market pricing coÂ­
ordination applied.
7.2 Regulation with Flexibility
The Regulation with Flexibility scenario is particularly important as it represents the most
realistic depiction of current regulatory trends in maritime decarbonisation. In this section, both
the officially announced IMO remedial unit GFS of 380 USD per tonne of CO 2 equivalent and a
higher alternative penalty of 1,200 USD are explored.
Initial model tests confirmed that the IMO remedial unit GFS of 380 USD alone does not suffiÂ­
ciently discourage the use of fossil fuels, allowing highÂ­emission fuels to remain economically
competitive. By contrast, the 1,200 USD penalty triggers a stronger shift in market behaviour.
Under this higher carbon price, the system moves from being primarily demandÂ­constrained,
where fuel users simply choose the cheapest available options, to becoming supplyÂ­constrained,
where the limited availability of lowÂ­carbon fuels becomes the key limiting factor. This leads to
greater uptake of biofuels, stronger differentiation in fuel allocation, and higher utilisation of price
coordination mechanisms across the various market configurations.
7.2.1 IMO Regulation Value Â­ 380 USD
Performance Analytics
Figure 7.4: Runtime evolution in seconds from 2025 to 2050 across the four market configuÂ­
rations in the Regulation with Flexibility under the IMO remedial unit GFS of 380 USD
scenario.
The introduction of flexible regulation under the IMO GFS of 380 USD already increases comÂ­
putational complexity compared to the No Regulation scenario. As shown in Figure 7.4, all
market configurations experience longer runtimes due to the need for allowance trading logic,
iterative market clearing, and dynamic price adjustments. While the system remains numerically
stable, additional layers of coordination are required to balance emissions between compliant
and nonÂ­compliant vessels, introducing extra iterations into the model.
The Cost-Only configuration is now the fastest to compute, as no price coordination nor trading
takes placeÍ¾ fuel allocation continues to follow simple cost minimisation. The runtime increases
only slightly relative to the unregulated case due to the presence of additional regulatory conÂ­
straints being checked at each solve.
The Emerging configuration remains relatively efficient but shows longer solve times than the
Cost-Only case. This is due to more frequent fairÂ­share iterations. These are likely triggered by
46 Modelling Market Dynamics within the Maritime Sector

new fuel prices that affect decisions elsewhere in the model, which then leads to more complex
linear problems solved in later time steps.
Computational effort is highest in the Global and Local configurations. In Global, the price alÂ­
gorithm operates at the fleet level, increasing fair share iterations and market updates, though
coordination across ports helps keep solution times manageable. In Local, prices are cleared
independently at each port, leading to a larger number of iterations. This disaggregated apÂ­
proach results in a runtime that nearly triples compared to the Cost Only configuration by 2050.
The solver performance data in T able L.3 highlights the computational impact of introducing
price coordination and market clearing. Iteration counts increase significantly, particularly in
the Local market, which records over 11,000 market dynamic iterations and 51,000 fair share
iterations. This is driven by the decentralised price determination process at the port level. In
contrast, Global requires fewer iterations (6,500 and 28,000 respectively), while Cost-Only and
Emerging remain much lower (1,400). Despite this, the model remains computationally feasible
under flexible regulation. All numeric results can be seen in Appendix L.
Market Dynamics
The table in Appendix M provides an aggregated overview of market behaviour under the
Regulation with Flexibility scenario with the IMO GFS of 380 USD. As in previous secÂ­
tions, results are presented across the four market structures.
Overall, introducing flexible regulation triggers a modest diversification of the fuel mix, though
fossil fuels remain dominant. As observed in Appendix H.2, LSFO demand steadily declines
across all markets, while LNG follows a bimodal trend, peaking around 2035 before temporarily
declining and then increasing again toward 2050. This reflects the systemâ€™s progressive but still
limited shift toward alternative fuels under the current IMO pricing level.
Low and zero carbon fuels such asFAME, Bio-oils, Bio-methane, Bio-methanol, Blue Ammonia ,
and eÂ­fuels are increasingly introduced into the fuel mix. However, their overall penetration reÂ­
mains limited due to cost competitiveness and compatibility constraints. Some fuels, such as
eÂ­Methanol and eÂ­Ammonia, are technically available but remain largely unused, as their prices
are still not attractive enough to drive widespread adoption.
The Cost-Only configuration serves as a reference baseline, showing minimal deviation from
pure costÂ­optimal allocations. In the Global Market, despite introducing price coordination,
demand patterns remain quite close to the costÂ­driven allocation, suggesting that at this penalty
level, the market price signal alone is not yet sufficient to induce strong behavioural changes.
In contrast, the Local and Emerging markets introduce more regional and vesselÂ­level heteroÂ­
geneity. Decentralised price signals lead to some variations in fuel selection across locations
and time periods, as individual ports and vessels respond differently to local conditions and
willingnessÂ­toÂ­pay. Nevertheless, while these more granular mechanisms allow some redistriÂ­
bution of fuel allocation, they do not fundamentally shift the systemÂ­wide fuel mix under the
current IMO GFS. Fossil fuels continue to meet the majority of demand, and supply remains
comfortably above consumption across all market setups.
Emissions and Expenses
The introduction of the IMO flexible regulation at 380 USD makes overall emissions shift. T otal
WTW emissions remain close across all market configurations.
Modelling Market Dynamics within the Maritime Sector 47

(a) T otal Equivalent WTW Emissions (in tons
CO2eq/year).
(b) T otal Intensity Equivalent WTW Emissions
(in g CO 2eq/MJ).
Figure 7.5: Global WTW Emission across the four different market typologies in the Regulation
with Flexibility IMO GFS setup.
For both metrics, T otal Equivalent WTW Emissions and Intensity Equivalent emissions there is a
decreasing trend in both, with a significant step decrease in the year 2036, which forces a switch
away from high emission fuels. Vessels that cannot comply economically begin transitioning
to cleaner fuels or rely on emissions trading to avoid penalties, resulting in an abrupt system
wide emissions drop. Looking at Intensity Equivalent Emissions, the value decreases from the
93g gCO 2eq/MJ in 2025 to around 45 gCO 2eq/MJ in 2050, achieving lower values than the No
Regulation scenario.
In terms of total system expenses, all configurations display relatively stable cost structures
under the IMO regulation. The Cost-Only scenario maintains the lowest overall expenditures,
while Emerging Market shows slightly higher total expenses due to modestly higher biofuel
deployment and more vessel adjustments.
FuelÂ­related expenses contribute to approximately one quarter of total costs, with vesselÂ­related
investments representing the largest share across all configurations. RegulationÂ­related exÂ­
penses remain relatively small in this IMO case, ranging between 1.19 and 1.22 trillion EUR,
indicating that the 380 USD penalty, while nonÂ­negligible, does not significantly increase comÂ­
pliance costs under current conditions.
Overall, the relatively minor differences across market types suggest that the IMO 380 USD
carbon price still allows the industry to comply mostly through businessÂ­asÂ­usual operations,
with limited restructuring of the fuel mix and technology adoption. These results are summarised
in T ableN.
Figure 7.6: Stacked expenses in USD (yÂ­axis) across the four market configurations (xÂ­axis) in
the Regulation with Flexibility scenario under the IMO remedial unit GFS of 380 USD.
7.2.2 High Penalty Value Â­ 1200 USD
Performance Analytics
When applying the 1200 USD/ton CO 2eq penalty in the Regulation with Flexibility sceÂ­
nario, the model exhibits a clear increase in computational effort compared to lower regulation
48 Modelling Market Dynamics within the Maritime Sector","levels. The stronger carbon price forces the system to adjust fuel allocation more drastically,
creating increasingly complex market dynamics that directly impact computational performance.
Figure 7.7: Runtime evolution in seconds from 2025 to 2050 across the four market configuraÂ­
tions in the Regulation with Flexibility under the 1200 USD penalty scenario.
As shown in Figure 7.7, the Cost-Only and Emerging Market setups remain computationally
stable. In contrast, introducing price formation, especially in the Global and more so in the
Local Market , significantly increases solver workload due to repeated market coordination.
In decentralised setups like the Local Market , regional price adjustments run independently,
increasing iterations and extending runtime. All numeric results are detailed in Appendix H.
Overall, stronger carbon pricing drives major shifts in fuel use while increasing market comÂ­
plexity. As fossil fuels lose viability, limited lowÂ­carbon fuels must be distributed across diverse
vessels and regions, adding negotiation cycles, especially in decentralised settings like the
Local Market . Despite this, the model remains tractable, showing robustness under ambitious
decarbonisation targets.
Market Dynamics
Aggregated results (table in Appendix M) show that system costs remain nearly identical across
market configurations under the 1,200 USD penalty, indicating stable total expenditures reÂ­
gardless of pricing flexibility. However, compared to the 380 USD IMO case, both prices and
supply increase substantially, signalling a shift from demandÂ­constrained to supplyÂ­constrained
dynamics. Prices are highest in the Emerging Market , followed by Local and Global, reflectÂ­
ing stronger marginal bidding behaviour in more flexible resale environments. Demand slightly
drops in the Emerging Market , while Cost-Only maintains the highest levels. These differences
illustrate how stronger carbon pricing reshapes market interactions and redistributes pricing
pressure, warranting further fuelÂ­level analysis.
Across scenarios, the system transitions from demandÂ­driven in No Regulation to increasingly
supplyÂ­constrained under Regulation with Flexibility (380 USD) and Regulation with
Flexibility (1200 USD), with rising prices and expanded lowÂ­carbon supply. Under Reg. No
Flex (1200 USD), demand drops further due to limited fuel substitution options.
Under the 1,200 USD penalty, demand patterns diverge more across market types as it can
be seen in Appendix I. LSFO steadily declines from 10 to 3 EJ/year by 2050 in all scenarios,
showing the effectiveness of stronger carbon pricing. LNG, however, follows distinct paths
in each market, reflecting sensitivity to local conditions. FAME and BioÂ­oil (HtL) show stable
demand across cases, aside from a brief drop in FAME demand in the Local Market in 2030.
Other fuels like eÂ­methanol, blue ammonia, and bioÂ­methane vary more significantly, with uptake
shaped by how each market responds to evolving price signals.
With the system now supplyÂ­constrained, the pricing algorithm plays a greater role in shaping
the fuel mix. The higher penalty shifts the focus from cost minimisation to marginal abatement
Modelling Market Dynamics within the Maritime Sector 49

efficiency, activating a wider range of lowÂ­ and zeroÂ­carbon fuels. Most options are now supÂ­
plied in substantial volumes, while grey ammonia sees no supply due to its emissions profile.
Although supply timing is broadly consistent across scenarios, volumes and rampÂ­up patterns
vary: Global and Emerging Markets show smoother trends, while the Local Market sees sharper
shifts, reflecting regional imbalances and fragmented competition.
Despite varying supplyÂ­demand outcomes, market prices for fuels remain within a similar range
across the Global, Local, and Emerging configurations, thanks to the flexibility allowed in fuel reÂ­
sale. The Global Market presents the most stable pricing, while the Local Market shows more
pronounced spikes, reflecting regional bottlenecks and supply frictions. The Emerging Market ,
in contrast, smooths out these price fluctuations due to its vesselÂ­level aggregation. Importantly,
the decoupling of price from pure production cost enables fuels with higher marginal abatement
potential to enter the system, even if their base cost is not the lowest. This price differentiation
is critical in reallocating fuel supply toward highÂ­value vessels and promotes a more responsive
and efficient fuel allocation system, especially under supplyÂ­constrained conditions.
Emissions and Expenses
The introduction of a carbon penalty of 1200 USD/tCO2eq under the Regulation with Flexibi-
lity scenario results in stronger decarbonisation effects across all market configurations. This
higher penalty leads to earlier fuel switching and emissions reductions throughout the simulaÂ­
tion.
(a) T otal WTW Equivalent Emissions (in tons
CO2eq/year).
(b) T otal Intensity Equivalent WTW Emissions (in
g CO 2eq/MJ).
Figure 7.8: Global WTW Emission across the four different market typologies in the Reg. with
Flexibility 1200 USD
As shown in Figure 7.8a, total WTW emissions decline equally sharp until 2029. After this
point, the penalty accelerates stability among market structures, although Cost Only continues
to produce the highest emissions due to lower fuel prices and higher demand. Figure 7.8b
shows that WTW intensity emissions remain relatively stable until 2028, followed by a marked
decline and reaching its lowest value of 29 gCO 2eq/MJ. This reflects the shift toward cleaner
fuels and more efficient allocation enabled by the flexibility mechanism.
Stronger decarbonisation leads to higher overall system costs, mainly due to increased fuel
expenses. Cleaner fuels are more expensive to produce and supply, with the Emerging Market
showing the highest fuel costs due to greater use of lowerÂ­emission alternative fuels. In contrast,
the Cost Only scenario remains cheapest, as it avoids market pricing and relies on lowerÂ­cost
fuels, which also drive up demand.
VesselÂ­related costs rise as well, driven by investments in retrofitting and new ship designs,
though these increases are more moderate than those for fuel.
Regulatory expenses vary across markets. In the Cost Only case, operators face higher penalÂ­
ties because they do not adjust fuel use. Markets with flexibility, such as Global, Local, and
50 Modelling Market Dynamics within the Maritime Sector","9 Conclusion
9.1 Hypothesis Validation
This thesis was guided by two core hypotheses: one addressing the mathematical feasibility
of introducing marketÂ­based price formation into maritime fuel allocation models, and the other
focusing on the analytical insights that these price signals could reveal about system behaviour
under decarbonization pathways.
9.1.1 Mathematical Hypothesis Validation
The first hypothesis proposed that it would be possible to model fuel prices directly within an
optimization framework, rather than assuming them as exogenous inputs. T o test this, the model
combined linear programming with a price search mechanism using a bisection algorithm. Since
fuel prices were not known in advance, the algorithm iteratively solved the LP for different price
levels, progressively narrowing the search interval until supply and demand were balanced.
Once equilibrium was reached, the shadow prices from the LP reliably reflected the marginal
value of fuel at each market state.
Following the price calculation, fuel was allocated across vessels based on their willingness to
pay, which incorporated fuel costs, regulatory penalties, and vesselÂ­specific preferences. The
allocation process respected both supply constraints and market behaviour. Through this itÂ­
erative combination of price search, optimization, and fuel allocation, the model consistently
converged to stable solutions across all regulatory and market scenarios. This confirms that
marketÂ­based pricing can be practically implemented within maritime fuel allocation models,
providing a realistic and computationally stable framework for analysing decarbonization poliÂ­
cies.
9.1.2 Analytical Hypothesis Validation
The second hypothesis argued that marketÂ­based prices would provide deeper insights into
how the maritime fuel system reacts to different regulatory designs, compared to traditional
costÂ­minimization models. The simulation results fully confirm this: as price signals emerge
under different policy scenarios, they directly influence fuel choices, investment strategies, and
vessel retrofitting decisions.
At low penalty levels, such as the IMOâ€™s current 380 USD per ton, price signals remain too
weak to drive meaningful decarbonization. Fossil fuels continue to dominate, and cleaner fuÂ­
els see limited adoption. In contrast, when penalties are increased to higher levels like 1200
USD per ton, the economic incentives become strong enough to encourage both fuel switching
and substantial investments in clean technologies. These effects are even more visible under
supplyÂ­constrained conditions, where price formation not only reflects resource scarcity but also
serves as a clear signal for longÂ­term investment planning.
In summary, the analytical hypothesis is validated: introducing price formation into the model
allows for a richer and more realistic representation of market behaviour, providing valuable
insights for the maritime sector and its stakeholders. The results show that prices not only guide
current allocations, but also shape longÂ­term decisions, making this approach highly useful for
policymakers, shipowners, fuel producers, and investors.
9.2 Main Insights Across Scenarios
The crossÂ­scenario comparison highlights several key conclusions:
Modelling Market Dynamics within the Maritime Sector 63

â€¢ Decarbonization effectiveness: Strong price signals are essential. The Regulation with
Flexibility 1200 USD scenario is the only one capable of triggering substantial fuel switchÂ­
ing and significant emission reductions. Lower penalties such as the current IMO 380
USD level do not create enough incentive for largeÂ­scale decarbonization.
â€¢ System costs: While the No Regulation scenario remains the least costly, it delivers no
decarbonization. The Regulation with Flexibility 1200 USD scenario hits a better balance,
achieving meaningful emissions reductions while controlling total system costs. In conÂ­
trast, rigid regulations and levyÂ­based mechanisms lead to higher overall expenses due
to their limited market flexibility.
â€¢ Market behaviour: The fairÂ­share allocation algorithm enables vessels to adjust fuel
choices in response to prevailing market signals, reflecting realistic decisionÂ­making proÂ­
cesses in maritime operations. While all regulatory types influence fuel selection, flexible
regulation tends to yield more efficient allocations due to greater adaptability in response
to price variations.
â€¢ Computational performance: While the model performs well, solution times increase
significantly under more complex regulatory scenarios, especially with higher penalties
and price coordination mechanisms. Future work may explore improved algorithms to
accelerate convergence without sacrificing pricing accuracy.
9.3 Thesis Implications
The findings suggest that combining flexible market mechanisms with strong carbon pricing repÂ­
resents the most effective policy pathway for maritime decarbonization. This approach ensures
environmental targets are met while allowing the market to adjust investments, fuel choices,
and technology adoption efficiently. Such a framework offers valuable signals not only for regÂ­
ulators, but also for shipowners, fuel producers, and technology providers.
Beyond the academic contribution, this modelling approach holds significant importance. First, it
offers the maritime sector a much more realistic tool to anticipate how markets will evolve under
different policy frameworks. Second, it provides actionable information for global stakeholders,
including shipowners, fuel suppliers, policymakers, and financial institutions, to make better
investment and operational decisions in the face of regulatory uncertainty. And third, due to
its relevance and applicability, this methodology will be directly integrated into the forecasting
tools used by the Center to support future scenario analysis and policy discussions at the global
level.
64 Modelling Market Dynamics within the Maritime Sector

References
[1] International Maritime Organization. IMO 2023 Revised Strategy on Reduction of GHG
Emissions from Ships. https://www.imo.org/en/MediaCentre/PressBriefings/pages/23Â­ghgÂ­
strategy.aspx. 2023.
[2] DNV. â€œMaritime Forecast to 2050â€. In: DNV Energy Transition Outlook (2024). url: https:
//www.dnv.com.
[3] International Energy Agency. â€œAmmonia T echnology Roadmapâ€. In: (2021).
[4] D. Giezen and H. Johnson. â€œEconomic feasibility of zeroÂ­carbon fuels in shippingâ€. In:
Transportation Research Part D: Transport and Environment 117 (2023), p. 102855.
[5] International Maritime Organization. Initial IMO Strategy on Reduction of GHG Emissions
from Ships. Resolution MEPC.304(72). London. 2018.
[6] International Maritime Organization. Revised IMO GHG Strategy. Resolution MEPC.377(80).
London. 2023.
[7] European Union. Regulation (EU) 2023/1805 on the Use of Renewable and Lowï¿¿Carbon
Fuels in Maritime Transport . Official Journal of the European Union. Brussels. 2023.
[8] DNV. Maritime Forecast to 2050 . HÃ¸vik, Norway: DNV, 2024.
[9] S. van der Giezen and coauthors. â€œCost Prospects for Green Ammonia and Methanol in
Shippingâ€. In: Energy Policy 174 (2023), p. 113423.
[10] International Renewable Energy Agency. Global Hydrogen Trade Outlook . Abu Dhabi:
IRENA, 2024.
[11] F Gonzalez and J Tjong. â€œSynthetic fuels and the road to commercial viabilityâ€. In: Energy
Policy 162 (2022), p. 112763.
[12] International Energy Agency. Global Hydrogen Review 2023 . 2023. url: https://www.iea
.org/reports/global-hydrogen-review-2023 .
[13] Victor Ahlqvist, PÃ¤r Holmberg, and Thomas T angerÃ¥s. â€œA survey comparing centralized
and decentralized electricity marketsâ€. In: Energy Strategy Reviews 39 (2022), p. 100745.
[14] J. Tirole. The Theory of Industrial Organization . MIT Press, 1988.
[15] Reuters. Platts Launches Daily Alternative Bunker Benchmarks . 2025. url: https://www.r
euters.com/%E2%80%A6.
[16] G. Gabrielsen and E. HjelmÃ¥s. â€œPort Bottlenecks and Fuel Price Premiaâ€. In: TransportaÂ­
tion Research Part E 173 (2023), p. 103994.
[17] Z. Li and X. Zhao. â€œDesigning carbon pricing for transport fuel marketsâ€. In: Energy Policy
156 (2021), p. 112398.
[18] H. P . Sapkota R.Kunwar. â€œAn Introduction to Linear Programming Problems with Some
RealÂ­Life Applicationsâ€. In: European Journal of Mathematical Sciences (2022). url: https
://ej-math.org/index.php/ejmath/article/view/108.
[19] Thomas Dueholm Hansen, Haim Kaplan, and Uri Zwick. â€œDantzigâ€™s pivoting rule for shortÂ­
est paths, deterministic MDPs, and minimum cost to time ratio cyclesâ€. In: Proceedings of
the 2014 Annual ACMÂ­SIAM Symposium on Discrete Algorithms (SODA) , pp. 847â€“860.
doi: 10.1137/1.9781611973402.63. eprint: https://epubs.siam.org/doi/pdf/10.1137/1.97816
11973402.63. url: https://epubs.siam.org/doi/abs/10.1137/1.9781611973402.63.
[20] Oliver Friedmann, Thomas Dueholm Hansen, and Uri Zwick. â€œSubexponential lower bounds
for randomized pivoting rules for the simplex algorithmâ€. In: Proceedings of the FortyÂ­Third
Annual ACM Symposium on Theory of Computing . STOC â€™11. San Jose, California, USA:
Association for Computing Machinery, 2011, pp. 283â€“292. isbn: 9781450306911. doi: 10
.1145/1993636.1993675. url: https://doi.org/10.1145/1993636.1993675.
[21] J. Oji et al. â€œNumerical Optimization of Sand Casting Parameters Using the Dantzigâ€™s
Simplex Methodâ€. In: Journal of Minerals and Materials Characterization and Engineering
(2013). url: https://www.scirp.org/journal/paperinformation?paperid=36919.
Modelling Market Dynamics within the Maritime Sector 65

[22] V Gabrel and C Murat and. â€œRobustness and duality in linear programmingâ€. In: Journal of
the Operational Research Society 61.8 (2010), pp. 1288â€“1296. doi: 10.1057/jors.2009.81.
eprint: https://doi.org/10.1057/jors.2009.81. url: https://doi.org/10.1057/jors.2009.81.
[23] Richard Anstee. Proof of Strong Duality . url: https://personal.math.ubc.ca/~anstee/math3
40/340strongduality.pdf.
[24] Hailong Li et al. â€œA review of the pricing mechanisms for district heating systemsâ€. In:
Renewable and Sustainable Energy Reviews 42 (2015), pp. 56â€“65. issn: 1364Â­0321. doi:
https://doi.org/10.1016/j.rser.2014.10.003 . url: https://www.sciencedirect.com/science/arti
cle/pii/S136403211400820X.
[25] Tim Nelson and Judith McNeill. â€œChapter 6 Â­ Role of Utility and Pricing in the Transitionâ€. In:
Future of Utilities Utilities of the Future . Ed. by Fereidoon P . Sioshansi. Boston: Academic
Press, 2016, pp. 109â€“128. isbn: 978Â­0Â­12Â­804249Â­6. doi: https://doi.org/10.1016/B978-
0-12-804249-6.00006-3 . url: https://www.sciencedirect.com/science/article/pii/B978012804
2496000063.
[26] Ranhang Zhao and Shouyu Chen. â€œFuzzy pricing for urban water resources: Model conÂ­
struction and applicationâ€. In: Journal of Environmental Management 88.3 (2008), pp. 458â€“
466. issn: 0301Â­4797. doi: https://doi.org/10.1016/j.jenvman.2007.03.004 . url: https://ww
w.sciencedirect.com/science/article/pii/S0301479707000965.
[27] Mark Kagan, Frederick van der Ploeg, and Cees Withagen. â€œBattle for Climate and Scarcity
Rents: Beyond the LinearÂ­Quadratic Caseâ€. In: Dynamic Games and Applications (2015).
doi: 10.1007/s13235-015-0154-2 .
[28] G. Bas et al. â€œMarPEM: An agent based model to explore the effects of policy instruments
on the transition of the maritime fuel system away from HFOâ€. In: Transportation Research
Part D: Transport and Environment 55 (2017), pp. 162â€“174. issn: 1361Â­9209. doi: https:
//doi.org/10.1016/j.trd.2017.06.017 . url: https://www.sciencedirect.com/science/article/pii
/S1361920916308215.
[29] Dawn Cassandra Parker and T atiana Filatova. â€œA conceptual design for a bilateral agentÂ­
based land market with heterogeneous economic agentsâ€. In: Computers, Environment
and Urban Systems 32.6 (2008). GeoComputation: Modeling with spatial agents, pp. 454â€“
463. issn: 0198Â­9715. doi: https://doi.org/10.1016/j.compenvurbsys.2008.09.012. url: https
://www.sciencedirect.com/science/article/pii/S0198971508000793.
[30] AslÄ±han Arslan. â€œShadow vs. market prices in explaining land allocation: Subsistence
maize cultivation in rural Mexicoâ€. In: Food Policy 36.5 (2011), pp. 606â€“614. issn: 0306Â­
9192. doi: https://doi.org/10.1016/j.foodpol.2011.05.004 . url: https://www.sciencedirect.co
m/science/article/pii/S0306919211000686.
[31] Alfred Marshall. Principles of Economics . Macmillan, 1920.
[32] Andreu Masï¿¿Colell, Michael D. Whinston, and Jerry R. Green. Microeconomic Theory .
Oxford University Press, 1995.
[33] David W. Bunn. Energy Modelling and the Management of Electricity Systems . Edward
Elgar, 2004.
[34] David W. Bunn. â€œDesigning the electricity market: experience and lessons learnedâ€. In:
IEEE Power and Energy Magazine 11.2 (2013), pp. 40â€“47.
[35] GÃ©rard Debreu. Theory of Value: An Axiomatic Analysis of Economic Equilibrium . New
Haven: Yale University Press, 1959.
[36] Kenneth J. Arrow and GÃ©rard Debreu. â€œExistence of an equilibrium for a competitive econÂ­
omyâ€. In: Econometrica 22.3 (1951), pp. 265â€“290.
[37] Kenneth J. Arrow and Leonid Hurwicz. â€œOn the stability of the competitive equilibrium, Iâ€.
In: Econometrica 26.4 (1958), pp. 522â€“552.
[38] Herbert Scarf. â€œSome Examples of Global Instability of the Competitive Equilibriumâ€. In:
International Economic Review 1.3 (1960), pp. 157â€“172.
66 Modelling Market Dynamics within the Maritime Sector

[39] P . T ormos and A. Lova. â€œA Competitive Heuristic Solution T echnique for ResourceÂ­Constrained
Project Schedulingâ€. In: Annals of Operations Research (2001). doi: 10.1023/A:10109978
14183. url: https://link.springer.com/article/10.1023/A:1010997814183.
[40] Cezar. PaparÄƒ. â€œLogistics Optimization for Resource Allocation and Scheduling Using
Time Slotsâ€. In: International Journal of Advanced Statistics and ITC for Economics and
Life Sciences (2024). url: https://www.academia.edu/126702443/Logistics_Optimization_f
or_Resource_Allocation_and_Scheduling_Using_Time_Slots.
[41] Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. â€œGlobal conÂ­
vergence of the HeavyÂ­ball method for convex optimizationâ€. In: 2015 European Control
Conference (ECC). 2015, pp. 310â€“315. doi: 10.1109/ECC.2015.7330562.
[42] S.M. Shakhno. â€œConvergence of the twoÂ­step combined method and uniqueness of the
solution of nonlinear operator equationsâ€. In: Journal of Computational and Applied MathÂ­
ematics 261 (2014), pp. 378â€“386. issn: 0377Â­0427. doi: https://doi.org/10.1016/j.cam.20
13.11.018. url: https://www.sciencedirect.com/science/article/pii/S0377042713006481.
[43] Paul T seng. â€œOn linear convergence of iterative methods for the variational inequality probÂ­
lemâ€. In: Journal of Computational and Applied Mathematics 60 (1995), pp. 237â€“252. url:
https://www.sciencedirect.com/science/article/pii/037704279400094H.
[44] Y . Zhang and R. A. T apia. â€œSuperlinear and Quadratic Convergence of PrimalÂ­Dual InteriorÂ­
Point Methods for Linear Programming Revisitedâ€. In: Journal of Optimization Theory and
Applications (1992). doi: 10.1007/BF00940179. url: https://link.springer.com/article/10.10
07/BF00940179.
[45] Komal T omar Chitra Solanki Pragati Thapliyal. â€œRole of Bisection Methodâ€. In:International
Journal of Computer Applications Technology and Research 3 (2014), pp. 533â€“535. url:
https://www.ijcat.com/archives/volume3/issue8/ijcatr03081009.pdf.
[46] Anders Fjendbo Jensen, Jeppe Rich, and Carsten Lynge Hansen. System convergence in
transport modelling: an explorative study on behavioural responses and model dynamics .
2023. url: https://backend.orbit.dtu.dk/ws/portalfiles/portal/119086056/system_convergen
ce_in_transport_modelling.pdf.
Modelling Market Dynamics within the Maritime Sector 67

68 Modelling Market Dynamics within the Maritime Sector

A Mockup of NavigaTE functionality for
price convergence
1 import numpy as np
2 import pandas as pd
3 import gurobipy as gp
4 from gurobipy import GRB
5
6 # Create a Gurobi model
7 model = gp. Model ( "" price_convergence "")
8
9 # Define variables
10 b1 = model . addVar ( name = ""b1 "")
11 b2 = model . addVar ( name = ""b2 "")
12 bunc = model . addVar ( name = ""bunc "")
13
14 # Define parameters
15 p1 = 50 # Coefficient for b1
16 p2 = 100 # Coefficient for b2
17 punc = 200 # Coefficient for bunc
18 C1 = 100 # Capacity for b1
19 C2 = 100 # Capacity for b2
20 D = 500 # Example demand value
21
22 # Set objective function
23 model . setObjective (b1 * p1 + b2 * p2 + bunc * punc , GRB. MINIMIZE )
24
25 # Add constraints
26 c1 = model . addConstr (b1 <= C1 , ""c1 "")
27 c2 = model . addConstr (b2 <= C2 , ""c2 "")
28 c3 = model . addConstr (b1 + b2 + bunc == D, ""c3 "")
29
30 # Optimize the model
31 model . optimize ()
32
33 # Get shadow prices (dual values ) for the constraints
34 if model . status == GRB. OPTIMAL :
35 print (f "" Shadow price for c1 ( supply constraint b1 ): {c1 .Pi }"")
36 print (f "" Shadow price for c2 ( supply constraint b2 ): {c2 .Pi }"")
37
38 # Print results
39 if model . status == GRB. OPTIMAL :
40 print (f "" Optimal solution found :"")
41 print (f ""b1 : {b1 .X}"")
42 print (f ""b2 : {b2 .X}"")
43 print (f ""bunc : {bunc .X}"")
44 print (f "" Objective value : {model . ObjVal }"")
45 else :
46 print (""No optimal solution found ."")
47
48 # Retrieve the shadow prices value
49 shadow_prices = {
50 ""c1 "": c1.Pi ,
51 ""c2 "": c2.Pi ,
52 }
53
54 # Print variable values
55 for f in model . getVars ():
56 print (f "" Variable {f. VarName }: Value = {f.X}"")
Modelling Market Dynamics within the Maritime Sector 69

57
58 # ----- Change of constraint capacity ---------------
59 C1_new = 100
60 C2_new = 450
61
62 # Update the model with new capacities
63 model . remove (c1)
64 model . remove (c2)
65 c1_new = model . addConstr (b1 <= C1_new , ""c1 "")
66 c2_new = model . addConstr (b2 <= C2_new , ""c2 "")
67 model . update ()
68
69 # Re - optimize the model
70 model . optimize ()
71
72 # Get shadow prices (dual values ) for the updated constraints
73 if model . status == GRB. OPTIMAL :
74 print (f "" Shadow price for c1 ( supply constraint b1 ): { c1_new .Pi }"")
75 print (f "" Shadow price for c2 ( supply constraint b2 ): { c2_new .Pi }"")
76
77 # Print results
78 if model . status == GRB. OPTIMAL :
79 print (f "" Optimal solution found after updating constraints :"")
80 print (f ""b1 : {b1 .X}"")
81 print (f ""b2 : {b2 .X}"")
82 print (f ""bunc : {bunc .X}"")
83 print (f "" Objective value : {model . ObjVal }"")
Listing A.1: T oy example demonstrating the core concept of shadow prices in linear
programming. The model minimizes total cost by allocating demand across three supply options
with different costs and capacity constraints. Two scenarios are shown: one with tighter supply
constraints and one with relaxed capacity on the second supplier. The resulting shadow prices
(dual values) illustrate how the marginal value of relaxing a constraint changes under different
conditions.
70 Modelling Market Dynamics within the Maritime Sector

B Initial MVP for concept proving of duality
for NavigaTE
1 import gurobipy as gp
2 from gurobipy import GRB
3 import pandas as pd
4 import numpy as np
5 import matplotlib . pyplot as plt
6
7
8 # Model parameters
9 np. random . seed (42)
10 years = list (np. arange (2025 , 2035) ) # Convert to list
11 fuels = [ "" Biodiesel "", "" Ethanol "", "" Hydrogen "", "" Diesel ""]
12
13 # Initial supply per fuel ( millions of liters )
14 supply = { fuel : [ float (np. random . randint (100 , 200) ) for _ in years ] for fuel in
fuels }
15
16 # Production costs per liter â‚¬(\)
17 initial_production_costs = { "" Biodiesel "": 1.2 , "" Ethanol "": 0.8 , "" Hydrogen "": 2.5 , ""
Diesel "": 1.0}
18
19 def smooth_cost_curve ( initial_cost , peak_year , end_year , peak_factor =1.5) :
20 years = np. arange (2025 , 2035)
21 peak_cost = initial_cost * peak_factor
22 costs = []
23 for year in years :
24 if year <= peak_year :
25 cost = initial_cost + ( peak_cost - initial_cost ) * ( year - 2025) / (
peak_year - 2025)
26 else :
27 cost = peak_cost - ( peak_cost - initial_cost ) * ( year - peak_year ) / (
end_year - peak_year )
28 costs . append ( cost )
29 return costs
30
31 # Generates the production costs per year
32 production_costs = { fuel : smooth_cost_curve ( initial_production_costs [ fuel ], 2028 ,
2034) for fuel in fuels }
33
34 # Emisions per liter (kg CO2 )
35 emissions = { "" Biodiesel "": 2.5 , "" Ethanol "": 1.8 , "" Hydrogen "": 0.0 , "" Diesel "": 3.2}
36
37 # Total market 's demand ( million liters per year , constant )
38 demand = 400
39
40 # Emissions threshold and penalty
41 emission_threshold = 800
42 penalty = 450
43
44 # Parameter to adjust the prices
45 alpha = 0.05 # Updating prices factor
46
47 # To store the results
48 all_results_with_shadow = []
49 all_results_without_shadow = []
50
51 # Initializes the prices with prduction costs
Modelling Market Dynamics within the Maritime Sector 71

52 prices = { fuel : production_costs [ fuel ] for fuel in fuels }
53 prices_without_shadow = prices . copy ()
54
55 # Simulation per year
56 for t in years :
57 model = gp. Model (f "" Market_Equilibrium_ {t}"")
58
59 # Decision variables : quantity of sold fuel (b_i )
60 b = model . addVars (fuels , lb =0, vtype =GRB. CONTINUOUS , name = ""b"")
61
62 # Emissions excess variable
63 e = model . addVar (lb =0, vtype =GRB. CONTINUOUS , name = ""e"")
64
65 # Constraint of the balance supply - demand
66 demand_balance = model . addConstr (gp. quicksum (b[f] for f in fuels ) == demand ,
name =f "" Demand_Balance "")
67
68 # Supply Constraint
69 for f in fuels :
70 if t > years [0]: # Ensure t -1 is a valid index
71 model . addConstr (b[f] <= supply [f][ years . index (t) - 1], name =f ""
Supply_Limit_ {f}"")
72
73 # Emissions constraint
74 model . addConstr (gp. quicksum ( emissions [f] * b[f] for f in fuels ) -
emission_threshold <= e, name =f "" Emission_Limit "")
75
76 # Calculate initial prices based in the production costs of the equivalent year
77 prices = { fuel : production_costs [ fuel ][ years . index (t)] * 1.15 for fuel in fuels
}
78 prices_without_shadow = { fuel : production_costs [ fuel ][ years . index (t)] * 1.15
for fuel in fuels }
79
80 # Objective function : minimize costs + emissions penalty
81 model . setObjective (
82 gp. quicksum ( prices [f] * b[f] for f in fuels ) + penalty * e,
83 GRB. MINIMIZE
84 )
85
86 # Model solver
87 model . optimize ()
88
89 # Verifies the optimal solution
90 if model . Status == GRB. OPTIMAL :
91 # Updates prices using shadow prices of the demand balance constraint
92 shadow_price = demand_balance .Pi
93 for f in fuels :
94 prices [f] = prices [f] + shadow_price
95
96 # extracts optimal values and stores them
97 results_with_shadow = { ""Year "": t}
98 results_without_shadow = { ""Year "": t}
99 for f in fuels :
100 results_with_shadow [f] = b[f].X
101 results_with_shadow [f + "" _price ""] = prices [f]
102 results_with_shadow [f + "" _shadow_price ""] = shadow_price
103 results_with_shadow [f + "" _supply ""] = supply [f][ years . index (t)]
104
105 results_without_shadow [f] = b[f].X
106 results_without_shadow [f + "" _price ""] = prices_without_shadow [f]
107 results_without_shadow [f + "" _supply ""] = supply [f][ years . index (t)]
108
109 results_with_shadow [ "" penalty ""] = e.X
72 Modelling Market Dynamics within the Maritime Sector

110 results_without_shadow [ "" penalty ""] = e.X
111
112 all_results_with_shadow . append ( results_with_shadow )
113 all_results_without_shadow . append ( results_without_shadow )
114
115 # Price evolution based in market equilibrium
116 if t > years [0]:
117 total_supply_previous = sum ( supply [f][ years . index (t) - 1] for f in
fuels )
118 for f in fuels :
119 prices [f] = prices [f] + alpha * (( demand / total_supply_previous ) -
1) * prices [f]
120 prices_without_shadow [f] = prices_without_shadow [f] + alpha * ((
demand / total_supply_previous ) - 1) * prices_without_shadow [f]
121
122 # Creates DataFrames with all results
123 df_all_results_with_shadow = pd. DataFrame ( all_results_with_shadow )
124 df_all_results_without_shadow = pd. DataFrame ( all_results_without_shadow )
125
126 # Stores Dataframes in Excel
127 df_all_results_with_shadow . to_excel ( "" price_dynamics_results_with_shadow .xlsx "",
index = False )
128 df_all_results_without_shadow . to_excel ( "" price_dynamics_results_without_shadow .xlsx ""
, index = False )
129
130 # Shows the complete DataFrames
131 print ("" Results with Shadow Prices :"")
132 print ( df_all_results_with_shadow )
133 print (""\ nResults without Shadow Prices :"")
134 print ( df_all_results_without_shadow )
135
136
137 # Creates a line cgart with the costs and prices of all fuels s
138 fig , ax1 = plt. subplots ( figsize =(14 , 8))
139
140 # Costs and price graphs
141 for fuel in fuels :
142 ax1. plot (years , [ production_costs [ fuel ][ years . index (t)] for t in years ], label =
f'{fuel } Cost ', linestyle = '-- ')
143 ax1. plot ( df_all_results_with_shadow [ 'Year '], df_all_results_with_shadow [ fuel +
'_price '], label =f '{fuel } Price with Shadow ')
144 ax1. plot ( df_all_results_without_shadow [ 'Year '], df_all_results_without_shadow [
fuel + '_price '], label =f '{fuel } Price without Shadow ')
145
146 ax1. set_xlabel ( 'Year ')
147 ax1. set_ylabel ( 'Price â‚¬() ')
148 ax1. set_title ( 'Cost and Price Dynamics for All Fuels ')
149 ax1. legend ()
150 ax1. grid ( True )
151
152 # Crates a second line chart for the quantity of sold fuel
153 ax2 = ax1. twinx ()
154 for fuel in fuels :
155 ax2. plot ( df_all_results_with_shadow [ 'Year '], df_all_results_with_shadow [ fuel ],
label =f '{fuel } Bunkered Quantity ', linestyle = ':')
156
157 ax2. set_ylabel ( 'Bunkered Quantity ( millions of liters )')
158 ax2. legend (loc= 'upper left ')
159
160 plt. savefig ( 'all_fuels_price_and_quantity_dynamics .png ')
161 plt. show ()
Modelling Market Dynamics within the Maritime Sector 73

Listing B.1: Illustrative Python implementation of a simplified multiÂ­year, multiÂ­fuel market
equilibrium model used to demonstrate the effect of shadow prices on price formation and
fuel allocation. The code simulates demand, supply constraints, and emissions limits for four
fuels over a decade, solving a cost minimization problem using Gurobi. Shadow prices from
the demand constraint are used to dynamically adjust fuel prices, reflecting marginal scarcity
and driving convergence. Results with and without shadow price adjustments are compared
to highlight their role in guiding economically consistent pricing under constraintÂ­driven market
behavior.
74 Modelling Market Dynamics within the Maritime Sector

C NavigaTE Bunker Pricing Algorithm
C.1 Main Solver Function
1 def solve ( self ):
2
3 if self . _options . include_market_dynamic ():
4
5 if self . _options . get_market_type () == MarketTypeID . EMERGING :
6
7 # if the market is emerging , each vessel makes individual
8 # offtake agreements with individual prices . Hence , the
9 # solution is just the fair -share solution but with the
10 # shadow - prices added on top as the price premium . These
11 # are added later during the transfer of results
12 self . _optimize_fair_share ()
13
14 else :
15
16 # a general price premium must be established across
17 # coupled markets requiring an iterative approach
18 self . _optimize_market_dynamic ()
19
20 else :
21
22 # if there is no market dynamics included the
23 # model is solved using production costs
24 self . _optimize_fair_share ()
Listing C.1: Main solver routine managing price formation across market types in NavigaTE.
When market dynamics are disabled, fuel allocation is based solely on production costs via a
fairÂ­share solution. For Emerging markets, prices are adjusted individually using vesselÂ­specific
shadow prices postÂ­optimization. In contrast, Global and Local markets trigger a coupled
optimization loop, solving for marketÂ­clearing price premiums across interconnected agents,
reflecting coordinated fuel price dynamics.
C.2 Market Dynamics Optimization
1 def _optimize_market_dynamic ( self ):
2
3 # extract tolerances and maximum iterations
4 market_type = self . _options . get_market_type ()
5 max_iter = self . _options . get_market_maximum_iterations ()
6 tol_quantity = self . _options . get_market_tolerance_quantity ()
7 tol_price = self . _options . get_market_tolerance_price ()
8
9 # define the aggregation key based on market type
10 if market_type == MarketTypeID . GLOBAL :
11 self . _get_market_key = lambda x, y: y
12 else :
13 self . _get_market_key = lambda x, y: (x, y)
14
15 # calculate the cumulative supply of fuels in order to compare against
16 self . _bunker_supply = self . _calculate_cumulative_fuel_supply ()
17
18 # initial floor of the shadow price is zero
19 floors = {k: 0. for k in self . _bunker_supply }
20
Modelling Market Dynamics within the Maritime Sector 75

21 # initial ceilings where demand < supply
22 ceilings = self . _calculate_initial_ceilings ()
23
24 converged = False
25 iteration = 0
26 while not converged and iteration < max_iter :
27
28 middles = {k: ( floors [k] + ceilings [k]) / 2. for k in floors }
29
30 self . _update_vessel_market_objective ( middles )
31 self . _optimize_fair_share ()
32
33 demands = self . _calculate_cumulative_fuel_demand ()
34 shadow_prices_min = self . _calculate_minimum_shadow_price ()
35
36 for k in floors :
37
38 demand = demands .get(k, 0.)
39 supply = self . _bunker_supply [k]
40 shadow_price_min = shadow_prices_min .get(k, None )
41
42 if demand < supply - tol_quantity :
43 ceilings [k] = middles [k]
44 floors [k] = 0.
45 if floors [k] > ceilings [k]:
46 floors [k] = ceilings [k]
47 continue
48
49 if shadow_price_min is None :
50 ceilings [k] = middles [k]
51 continue
52
53 if shadow_price_min > tol_price :
54 floors [k] = middles [k]
55 else :
56 ceilings [k] = middles [k]
57
58 if floors [k] > ceilings [k]:
59 floors [k] = ceilings [k]
60
61 converged = self . _check_market_convergence (floors , ceilings )
62 iteration += 1
63
64 self . _update_vessel_market_objective ( floors )
65 self . _optimize_fair_share ()
66
67 self . _bunker_fuel_premium = floors
68 self . _market_dynamic_iterations += iteration
Listing C.2: Core market equilibrium algorithm in NavigaTE for simulating price convergence
under Global and Local market dynamics. This iterative bisection method adjusts fuel price
premiums between supply floors and demand ceilings, based on vessel responses and shadow
price signals. Convergence is determined by fuelÂ­level price and quantity tolerances, ensuring
consistent bunker fuel allocation and marketÂ­clearing prices across coupled actors.
C.3 Initial Price Ceiling Calculation
1 def _calculate_initial_ceilings ( self ):
2 increase_factor = self . _options . get_market_price_ceiling_growth ()
3 min_ceiling = self . _options . get_market_price_ceiling_minimum ()
4 tol_quantity = self . _options . get_market_tolerance_quantity ()
76 Modelling Market Dynamics within the Maritime Sector

5
6 self . _optimize_fair_share ()
7
8 demands = self . _calculate_cumulative_fuel_demand ()
9 shadow_prices_max = self . _calculate_maximum_shadow_price ()
10
11 iteration = 0
12 ceilings = {k: 0.0 for k in self . _bunker_supply }
13
14 while True :
15 still_binding = set ()
16 shadow_prices_min = self . _calculate_minimum_shadow_price ()
17
18 for k in self . _bunker_supply :
19 demand = demands .get(k, 0.)
20 supply = self . _bunker_supply [k]
21 shadow_price_min = shadow_prices_min .get(k, None )
22
23 if demand < supply - tol_quantity or shadow_price_min is None :
24 continue
25
26 if ceilings [k] == 0.0:
27 shadow_price_max = shadow_prices_max .get(k, 0.0)
28 ceilings [k] = max( min_ceiling , increase_factor * max(
shadow_price_max , shadow_price_min ))
29 else :
30 ceilings [k] *= increase_factor
31
32 still_binding .add(k)
33
34 if not still_binding :
35 break
36
37 margin_update = {k: ceilings [k] for k in still_binding }
38 self . _update_vessel_market_objective ( margin_update )
39 self . _optimize_fair_share ()
40 demands = self . _calculate_cumulative_fuel_demand ()
41 iteration += 1
42
43 self . _market_dynamic_ceiling_iterations = iteration
44 return ceilings
Listing C.3: First outer loop of the market dynamics routine in NavigaTE, responsible for
computing initial price ceilings. By iteratively adjusting fuelÂ­specific ceilings based on minimum
and maximum observed shadow prices, this routine ensures that subsequent price convergence
starts from a feasible upper bound where constraints are no longer binding.
C.4 Market Convergence Check
1 def _check_market_convergence (self , floors , ceilings ):
2 return max(abs( ceilings [k] - floors [k]) for k in floors ) <= self . _options .
get_market_tolerance_price ()
Listing C.4: Market convergence check in NavigaTE, ensuring that the difference between
fuelÂ­specific price floors and ceilings falls below a predefined tolerance threshold, thereby
terminating the iterative marketÂ­clearing loop once equilibrium is reached
C.5 Update Vessel Market Objective
1 def _update_vessel_market_objective (self , shadow_price ):
Modelling Market Dynamics within the Maritime Sector 77

2 for (v, p, f), bunker in self . _bunker . items ():
3 port_name = self . _vessels [v]. get_route (). get_ports ()[p]. get_name ()
4 key = (v, port_name , f)
5 local_key = ( port_name , f)
6 market_key = self . _get_market_key ( port_name , f)
7
8 if market_key not in shadow_price :
9 continue
10
11 price = self . _bunker_fuel_cost [ local_key ] + shadow_price [ market_key ] + self
. _bunker_levy_cost [key]
12 bunker .Obj = price * self . _multipliers [v]
Listing C.5: Update routine for vesselÂ­specific market objectives in NavigaTE, applying marketÂ­
clearing shadow prices and regulatory levies to adjust fuel costs used in the optimization, based
on port and fuelÂ­level market keys.
C.6 Cumulative Fuel Supply Calculation
1 def _calculate_cumulative_fuel_supply ( self ):
2 tol_quantity = self . _options . get_market_tolerance_quantity ()
3 supplies = {}
4
5 for port_name , port in self . _ports . items ():
6 for f, fuel in self . _fuels . items ():
7 if not port . bunkering_allowed (f):
8 continue
9
10 market_key = self . _get_market_key ( port_name , f)
11
12 if self . _scope == BunkerScopeID . EXISTING :
13 supply = port . get_expectation (). get_existing_bunker_supply (f, self .
_idx )
14 else :
15 supply = port . get_expectation (). get_expected_bunker_supply (f, self .
_idx )
16
17 if np. isfinite ( supply ) and supply > tol_quantity :
18 supplies . setdefault ( market_key , 0.)
19 supplies [ market_key ] += supply
20
21 return supplies
Listing C.6: Computation of cumulative fuel supply across ports in NavigaTE, aggregating
supply volumes per market region and fuel type based on portÂ­level availability and model scope
settings.
C.7 Cumulative Fuel Demand Calculation
1 def _calculate_cumulative_fuel_demand ( self ):
2 tol_sol = self . _options . get_solution_tolerance ()
3 demands = {}
4
5 for (v, p, f), bunker in self . _bunker . items ():
6 port_name = self . _vessels [v]. get_route (). get_ports ()[p]. get_name ()
7 key = (v, port_name , f)
8
9 if key not in self . _fair_share_fuel :
10 continue
11
78 Modelling Market Dynamics within the Maritime Sector

12 market_key = self . _get_market_key ( port_name , f)
13
14 if market_key not in self . _bunker_supply :
15 continue
16
17 if self . _bunker_supply [ market_key ] < tol_sol :
18 continue
19
20 demand = bunker .X * self . _multipliers [v]
21
22 if market_key not in demands :
23 demands [ market_key ] = demand
24 else :
25 demands [ market_key ] += demand
26
27 return demands
Listing C.7: Computation of cumulative fuel demand in NavigaTE, aggregating vesselÂ­level fuel
uptake across ports and routes into marketÂ­specific demand volumes, aligned with supply and
convergence routines.
C.8 Maximum Shadow Price Calculation
1 def _calculate_maximum_shadow_price ( self ):
2 return self . _calculate_extremum_shadow_price (max , allow_zero = False )
Listing C.8: Compute the maximum shadow price across all market segments by leveraging a
general extremum calculation routine, excluding zero values to identify binding constraints with
the highest marginal cost.
C.9 Minimum Shadow Price Calculation
1 def _calculate_minimum_shadow_price ( self ):
2 return self . _calculate_extremum_shadow_price (min , allow_zero = False )
Listing C.9: Calculate the minimum shadow price across all market segments using a general
extremum method, excluding zero values to focus on the lowest positive marginal cost indicating
binding constraints.
C.10 Extremum Shadow Price Calculation
1 def _calculate_extremum_shadow_price (self , method , allow_zero = False ):
2 if not (( method is min) or ( method is max)):
3 raise ValueError ("" Extremum method must be either 'min ' or 'max '."")
4
5 tol_sol = self . _options . get_solution_tolerance ()
6 shadow_prices = self . _extract_shadow_prices ()
7 extremum_shadow_prices = {}
8
9 for (v, port_name , f), shadow_price in shadow_prices . items ():
10 if (not allow_zero ) and ( shadow_price < tol_sol ):
11 continue
12
13 key = self . _get_market_key ( port_name , f)
14
15 if key not in extremum_shadow_prices :
16 extremum_shadow_prices [key] = shadow_price
17 else :
Modelling Market Dynamics within the Maritime Sector 79

18 extremum_shadow_prices [key] = method ( extremum_shadow_prices [key],
shadow_price )
19
20 return extremum_shadow_prices
Listing C.10: General method to calculate the extremum (minimum or maximum) shadow price
across market segments, optionally excluding values below a solution tolerance to filter out
negligible shadow prices. The function aggregates shadow prices by market key and applies
the specified extremum method.
C.11 Shadow Price Extraction
1 def _extract_shadow_prices ( self ):
2 tol_sol = self . _options . get_solution_tolerance ()
3 shadow_prices = {}
4
5 for (v, port_name , f), constraint in self . _fair_share_fuel . items ():
6 key = (v, port_name , f)
7
8 if not self . _ports [ port_name ]. bunkering_allowed (f):
9 continue
10
11 market_key = self . _get_market_key ( port_name , f)
12 if market_key not in self . _bunker_supply :
13 continue
14
15 if self . _bunker_supply [ market_key ] < tol_sol :
16 continue
17
18 shadow_prices [key] = -constraint .Pi / self . _multipliers [v]
19
20 return shadow_prices
Listing C.11: Extract shadow prices from the fairÂ­share fuel constraints for each vessel, port,
and fuel combination. Shadow prices are normalized by vessel multipliers and filtered based
on bunkering permission and supply thresholds to ensure only relevant market segments are
considered.
80 Modelling Market Dynamics within the Maritime Sector

D Test Environment Files
The following configuration files are used to construct and execute the test environment:
â€¢ test_alternative_powers
â€¢ test_ban_vessels
â€¢ test_bunker_logistics
â€¢ test_bunker_options
â€¢ test_bunker_regions
â€¢ test_canals
â€¢ test_curves
â€¢ test_efficiencies
â€¢ test_feedstocks
â€¢ test_fleet_inertia
â€¢ test_fleets
â€¢ test_forecasts
â€¢ test_fuel_availability
â€¢ test_fuel_constraints
â€¢ test_fuel_conversion
â€¢ test_fuel_inertia
â€¢ test_fuels
â€¢ test_levies
â€¢ test_levies_activate
â€¢ test_model_definition
â€¢ test_plant_readiness
â€¢ test_plants
â€¢ test_ports
â€¢ test_producers
â€¢ test_regions
â€¢ test_regulations
â€¢ test_regulations_activate
â€¢ test_reports
â€¢ test_sources
â€¢ test_speed_management
â€¢ test_technology_availability
â€¢ test_technology_uptake_initial
â€¢ test_technology_uptake_update
â€¢ test_time_steps_yearly
â€¢ test_timetables
â€¢ test_transports
â€¢ test_uptakes_fleet
â€¢ test_vessels
Modelling Market Dynamics within the Maritime Sector 81

E Modelling Framework
Scenario Dynamic Pricing Policy Active Fuel Mix Response
CostÂ­Only + No Regulation No No Static
CostÂ­Only + Levy No Yes Moderate
CostÂ­Only + Flexible Regulation No Yes High
CostÂ­Only + NonÂ­Flexible Reg. No Yes High
Global + No Regulation Yes No PriceÂ­Sensitive
Global + Levy Yes Yes PriceÂ­Sensitive
Global + Flexible Regulation Yes Yes PriceÂ­ and PolicyÂ­Sensitive
Global + NonÂ­Flexible Reg. Yes Yes PolicyÂ­Constrained
Local + No Regulation Yes No PriceÂ­Sensitive
Local + Levy Yes Yes LevyÂ­Driven
Local + Flexible Regulation Yes Yes Combined Shift
Local + NonÂ­Flexible Reg. Yes Yes Constrained
Emerging + No Regulation No No CostÂ­Minimal
Emerging + Levy No Yes LevyÂ­Driven
Emerging + Flexible Regulation No Yes Moderate Adjustment
Emerging + NonÂ­Flexible Reg. No Yes Constrained
T able E.1: Summary of active mechanisms across all simulated scenarios, highlighting the
presence of dynamic pricing, policy application, and resulting fuel mix behaviour.
82 Modelling Market Dynamics within the Maritime Sector

F Test Environment Plots
F .1 Global Â­ Middle East Plots
Figure F .1: Bunker fuel supplyÂ­demand interaction and price convergence in Middle East ports
under Global Market assumptions in the test environment. The left figure presents the price
evolution in [USD/GJ] of each fuel type while right figure shows bunker demand versus available
supply in [kt/yr] for key alternative fuels.
F .2 Local Â­ Middle East Plots
Figure F .2: Bunker fuel supplyÂ­demand interaction and price convergence in Middle East ports
under Local Market assumptions in the test environment. The left figure presents the price
evolution in [USD/GJ] of each fuel type while right figure shows bunker demand versus available
supply in [kt/yr] for key alternative fuels..
F .3 Emerging Â­ Middle East Plots
Figure F .3: Bunker fuel supplyÂ­demand interaction and price convergence in Middle East ports
under Emerging Market assumptions in the test environment. The left figure presents the price
evolution in [USD/GJ] of each fuel type while right figure shows bunker demand versus available
supply in [kt/yr] for key alternative fuels.
Modelling Market Dynamics within the Maritime Sector 83

84 Modelling Market Dynamics within the Maritime Sector

G No Regulation Environment Â­ Results
G.1 Performance Analytics
(a) Linear Solves
 (b) FairÂ­Share Iterations
(c) Market Dynamic Iterations
 (d) Ceiling Iterations
Figure G.1: Cumulative expected, existing and total solver calls and iteration counts under the
No Regulation scenario across all model runs.
G.2 Market Dynamics
G.2.1 Fuel Supply and Bunker Prices â€“ CostÂ­Only
Africa
Figure G.2: Bunker fuel cost (left), and fuel supply and demand (right) in African ports.
Modelling Market Dynamics within the Maritime Sector 85

Americas
Figure G.3: Bunker fuel cost (left), and fuel supply and demand (right) in American ports.
Asia
Figure G.4: Bunker fuel cost (left), and fuel supply and demand (right) in Asian ports.
Europe
Figure G.5: Bunker fuel cost (left), and fuel supply and demand (right) in European ports.
Middle East
Figure G.6: Bunker fuel cost (left), and fuel supply and demand (right) in Middle Eastern ports.
86 Modelling Market Dynamics within the Maritime Sector

Global Overview
Figure G.7: Global fuel supply and demand overview for the CostÂ­Only scenario.
G.2.2 Fuel Supply and Bunker Prices â€“ Emerging
Africa
Figure G.8: Market price (left), and fuel supply and demand (right) in African ports under the
Emerging configuration.
Americas
Figure G.9: Market price (left), and fuel supply and demand (right) in American ports under the
Emerging configuration.
Modelling Market Dynamics within the Maritime Sector 87

Asia
Figure G.10: Market price (left), and fuel supply and demand (right) in Asian ports under the
Emerging configuration.
Europe
Figure G.11: Market price (left), and fuel supply and demand (right) in European ports under
the Emerging configuration.
Middle East
Figure G.12: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Emerging configuration.
Global Overview
Figure G.13: Global fuel supply and demand overview for the Emerging configuration.
88 Modelling Market Dynamics within the Maritime Sector

G.2.3 Fuel Supply and Bunker Prices â€“ Global
Africa
Figure G.14: Market price (left), and fuel supply and demand (right) in African ports under the
Global configuration.
Americas
Figure G.15: Market price (left), and fuel supply and demand (right) in American ports under
the Global configuration.
Asia
Figure G.16: Market price (left), and fuel supply and demand (right) in Asian ports under the
Global configuration.
Modelling Market Dynamics within the Maritime Sector 89

Europe
Figure G.17: Market price (left), and fuel supply and demand (right) in European ports under
the Global configuration.
Middle East
Figure G.18: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Global configuration.
Global Overview
Figure G.19: Global fuel supply and demand overview for the Global configuration.
90 Modelling Market Dynamics within the Maritime Sector

G.2.4 Fuel Supply and Bunker Prices â€“ Local
Africa
Figure G.20: Market price (left), and fuel supply and demand (right) in African ports under the
Local configuration.
Americas
Figure G.21: Market price (left), and fuel supply and demand (right) in American ports under
the Local configuration.
Asia
Figure G.22: Market price (left), and fuel supply and demand (right) in Asian ports under the
Local configuration.
Modelling Market Dynamics within the Maritime Sector 91

Europe
Figure G.23: Market price (left), and fuel supply and demand (right) in European ports under
the Local configuration.
Middle East
Figure G.24: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Local configuration.
Global Overview
Figure G.25: Global fuel supply and demand overview for the Local configuration.
92 Modelling Market Dynamics within the Maritime Sector

H Regulation with Flexibility Environment
IMO Regulation 380 USD Â­ Results
H.1 Performance Analytics
(a) Linear Solves
 (b) FairÂ­Share Iterations
(c) Market Dynamic Iterations
 (d) Ceiling Iterations
Figure H.1: Cumulative expected, existing and total solver calls and iteration counts under the
Regulation with Flexibility (IMO GFS of 380USD) scenario across all model runs.
H.2 Market Dynamics
H.2.1 Fuel Supply and Bunker Prices â€“ CostÂ­Only, Regulation with Flexibility
Africa
Figure H.2: Bunker fuel cost (left), and fuel supply and demand (right) in African ports.
Modelling Market Dynamics within the Maritime Sector 93

Americas
Figure H.3: Bunker fuel cost (left), and fuel supply and demand (right) in American ports.
Asia
Figure H.4: Bunker fuel cost (left), and fuel supply and demand (right) in Asian ports.
Europe
Figure H.5: Bunker price and fuel supply in European ports.
Middle East
Figure H.6: Bunker fuel cost (left), and fuel supply and demand (right) in Middle Eastern ports.
94 Modelling Market Dynamics within the Maritime Sector

Global Overview
Figure H.7: Global fuel supply and demand overview for the CostÂ­Only scenario.
H.2.2 Fuel Supply and Bunker Prices â€“ Emerging
Africa
Figure H.8: Market price (left), and fuel supply and demand (right) in African ports under the
Emerging configuration.
Americas
Figure H.9: Market price (left), and fuel supply and demand (right) in American ports under the
Emerging configuration.
Modelling Market Dynamics within the Maritime Sector 95

Asia
Figure H.10: Market price (left), and fuel supply and demand (right) in Asian ports under the
Emerging configuration.
Europe
Figure H.11: Market price (left), and fuel supply and demand (right) in European ports under
the Emerging configuration.
Middle East
Figure H.12: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Emerging configuration.
Global Overview
Figure H.13: Global fuel supply and demand overview for the Emerging configuration.
96 Modelling Market Dynamics within the Maritime Sector

H.2.3 Fuel Supply and Bunker Prices â€“ Global
Africa
Figure H.14: Market price (left), and fuel supply and demand (right) in African ports under the
Global configuration.
Americas
Figure H.15: Market price (left), and fuel supply and demand (right) in American ports under
the Global configuration.
Asia
Figure H.16: Market price (left), and fuel supply and demand (right) in Asian ports under the
Global configuration.
Modelling Market Dynamics within the Maritime Sector 97

Europe
Figure H.17: Market price (left), and fuel supply and demand (right) in European ports under
the Global configuration.
Middle East
Figure H.18: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Global configuration.
Global Overview
Figure H.19: Global fuel supply and demand overview for the Global configuration.
98 Modelling Market Dynamics within the Maritime Sector

H.2.4 Fuel Supply and Bunker Prices â€“ Local
Africa
Figure H.20: Market price (left), and fuel supply and demand (right) in African ports under the
Local configuration.
Americas
Figure H.21: Market price (left), and fuel supply and demand (right) in American ports under
the Local configuration.
Asia
Figure H.22: Market price (left), and fuel supply and demand (right) in Asian ports under the
Local configuration.
Modelling Market Dynamics within the Maritime Sector 99

Europe
Figure H.23: Market price (left), and fuel supply and demand (right) in European ports under
the Local configuration.
Middle East
Figure H.24: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Local configuration.
Global Overview
Figure H.25: Global fuel supply and demand overview for the Local configuration.
100 Modelling Market Dynamics within the Maritime Sector

I Regulation with Flexibility Environment
1200 USD penalty Â­ Results
I.1 Performance Analytic
(a) Linear Solves
 (b) FairÂ­Share Iterations
(c) Market Dynamic Iterations
 (d) Ceiling Iterations
Figure I.1: Cumulative expected, existing and total solver calls and iteration counts under the
Regulation with Flexibility (1200USD) scenario across all model runs.
I.2 Market Dynamics
I.2.1 Fuel Supply and Bunker Prices â€“ CostÂ­Only, Regulation with Flexibility
1200
Africa
Figure I.2: Bunker fuel cost (left), and fuel supply and demand (right) in African ports.
Modelling Market Dynamics within the Maritime Sector 101

Americas
Figure I.3: Bunker fuel cost (left), and fuel supply and demand (right) in American ports.
Asia
Figure I.4: Bunker fuel cost (left), and fuel supply and demand (right) in Asian ports.
Europe
Figure I.5: Bunker fuel cost (left), and fuel supply and demand (right) in European ports.
Middle East
Figure I.6: Bunker fuel cost (left), and fuel supply and demand (right) in Middle Eastern ports.
102 Modelling Market Dynamics within the Maritime Sector

Global Overview
Figure I.7: Global fuel supply and demand overview for the CostÂ­Only scenario.
I.2.2 Fuel Supply and Bunker Prices â€“ Emerging
Africa
Figure I.8: Market price (left), and fuel supply and demand (right) in African ports under the
Emerging configuration.
Americas
Figure I.9: Market price (left), and fuel supply and demand (right) in American ports under the
Emerging configuration.
Modelling Market Dynamics within the Maritime Sector 103

Asia
Figure I.10: Market price (left), and fuel supply and demand (right) in Asian ports under the
Emerging configuration.
Europe
Figure I.11: Market price (left), and fuel supply and demand (right) in European ports under the
Emerging configuration.
Middle East
Figure I.12: Market price (left), and fuel supply and demand (right) in Middle Eastern ports under
the Emerging configuration.
Global Overview
Figure I.13: Global fuel supply and demand overview for the Emerging configuration.
104 Modelling Market Dynamics within the Maritime Sector

I.2.3 Fuel Supply and Bunker Prices â€“ Global
Africa
Figure I.14: Market price (left), and fuel supply and demand (right) in African ports under the
Global configuration.
Americas
Figure I.15: Market price (left), and fuel supply and demand (right) in American ports under the
Global configuration.
Asia
Figure I.16: Market price (left), and fuel supply and demand (right) in Asian ports under the
Global configuration.
Modelling Market Dynamics within the Maritime Sector 105

Europe
Figure I.17: Market price (left), and fuel supply and demand (right) in European ports under the
Global configuration.
Middle East
Figure I.18: Market price (left), and fuel supply and demand (right) in Middle Eastern ports under
the Global configuration.
Global Overview
Figure I.19: Global fuel supply and demand overview for the Global configuration.
106 Modelling Market Dynamics within the Maritime Sector

I.2.4 Fuel Supply and Bunker Prices â€“ Local
Africa
Figure I.20: Market price (left), and fuel supply and demand (right) in African ports under the
Local configuration.
Americas
Figure I.21: Market price (left), and fuel supply and demand (right) in American ports under the
Local configuration.
Asia
Figure I.22: Market price (left), and fuel supply and demand (right) in Asian ports under the
Local configuration.
Modelling Market Dynamics within the Maritime Sector 107

Europe
Figure I.23: Market price (left), and fuel supply and demand (right) in European ports under the
Local configuration.
Middle East
Figure I.24: Market price (left), and fuel supply and demand (right) in Middle Eastern ports under
the Local configuration.
Global Overview
Figure I.25: Global fuel supply and demand overview for the Local configuration.
108 Modelling Market Dynamics within the Maritime Sector

J Regulation w/o Flexibility Â­ Results
J.1 Performance Diagnostics
(a) Linear Solves
 (b) FairÂ­Share Iterations
(c) Market Dynamic Iterations
 (d) Ceiling Iterations
Figure J.1: Cumulative expected, existing and total solver calls and iteration counts under the
Regulation without Flexibility scenario across all model runs.
J.2 Market Dynamics
J.2.1 Fuel Supply and Bunker Prices â€“ CostÂ­Only, Regulation without
Flexibility 1200
Africa
Figure J.2: Market price (left), and fuel supply and demand (right) in African ports under the
CostÂ­ Only configuration.
Modelling Market Dynamics within the Maritime Sector 109

Americas
Figure J.3: Market price (left), and fuel supply and demand (right) in American ports under the
CostÂ­ Only configuration.
Asia
Figure J.4: Market price (left), and fuel supply and demand (right) in Asian ports under the CostÂ­
Only configuration.
Europe
Figure J.5: Market price (left), and fuel supply and demand (right) in European ports under the
CostÂ­ Only configuration.
110 Modelling Market Dynamics within the Maritime Sector

Middle East
Figure J.6: Market price (left), and fuel supply and demand (right) in Middle Eastern ports under
the CostÂ­Only configuration.
Global Overview
Figure J.7: Global fuel supply and demand overview for the CostÂ­Only scenario.
J.2.2 Fuel Supply and Bunker Prices â€“ Emerging
Africa
Figure J.8: Market price (left), and fuel supply and demand (right) in African ports under the
Emerging configuration.
Modelling Market Dynamics within the Maritime Sector 111

Americas
Figure J.9: Market price (left), and fuel supply and demand (right) in American ports under the
Emerging configuration.
Asia
Figure J.10: Market price (left), and fuel supply and demand (right) in Asian ports under the
Emerging configuration.
Europe
Figure J.11: Market price (left), and fuel supply and demand (right) in European ports under the
Emerging configuration.
112 Modelling Market Dynamics within the Maritime Sector

Middle East
Figure J.12: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Emerging configuration.
Global Overview
Figure J.13: Global fuel supply and demand overview for the Emerging scenario.
J.2.3 Fuel Supply and Bunker Prices â€“ Global
Africa
Figure J.14: Market price (left), and fuel supply and demand (right) in African ports under the
Global configuration.
Modelling Market Dynamics within the Maritime Sector 113

Americas
Figure J.15: Market price (left), and fuel supply and demand (right) in American ports under the
Global configuration.
Asia
Figure J.16: Market price (left), and fuel supply and demand (right) in Asian ports under the
Global configuration.
Europe
Figure J.17: Market price (left), and fuel supply and demand (right) in European ports under the
Global configuration.
114 Modelling Market Dynamics within the Maritime Sector

Middle East
Figure J.18: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Global configuration.
Global Overview
Figure J.19: Global fuel supply and demand overview for the Global scenario.
J.2.4 Fuel Supply and Bunker Prices â€“ Local
Africa
Figure J.20: Market price (left), and fuel supply and demand (right) in African ports under the
Local configuration.
Modelling Market Dynamics within the Maritime Sector 115

Americas
Figure J.21: Market price (left), and fuel supply and demand (right) in Asian ports under the
Local configuration.
Asia
Figure J.22: Market price and fuel supply in Asian ports under the Local configuration.
Europe
Figure J.23: Market price (left), and fuel supply and demand (right) in European ports under the
Local configuration.
Middle East
Figure J.24: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Local configuration.
116 Modelling Market Dynamics within the Maritime Sector

Global Overview
Figure J.25: Global fuel supply and demand overview for the Local scenario.
Modelling Market Dynamics within the Maritime Sector 117

K LevyÂ­Based Regulation Â­ Results
K.1 Performance Diagnostics
(a) Linear Solves
 (b) FairÂ­Share Iterations
(c) Market Dynamic Iterations
 (d) Ceiling Iterations
Figure K.1: Cumulative expected, existing and total solver calls and iteration counts under the
Levy-Based scenario across all model runs.
K.2 Market Dynamics
K.2.1 Fuel Supply and Bunker Prices â€“ CostÂ­Only, Levies
Africa
Figure K.2: Market price (left), and fuel supply and demand (right) in African ports under the
CostÂ­Only configuration.
118 Modelling Market Dynamics within the Maritime Sector

Americas
Figure K.3: Market price (left), and fuel supply and demand (right) in American ports under the
CostÂ­Only configuration.
Asia
Figure K.4: Market price (left), and fuel supply and demand (right) in Asian ports under the
CostÂ­Only configuration.
Europe
Figure K.5: Market price (left), and fuel supply and demand (right) in European ports under the
CostÂ­Only configuration.
Modelling Market Dynamics within the Maritime Sector 119

Middle East
Figure K.6: Bunker price and fuel supply in Middle Eastern ports.
Global Overview
Figure K.7: Global fuel supply and demand overview for the CostÂ­Only scenario.
K.2.2 Fuel Supply and Bunker Prices â€“ Emerging
Africa
Figure K.8: Market price (left), and fuel supply and demand (right) in African ports under the
Emerging configuration.
120 Modelling Market Dynamics within the Maritime Sector

Americas
Figure K.9: Market price (left), and fuel supply and demand (right) in American ports under the
Emerging configuration.
Asia
Figure K.10: Market price (left), and fuel supply and demand (right) in Asian ports under the
Emerging configuration.
Europe
Figure K.11: Market price (left), and fuel supply and demand (right) in European ports under the
Emerging configuration.
Modelling Market Dynamics within the Maritime Sector 121

Middle East
Figure K.12: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Emerging configuration.
Global Overview
Figure K.13: Global fuel supply and demand overview for the Emerging scenario.
K.2.3 Fuel Supply and Bunker Prices â€“ Global
Africa
Figure K.14: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Global configuration.
122 Modelling Market Dynamics within the Maritime Sector

Americas
Figure K.15: Market price (left), and fuel supply and demand (right) in American ports under the
Global configuration.
Asia
Figure K.16: Market price (left), and fuel supply and demand (right) in Asian ports under the
Global configuration.
Europe
Figure K.17: Market price (left), and fuel supply and demand (right) in European ports under
the Global configuration.
Modelling Market Dynamics within the Maritime Sector 123

Middle East
Figure K.18: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Global configuration.
Global Overview
Figure K.19: Global fuel supply and demand overview for the Global configuration.
K.2.4 Fuel Supply and Bunker Prices â€“ Local
Africa
Figure K.20: Market price (left), and fuel supply and demand (right) in African ports under the
Local configuration.
124 Modelling Market Dynamics within the Maritime Sector

Americas
Figure K.21: Market price (left), and fuel supply and demand (right) in American ports under the
Local configuration.
Asia
Figure K.22: Market price (left), and fuel supply and demand (right) in Asian ports under the
Local configuration.
Europe
Figure K.23: Market price (left), and fuel supply and demand (right) in European ports under
the Local configuration.
Modelling Market Dynamics within the Maritime Sector 125

Middle East
Figure K.24: Market price (left), and fuel supply and demand (right) in Middle Eastern ports
under the Local configuration.
Global Overview
Figure K.25: Global fuel supply and demand overview for the Local configuration.
126 Modelling Market Dynamics within the Maritime Sector

L Numeric Performance Analytics Results
Indicator No Regulation Reg. Flex 380 Reg. Flex 1200 Reg. No Flex LevyÂ­Based
Total Runtime (s)
CostÂ­Only 3,176.19 5,054.86 4,411.90 5,054.86 4,677.31
Global 12,966.62 12,966.62 46,174.57 41,457.37 8,787.99
Local 17,970.77 17,970.77 85,578.09 85,729.03 29,888.34
Emerging 5,397.78 5,397.78 4,508.61 4,590.55 2,299.54
Build Time (s)
CostÂ­Only 2,669.96 2,978.50 2,961.06 2,978.50 3,126.16
Global 12,218.36 3,325.38 3,313.44 3,325.38 2,364.08
Local 17,296.78 2,899.23 3,583.71 2,899.23 4,121.30
Emerging 4,541.82 2,913.45 3,004.93 2,913.45 1,649.01
Solve Time (s)
CostÂ­Only 506.23 1,421.04 770.44 1,421.04 672.03
Global 748.26 37,410.71 42,102 37,410.71 5,224.74
Local 674.00 80,471.98 81,199.50 80,471.98 22,541.63
Emerging 855.96 1,019.93 801.71 1,019.93 246.28
T able L.1: CrossÂ­scenario comparison: Runtime and Model Size.
Indicator No Regulation Reg. Flex 380 Reg. Flex 1200 Reg. No Flex LevyÂ­Based
LP Variables
CostÂ­Only 2,487,034 2,225,561 2,224,796 2,493,095 2,491,329
Global 2,221,710 2,224,796 2,224,796 2,226,536 2,226,300
Local 2,221,710 2,224,796 2,224,796 2,226,536 2,226,300
Emerging 2,220,945 2,224,796 2,224,796 2,226,536 2,224,770
LP Constraints
CostÂ­Only 1,253,299 927,385 927,056 1,358,070 1,255,539
Global 925,714 32,999 927,056 928,796 927,688
Local 925,714 57,886 927,056 928,796 927,688
Emerging 925,385 927,056 927,056 928,796 927,030
Linear Solves
CostÂ­Only 919 1,744 3,113 2,165 1,272
Global 9,107 35,791 116,396 143,018 32,275
Local 11,044 63,823 206,335 321,690 90,190
Emerging 936 1,800 3,284 4,274 1,617
T able L.2: CrossÂ­scenario comparison: LP Size and Linear Solves.
Modelling Market Dynamics within the Maritime Sector 127

Indicator No Regulation Reg. Flex 380 Reg. Flex 1200 Reg. No Flex LevyÂ­Based
FairÂ­Share Iterations
CostÂ­Only 568 1,393 2,762 1,814 921
Global 5,743 28,190 104,150 131,198 26,030
Local 7,025 51,514 181,506 293,150 75,395
Emerging 585 1,449 2,933 3,923 1,266
Market Dynamic Iterations
CostÂ­Only 0 0 0 0 0
Global 2,308 6,526 10,491 9,949 5,141
Local 2,935 11,031 22,445 25,813 13,026
Emerging 0 0 0 0 0
Ceiling Iterations
CostÂ­Only 0 0 0 0 0
Global 354 373 1,053 1,169 402
Local 382 576 1,682 2,025 1,067
Emerging 0 0 0 0 0
T able L.3: CrossÂ­scenario comparison: Iterations Statistics.
128 Modelling Market Dynamics within the Maritime Sector

M Numeric Market Dynamic Results
Scenario Metric CostÂ­Only Global Market Local Market Emerging
Market
4*No Regulation Cost (M USD) 690,719.43 690,719.33 778,085.34 690,719.34
Price (USD/t) 0.00 713,501.44 805,344.65 725,865.82
Supply (t) 369,281,258 369,286,186 369,071,577 369,285,872
Demand (t) 8,487,192,726 8,484,704,279 8,478,843,319 8,476,645,787
4*Reg. w/ Flex (380 USD) Cost 966,477.22 964,334.17 964,696.03 964,335.60
Price 0.00 1,213,406.49 1,232,522.76 1,215,866.54
Supply 2,714,684,620 2,704,209,913 2,759,435,249 2,671,962,598
Demand 8,045,589,671 8,024,744,465 8,023,802,091 7,977,805,540
4*Reg. w/ Flex (1200 USD) Cost 2,490,767.94 2,487,124.34 2,487,890.55 2,486,688.02
Price 0.00 3,570,903.77 3,773,112.60 3,905,451.29
Supply 3,773,956,265 3,773,829,322 3,941,909,638 3,628,544,662
Demand 8,478,770,347 8,274,803,009 8,279,792,441 8,075,026,520
4*Reg. No Flex (1200 USD) Cost 2,255,091.24 2,160,127.27 2,151,375.34 1,998,938.66
Price 0.00 2,991,541.88 3,174,417.87 3,120,385.72
Supply 2,802,349,106 2,784,245,401 2,775,875,916 2,785,968,196
Demand 7,801,511,169 7,698,372,354 7,664,615,429 7,706,438,704
4*Levies (800) Cost 656,800.46 1,037,688.64 750,356.90 655,218.78
Price 0.00 1,407,899.18 1,160,963.22 806,261.76
Supply 2,667,937,884 2,928,833,184 2,695,560,265 2,505,444,697
Demand 7,801,568,467 7,736,061,708 7,774,773,067 7,683,880,042
T able M.1: Comparison of aggregated system indicators across all policy scenarios and market
typologies.
Modelling Market Dynamics within the Maritime Sector 129

N Numeric Expenses Results
Indicator No Regulation Reg. Flex 380 Reg. Flex 1200 Reg. No Flex LevyÂ­Based
Total Fuel Related Expenses (EUR)
CostÂ­Only 4.30268 Ã—1012 5.43144Ã—1012 6.57223Ã—1012 5.80Ã—1012 1.41207Ã—1013
Global 4.31067Ã—1012 5.46669Ã—1012 8.03188Ã—1012 6.22Ã—1012 1.43008Ã—1013
Local 4.31318Ã—1012 5.47554Ã—1012 8.45076Ã—1012 6.37Ã—1012 1.43254Ã—1013
Emerging 4.31892 Ã—1012 5.54100Ã—1012 9.23706Ã—1012 6.81Ã—1012 1.43898Ã—1013
Vessel Related Expenses (EUR)
CostÂ­Only 1.36084 Ã—1013 1.47585Ã—1013 1.48015Ã—1013 1.49272Ã—1013 1.56244Ã—1013
Global 1.36097Ã—1013 1.47693Ã—1013 1.49448Ã—1013 1.49803Ã—1013 1.56354Ã—1013
Local 1.36143Ã—1013 1.47721Ã—1013 1.49823Ã—1013 1.50008Ã—1013 1.56346Ã—1013
Emerging 1.36154 Ã—1013 1.47959Ã—1013 1.50536Ã—1013 1.50184Ã—1013 1.56561Ã—1013
Regulation Expenses (EUR)
CostÂ­Only 0 1.19951Ã—1012 9.14584Ã—1011 2.60573Ã—1012 0
Global 0 1.19308Ã—1012 6.05848Ã—1011 2.27767Ã—1012 0
Local 0 1.19458Ã—1012 7.18097Ã—1011 2.23543Ã—1012 0
Emerging 0 1.22515Ã—1012 7.33278Ã—1011 2.30779Ã—1012 0
T able N.1: CrossÂ­scenario comparison of all expenses across five policy settings and four marÂ­
ket typologies.
130 Modelling Market Dynamics within the Maritime Sector

Modelling Market Dynamics within the Maritime Sector 131

T echnical
University of
Denmark
Brovej, Building 258
2800 Kgs. Lyngby
Tlf. 4525 1700
www.byg.dtu.dk"
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation,"1 Introduction 1
1.1 Motivation & Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 Problem Statement & Project Objectives . . . . . . . . . . . . . . . . . . . . 1
1.3 Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.4 Methods & T ools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.5 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.6 Thesis overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6",,,"9 Comparison and Discussion
After evaluating all three methods on both navigation tasks individually, they shall now be
compared with each other to assess their strengths and weaknesses. In a first step all
policies and reward functions trained for the GOÂ­Navigation task are assessed. After that
the the policies and reward functions trained for the EAÂ­Navigation task are compared.
Since all policies and reward functions have been optimised w.r.t. the validation dataset,
the comparison shall be done using the so far unused and therefore independent test
dataset.
9.1 GoalÂ­Only Navigation
GOÂ­Navigation was subject of three experiments, as BC, AIRL and AVRIL have been used
to train policies and, if applicable, reward functions. These policies and reward functions
will be compared and advantages and disadvantages w.r.t. the GOÂ­Navigation task are
discussed.
AIRL was used to train two policies and reward functions. Both, the HE and LE variant,
will be compared in this section.
9.1.1 Policies
The comparison of all three methods starts with an assessment of the learned policies.
The first step is thereby the analysis of the performance metrics computed in all test sceÂ­
narios. Those can be found in T able 9.1.
BC HEÂ­AIRL LEÂ­AIRL AVRIL
Error Mean Std Mean Std Mean Std Mean Std
End Pos. ( m) 881.74 1301.99 92.49 55.61 270.04 616.12 427.61 858.09
End Vel. ( m
s ) 0.31 0.49 5 .08 3 .41 4 .01 3 .49 0 .55 1 .09
End Cou. (Â°) 34.89 22.01 93.79 53.67 76.16 64.52 28.73 24.90
T able 9.1: Navigational performance of goalÂ­only BC, AIRL and AVRIL in test AIS scenarÂ­
ios
The performance metrics show that the policies trained by AIRL have the lowest mean
end position errors. Especially, the HEÂ­AIRLÂ­policy has a very low end position error with
a mean of only 92 .49 m and a standard deviation of 55 .61 m. The policy learned by AVRIL
is third with a mean end position error of 427.61 m. BC is least accurate in terms of the end
position error with a mean error of 881 .74 m. The very good end position accuracy of the
AIRL policies does not transfer to the other metrics. Both policies show significantly larger
end velocity and end course errors compared to the AVRIL and BCÂ­policies. The AIRL
policies have a mean velocity error of over 4 m s âˆ’1 and a course error that indicates no
consistent orientation upon ending a trajectory. This hints at the AIRLÂ­policies achieving
the navigational goals different than the experts do. BC on the other hand has a very
low mean end velocity error of only 0 .31 m sâˆ’1 and a reasonable mean end course error
of 34 .89Â°. This indicates that it imitates the expert better than the AIRL policies. AVRIL
achieves similar metrics to BC as the velocity error is slightly higher, but the course error
is lower. Therefore, AVRIL should also show better imitation performance compared to
the AIRL policies.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 113

Apart from this metricÂ­based assessment a qualitative comparison in real scenarios can
confirm these observations and provide a deeper understanding of the learned behaviours.
Figure 9.1: Scenario 324b2acf7d Â­ Comparison of
goalÂ­only BC, AIRL and AVRIL policies
Figure 9.1 visualises one of the
test scenarios where all four poliÂ­
cies act to move towards the
goalÂ­position. All policies move
the OV towards the goalÂ­position,
however there are subtle differÂ­
ences. The BCÂ­policy sweeps
of the shortest path towards the
port side of the OV. This prolongs
the path and results in it not fully
reaching the goalÂ­position. The
LEÂ­AIRL and AVRIL policies manÂ­
age to reach the goalÂ­position via
relatively linear paths that look
similar to the expertâ€™s path. The
LEÂ­AIRL policy thereby takes the
more direct route and starts roÂ­
tating upon reaching the goalÂ­
location. The HEÂ­AIRLÂ­policy
acts completely different. It accelÂ­
erates to reach the goalÂ­location very quickly, to then start rotating and circling around the
goalÂ­position. This behaviour is clearly not imitating the expert well but is still fulfilling the
GOÂ­Navigation task.
Figure 9.2: Scenario 2e31947348 Â­ Comparison of goalÂ­only BC, AIRL and AVRIL policies
A scenario where the expert does not follow a linear path is shown in Figure 9.2. Again
the LEÂ­AIRL, AVRIL and BCÂ­policies act naturally by taking relatively linear paths to the
goalÂ­position which are characterised by modest velocities. All three end the trajectory
with reasonable course error but while for BC and AVRIL this is due to the angle they
reach the goalÂ­location, the LEÂ­AVRIL policy again rotates at the goalÂ­location. Last, the
HEÂ­AIRLÂ­policy does not act naturally. It again moves fast towards the goalÂ­location and
starts rotating around the goalÂ­location once reaching it. This time the trajectory towards
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 114

the goalÂ­location also features considerable deviations from the shortest possible path.
Figure 9.3: Scenario 17a7ee92f0 Â­ Comparison of
goalÂ­only BC, AIRL and AVRIL policies
The last scenario shown in FigÂ­
ure 9.3 also requires the policies
to turn the OV to reach the goalÂ­
position. The policy learned by
BC shows the worst result in this
scenario, as it does not reach the
goalÂ­position. In the initial stages
of the trajectory, it accumulates
considerable error w.r.t. the exÂ­
pert trajectory. Until the point it
turns towards the goalÂ­position,
it has drifted so far from the
shortest path, that it simply does
not manage to reach the goalÂ­
position anymore. The policies
learned by AVRIL and AIRL manÂ­
age to reach the goalÂ­position.
The HEÂ­AIRLÂ­policy again moves
with high velocity and the already
explained circling motions around
the goalÂ­location. The LEÂ­AIRLÂ­
policy takes the most direct path towards the goalÂ­position and again rotates at the goalÂ­
location upon reaching it. The AVRIL policy imitates the expert the best, as it moves with
decent velocity and uses a similar path.
The conclusion of this analysis is that the policies trained by AIRL and AVRIL manage
to reach the goalÂ­position more reliably and with higher accuracy. The AVRIL and LEÂ­
AIRL policies show good imitation of the experts in the qualitative assessment of real
scenarios. However, AIRL is very sensitive to parameter choices, as a policy with high
entropy can achieve good position errors but also acts unnaturally. All learned policies
were not affected by the erroneous yaw rates inferred in Chapter 4.3.4. Overall, all IRL
methods are able to outperform the BC baseline in the GOÂ­Navigation task.
9.1.2 Rewards
Only AIRL and AVRIL learn reward functions, so that the results of only those two algoÂ­
rithms can be compared at this point. This section only compares the reward functions.
For a detailed analysis please refer to the respective chapters.
First, the imitation performance of all four policies can be validated by assessing the obÂ­
tained cumulated rewards of all policies w.r.t. the learned reward functions. Figure 9.4
visualises this for the LEÂ­AIRL, HEÂ­AIRL reward function and the mean of the AVRIL reÂ­
ward distribution. The three policies that show decent imitation performance obtain similar
rewards compared to the experts. This confirms that they act similarly to the experts or,
in case of the AVRILÂ­reward, even better than the experts according to the respective
reward function. However, the HEÂ­AIRLÂ­policy obtains considerably lower cumulated reÂ­
wards in the test scenarios for all three learned rewards. This shows that it has learned
behaviours that do not imitate the experts well, which was also concluded from the qualÂ­
itative inspection. Further optimisation w.r.t. one of the AIRL reward functions may lead
to more natural behaviours of the HEÂ­AIRLÂ­policy.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 115

(a) LEÂ­AIRL Reward
(b) HEÂ­AIRL Reward
(c) AVRIL Reward
Figure 9.4: Cumulated expert vs. learner rewards under goalÂ­only AIRL and AVRIL reward
functions in test AIS scenarios
Next, the similarities and differences between the learned reward functions are analysed.
This is based on the analysis of Chapters 7 and 8. The most important aspects are:
â€¢ Navigational Goals: All three learned rewards encode the navigational goals of the
experts at least as local maxima of the reward landscape. Maxima of the reward are
present if the OV is moving with a small bearing angle towards the goalÂ­position.
Figure 9.5 shows the three reward functions as scenario reward heatmaps for a
representative scenario where the OV has a course of 315Â°, a velocity of 10 kn and
is not taking any actions. Within reward regions that comply with the navigational
goals, the distance to the goal is also encoded similarly. The highest reward of these
maxima is not issued directly at the goalÂ­location. Instead, it is issued along the path
to encourage slower movements to the goalÂ­location.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 116

â€¢ Interpretability: In AIRL the interpretability of the reward function is depending on
the entropy of the policy. If the entropy is high, the reward function is able to well
represent the navigational goals of the GOÂ­Navigation task. However, if the entropy
is low, high rewards are also issued to states and actions that do not correspond to
the correct navigationÂ­goals of the experts. A possible reason for this is introduced
in Chapter 7. This shows that there is a tradeÂ­off between imitation performance
of the policy and interpretability of the reward function, which is dependent on the
hyperparameter choice. AVRIL learns a reward distribution consisting of the mean
and standard deviation of a Gaussian normal distribution. Qualitatively the regions
of high reward and low uncertainty confirm the expectations. However, in compariÂ­
son to the mean the standard deviation is very large. This makes the learned reward
distribution uninterpretable as sampling from this distribution would not reliably reÂ­
ward favourable states and actions higher than unfavourable states and actions.
(a) LEÂ­AIRL Reward
 (b) HEÂ­AIRL Reward
 (c) AVRIL Reward
Figure 9.5: Scenario 324b2acf7d Â­ Comparison of goalÂ­only AIRL and AVRIL reward funcÂ­
tions
These insights show that all three reward functions learned to represent the goal of GOÂ­
Navigation at least as local optimums of the reward functions. However, only the reward
function corresponding to the HEÂ­AIRLÂ­policy is fully interpretable. Depending on the
hyperparameter selection and the training process AIRL may also lead to reward funcÂ­
tions that reward states and actions higher which are not desirable for imitation purposes.
AVRIL learns a reward distribution, which has very high standard deviation that makes it
uninterpretable. This high uncertainty may also be attributed to the optimisation objecÂ­
tive of the Bayesian hyperparameter search. Overall, AIRL seems to be the preferable
method to infer a reward function from expert data for GOÂ­Navigation based on the inÂ­
sights gained from the conducted experiments. Additionally, it is the only algorithm where
the policy truly uses the reward function for its own optimisation, which allows for further
optimisation of the policy w.r.t. the last reward function estimate.
9.1.3 Conclusion
The findings of the analysis can be summarised as shown in T able 9.2. While the AIRL
policy shows the best performance in GOÂ­Navigation task both in accuracy and imitation
depending on the hyperparameter selection, BC and AVRIL policies show less accuracy
in terms of the position error, but also act very naturally. A reward function of AIRL can,
depending on the hyperparameter choice, correctly encode the navigational goals of GOÂ­
Navigation in interpretable way. In this work AVRIL did not offer this interpretability as the
optimisation objective possibly lead it to downÂ­prioritise learning an interpretable reward.
This leads to the conclusion that AIRL is the best choice if both a policy and a reward
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 117

BC AIRL AVRIL
Policy
T askÂ­specific performance o + + +
Imitation of expert + + + + +
Optimality w.r.t. rewards + + +
Reward
T ask representation n/a + +
Interpretability n/a + Â­ Â­
Connection to policy n/a + Â­
T able 9.2: Evaluation of BC, AIRL and AVRIL for goalÂ­only navigation
function shall be learned in the GOÂ­NavigationÂ­task. If only a policy is desired BC and
AVRIL are also valid choices, especially as these have computational advantages due to
their offline nature.
9.2 EnvironmentÂ­Aware Navigation
All three algorithms Â­ BC, AIRL and AVRIL Â­ have also been applied to the EAÂ­Navigation
task to learn policies and reward functions. As GNNÂ­based network architectures showed
improved performance to vectorÂ­based architectures in the BC experiment of Chapter 6.3,
those were used for all three algorithms and are subject to this comparison.
9.2.1 Policies
First, the policies trained by all three algorithms are compared and assessed using the
test scenarios. For AIRL it has to be kept in mind, that the learning process was limited
by long training times, which offers still considerable room for improvement of its results.
Nevertheless, the current best policy is part of this comparison.
The comparison begins with the assessment of the navigational performance metrics
computed in all test scenarios. T able 9.3 shows these metrics.
BC AIRL AVRIL
Mean Std Mean Std Mean Std
Avg. Position Error ( m) 454.16 806.54 735.06 942.10 572.34 785.28
Avg. Velocity Error ( m
s ) 0.24 0 .40 8 .42 39 .18 0.21 0.38
Avg. Course Error (Â°) 8.47 15.14 10.14 15.85 8.19 9.27
End Position Error ( m) 656.93 1591.03 1535 .31 2211 .78 778 .38 1038 .86
End Velocity Error ( m
s ) 0.36 0.54 21.71 89.57 0.29 0.47
End Course Error (Â°) 21.53 31 .36 21 .58 33 .31 17.57 19.15
T able 9.3: Navigational performance of environmentÂ­aware BC, AIRL and AVRIL in test
AIS scenarios
The metrics reveal that the BCÂ­policy performs best in terms of average position error
with a mean of 454 .16 m compared to 572.34 m for the AVRILÂ­policy and 735.06 m for the
AIRLÂ­policy. BC and AVRIL perform equally well w.r.t. the average velocity and course
errors, while AIRL exhibits a large average velocity error, due to occasional fast spinning
motions. These metrics indicate that the learned BCÂ­policy and AVRILÂ­policy perform
better in the EAÂ­Navigation task then the learned AIRLÂ­policy does.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 118

The position, velocity and course errors at the end of the trajectories confirm this. The
BC and AVRILÂ­policies show the smallest mean end position and end velocity errors with
a large margin compared to AIRL. For the mean end course error, AVRIL acts with the
highest accuracy, while BC and AIRL show similar errors. All mean end course errors
are also smaller than any mean end course error achieved by a policy trained for GOÂ­
Navigation (see T able9.1). This indicated that all policies have learned to move the OV
naturally towards the goalÂ­position in at least the majority of the scenarios. Qualitative
assessment has to confirm this.
BC AIRL AVRIL Expert
Traj. with groundings (%) 21.85 23.53 23.53 11.76
Traj. with vessel collisions (%) 2.52 6.72 2.52 2.52
Traj. with shore collisions (%) 5.04 5.88 7.56 6.72
Traj. with buoy collisions (%) 3.36 3.36 1.68 1.68
T able 9.4: Navigational safety of environmentÂ­aware BC, AIRL and AVRIL in test AIS
scenarios
Next, is the comparison of all three policies using the navigational safety metrics (see
T able9.4). The share of trajectories with groundings is high among all three policies. It
is around two times bigger than for the experts, which shows that all three policies only
have limited awareness of shallow waters. In terms of collisions with TVs, the BC and
AVRIL policy show similar performance compared to the experts, while the AIRL policy
performs worse by colliding with other vessels in almost three times more trajectories. The
awareness of shore is on a good level for all three policies. While BC and AIRL collide less
with land, the AVRILÂ­policy collides with land slightly more often compared to the experts.
The last features of the maritime environment are buoys. Only the AVRILÂ­policy performs
on a similar level compared to the experts. The BC and AIRL policy collide twice as often
with buoys compared to the experts. Overall, these metrics show that the BC and AVRIL
policies have better awareness of the environmental features. However, they still do not
act on the same level as the experts do.
This leads to the qualitative assessment of all three policies inside representative test
scenarios. The focus of the comparison is on the general waypointÂ­following behaviour,
the COLREGs and the awareness of environmental features like buoys or shallow waters.
First, the waypointÂ­following performance can be compared. Figures 9.6 and 9.7 show
that all three policies steer the OV towards the goalÂ­location. Due to earlier decisions
along the path, they do not always manage to reach the waypoint with high accuracy. The
only exception from this are the cases, where the AIRLÂ­policy starts the highly dynamic
spinning movements. One example of this behaviour has been illustrated in Figure 7.16
in Chapter 7.3.2.
The overtaking behaviour of all three policies is shown in Figure 9.6. The scenario shows
that not all three policies manage to perform the overtake. The BCÂ­policy shows the best
performance as it steers towards the starboard side when conducting the overtake to
maximise the distance to the TVs. The AVRILÂ­policy also succeeds but acts in a less
safe manner. It keeps a relatively linear path towards the goalÂ­position, which still avoids
collision. The AIRLÂ­policy collides on its way to the goalÂ­location as it drifts towards the
port side, which eventually leads to a collision with one of the TVs.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 119

Figure 9.6: Scenario 1f29d36b90 Â­ Comparison of environmentÂ­aware BC, AIRL and
AVRIL policies in overtake scenario
Figure 9.7: Scenario 17a7ee92f0 Â­ Comparison of
environmentÂ­aware BC, AIRL and AVRIL policies in
headÂ­on scenario
Figure 9.7 shows a representaÂ­
tive headÂ­on scenario. The BCÂ­
policy again manages to avoid
collision and to act COLREGÂ­
compliant by passing the TV on
the TVs port side. The AIRLÂ­
policy shows similar behaviour in
this scenario as it manages the
headÂ­on collision scenario sucÂ­
cessfully. The AVRILÂ­policy does
not avoid collision in this setting.
At the start of the trajectory, it deÂ­
viates towards the port side which
results in a collision when the polÂ­
icy tries to steer the OV back toÂ­
wards the port side of the TV.
The last of the relevant COLREG
situations is a crossing scenario.
Figure 9.8 shows a crossing sceÂ­
nario. In this case AIRL shows
the worst performance as it just
follows a linear trajectory towards
the goalÂ­position without reacting to the crossing TV. This is clearly not COLREGÂ­
compliant and also not safe. The policies learned by BC and AVRIL perform better in
this regard. They recognise the crossing TV and initiate a collisionÂ­avoiding maneuver.
This is characterised by steering towards the port side and crossing in front of the crossing
TV. This is also not COLREGÂ­compliant, but because the avoiding action is large and the
distance to the TV is maximised this behaviour can still be regarded as safe.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 120

Figure 9.8: Scenario 1a4eb14172 Â­ Comparison of environmentÂ­aware BC, AIRL and
AVRIL policies in crossing scenario
Figure 9.9: Scenario 1a99369bcb Â­ Comparison of
environmentÂ­aware BC, AIRL and AVRIL policies in
environmentÂ­restricted scenario
Last, the awareness of all three
policies regarding buoys, land,
searoutes and water depths shall
be compared. For this purpose,
Figure 9.9 shows a representaÂ­
tive test scenario. It shows that
the BCÂ­policy has gained some
awareness about its surroundings
as it avoids collision with other
vessels and moves along the
searoutes and the deeper waters.
The other two policies do not perÂ­
form that well. The AVRILÂ­policy
takes a shortcut via shallower waÂ­
ters to move to the goalÂ­position
and does not show any reaction
to environmental features such as
buoys or searoutes. The same
applies to the AIRLÂ­policy, which
even takes a longer route through
shallower waters. Therefore, only
the BCÂ­policy can be attributed
with environmentalÂ­awareness.
Overall, not all policies succeed
fully in the EAÂ­Navigation task.
BC shows the best performance
as it shows reasonable reactions
in most challenges. AIRL, on the other hand, often shows insufficient and environmentÂ­
unaware behaviour. This is probably also attributed to not being trained sufficiently due to
training time and hyperparameter limitations. AVRIL performs in between, which can be
attributed to the exploitation of a local minimum of the lossÂ­function. However, all policies
still need a lot of improvement to reliably master maritime navigation as seen from the
performance metrics. Especially, the awareness of shallow waters needs to be improved.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 121

9.2.2 Rewards
AIRL and AVRIL also learned a reward function. This section compares the properties
and limitations of both without providing an inÂ­depth analysis. For more details, please
refer to the respective chapters. As the reward distribution of the best performing AVRIL
configuration collapsed towards a standard normal distribution and was uninformative
w.r.t. the qualitative structure of the reward distribution, the reward distribution of the
second best parameter configuration is compared at this point.
First, the imitation performance of all three policies can also be evaluated using the reward
functions. The higher the cumulated rewards when acting in the test scenarios the better
the imitation performance according to the learned reward function. Figure 9.10 visuÂ­
alises the rewards obtained by the experts and all three policies w.r.t. the reward function
learned by AIRL and the mean of the reward distribution learned by AVRIL. For the AIRL
reward function one can see that the policies trained by BC and AVRIL on average obtain
higher rewards but still do not act as good as the experts according to the reward funcÂ­
tion. The AIRLÂ­policy obtains less cumulated reward and also exhibits a greater variance
in the cumulated rewards, which links to the already mentioned spinning behaviours. This
confirms the previous observation that BC and AVRIL trained better performing policies in
this projects experiments. For the mean of the AVRIL reward distribution, all three policies
achieve cumulated rewards in a similar range. The experts show much greater variance in
the obtained cumulative mean of the reward distribution. As already analysed, this reward
distribution is not interpretable as it has collapsed to a standard normal distribution.
(a) AIRL Reward
 (b) AVRIL Reward
Figure 9.10: Cumulated expert vs. learner rewards under environmentÂ­aware AIRL and
AVRIL in test AIS scenarios
The comparison of both reward functions is based on the insights gained in Chapters 7
and 8. The most important aspects are:
â€¢ Interpretability: In the conducted experiments, only AIRL learned an interpretable
reward function. AVRILs reward distribution collapsed towards a standard normal
distribution (see Figure 9.11). The reward distribution associated with the lowest
position error policy did not even provide any qualitative structure in its reward disÂ­
tribution. Another policyÂ­reward combination provided a reward distribution that conÂ­
tained qualitative structure but still collapsed to a standard normal distribution.
â€¢ Navigational goals: The navigational goals are not well represented by both learned
reward functions. AIRL mainly focuses on the proximity to the goalÂ­location and
rarely issues different rewards based on the presence of environmental features.
For AVRIL, only qualitative statements about the structure can be made, as the
full distribution is informationless. The reward distribution shows dependencies to
some environmental features but is too inconsistent from scenario to scenario to
derive comprehensible insights.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 122

â€¢ Limitations: Both rewards can be improved. In the case of AIRL computation times
were the main limitating factor, while AVRIL suffered from a hyperparameter selecÂ­
tion that prioritised good performing policies. Overcoming these issues can potenÂ­
tially lead to more interpretable results.
(a) AIRL Reward
 (b) PatternÂ­less AVRIL Reward
(c) AVRIL Reward
Figure 9.11: Scenario 324b2acf7d Â­ Comparison of environmentÂ­aware AIRL and AVRIL
reward functions
These insights show that AIRL and AVRIL both struggle at learning interpretable and EAÂ­
NavigationÂ­conform reward function. While both show signs of awareness of environmenÂ­
tal features, AIRL prioritises the distance to the goalÂ­location and AVRIL produces reward
distributions that are just standard normal distributions. Further improvements are reÂ­
quired to leverage the potential that has been shown in the GOÂ­Navigation task. For AIRL
this means expanding the experiments to longer training times and for AVRIL optimising
the hyperparamters w.r.t. a more suitable optimisation objective or adding schedules for
the lossÂ­function coefficients to escape the local minimum of the lossÂ­function.
9.2.3 Conclusion
This comparison can be summarised as shown in T able9.5. The BCÂ­policy shows the best
performance in the EAÂ­Navigation task as it has the highest awareness of the environment
around it and achieves partial COLREGÂ­compliance. By doing so it also imitates the
experts considerably better than the AIRL and AVRILÂ­policies. AIRL shows the worst taskÂ­
specific performance but offers great potential if policies could be trained longer and better
parameter optimisation could be achieved. The learned reward functions by AIRL and
AVRIL do not meet the high expectations set out by the GOÂ­Navigation experiments. The
AIRL reward function is interpretable but does not offer many insights into the COLREGÂ­
relevant aspects. More computation can also benefit this reward function. The AVRIL
reward distribution shows signs of environmental awareness but is very inconsistent in
the representation of the expertsâ€™ navigational motives. Additionally, its reward function is
informationless, as it collapsed towards a standard normal distribution.
This leads to the conclusion that BC is the best choice for EAÂ­Navigation considering the
setting of the experiments. However, further improvements may also lead to AIRL and
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 123

BC AIRL AVRIL
Policy
T askÂ­specific performance + Â­ o
Imitation of expert + Â­ o
Optimality w.r.t. rewards + o o
Reward
T ask representation n/a Â­ Â­
Interpretability n/a o Â­ Â­
Connection to policy n/a + Â­
T able 9.5: Evaluation of BC, AIRL and AVRIL for env.Â­aware navigation
AVRIL being reasonable choices.
9.3 Discussion
This thesis evaluates three different methods Â­ BC, AIRL and AVRIL Â­ for learning poliÂ­
cies in maritime navigation tasks. A key objective is to determine if these methods can
also learn interpretable reward functions that encode the navigational goals of the exÂ­
perts. Therefore, this section shall discuss the advantages and disadvantages of all three
methods based on the experience gained from the conducted experiments.
In the two navigational tasks all three algorithms showed different results in the conducted
experiments. While in the simpler GOÂ­Navigation task the two IRL methods managed to
learn policies that outperform the baseline BC approach, they failed to do so in the more
complex EAÂ­Navigation task. Still, AIRL and AVRIL offer great potential also in the EAÂ­
Navigation task, if certain limitations were to be overcome. Especially, AIRL ,through its
exploratory nature, can learn policies that are able to succeed in complex tasks given
enough computational time. However, through this exploratory property this approach
might show not the best imitation of experts as was seen in parts of the experiments. BC
and AVRIL have advantages in this regard as they solely rely on offline expert data. All
algorithms, regardless of the navigation task, also showed that they can act nearÂ­optimal
w.r.t. the learned reward function.
In terms of the learned reward functions the results are again split. In GOÂ­Navigation
AIRL managed to learn a reward function that allowed to interpret the navigational gaols
of the experts but was dependent on the hyperparameter choice. The reward distribuÂ­
tion of AVRIL suffered from high uncertainty which diminishes the interpretability, but the
structure of the distribution still represented the navigational goals well. In EAÂ­Navigation
both failed at learning rewards that could fully explain the navigational goals. However,
possible improvements have been identified and the results from the GOÂ­Navigation task
indicate that both methods can excel further at learning rewards. Nevertheless, AIRL is
the only method that truly uses only the reward for policy learning which gives its reward
function much more credibility.
Apart from the performance related aspects of the three algorithms, some algorithmic
constraints also require consideration and may motivate the usage of one or another alÂ­
gorithm. The relevant aspects are:
â€¢ Hyperparameters: All three methods differ in their number of hyperparameters and
their sensitivity to hyperparameter choices. BC requires the definition of just a few
parameters and is generally robust to different parameter settings. AVRIL does also
not require the definition of many parameters, however the correct choice of those
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 124

is more important in order to obtain an informative reward distribution. In contrast,
AIRL involves considerably more parameters, split between the reward and policy
learning components. This is especially true, if the policy is learned by an algorithm
like PPO, which has a lot of parameters. Finding parameter settings that yield good
performance is also more difficult for AIRL, since poor choices can lead to unstaÂ­
ble training. Consequently, BC is more advantageous in terms of hyperparameter
selection and tuning.
â€¢ Training time: The time required to train good performing policies is vastly different
between the offline and online methods. BC and AVRIL are comparably fast to
train due to their offline nature. In contrast, AIRL requires considerably more time
to train, since it has to iteratively sample new demonstrations from the environment
and backpropagate loses through a total of three neural networks. This makes it less
efficient, leading to higher training times. As a result, BC and AVRIL are preferable
in terms of training times.
â€¢ Data requirements: BC and AVRIL are offline methods while AIRL is an online
method. Accordingly, the methods also exhibit different requirements towards the
available expert data. BC and AVRIL perform best, when a lot of expert data is
available. Ideally, this data should be preprocessed so that all relevant traits, which
should be learned, are well represented. AIRL, on the other hand, can work with
just a few expert demonstrations, because it can explore good and bad behaviours,
also in unseen sceanrios, while acting inside the environment. Consequently, AIRL
can be applied to a wider range of problems, while the two offline methods achieve
their best results when enough highÂ­quality expert data is available.
T able9.6 summarises the results of this discussion by rating the properties of all three
algorithms w.r.t. the already introduced categories.
BC AIRL AVRIL
Policy
T askÂ­specific performance + + + +
Imitation of expert + o +
Optimality w.r.t. rewards + + +
Reward
T ask representation n/a + +
Interpretability n/a + Â­ Â­
Connection to policy n/a + Â­
Algorithm
Parameter tuning + + Â­ Â­ Â­
Training time + Â­ Â­ +
Data requirements Â­ + Â­
T able 9.6: Overall evaluation of BC, AIRL and AVRIL
The properties of all three algorithms have now been thoroughly evaluated and discussed.
However, one question remains: Under which circumstances should one choose an offline
IL algorithm like BC, an online IRL method like AIRL, or a classical RL approach that
requires the manual definition of a reward function?
The answer to this question is highly dependent on the setup of the problem. The two
main factors influencing the decision are the availability of highÂ­quality expert data and
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 125

the complexity of the task. In this context, task complexity refers to the issue of encoding
human expert knowledge into a reward function. It is further assumed, that an environÂ­
ment is available in which IRL and RL algorithms can interact. If such an environment is
not available, offline methods provide clear advantages.
In a setting where plenty of highÂ­quality expert data is available offline methods like BC
are probably preferable, as they train faster and are more robust in parameter selection.
The compounding error should also be minimal, given that the expert demonstrations
sufficiently cover the states and action the agent may encounter. In case the compunding
error proves to be an issue, a policy learned by BC can also be further optimised using
an IRL algorithm like AIRL to excel in unseen scenarios.
If no expert data is available, one is forced to adopt RL methods to learn policies. In this
case the developer or researcher has no choice other than to embed his domain knowlÂ­
edge into an expressive reward function and spend considerable effort in training and
hyperparameter optimisation. This is feasible for small and simple problems, however,
the larger the complexity of the task, the more difficult this gets.
In fact, it might be worth to generate a few highÂ­quality expert demonstrations and apply
IRL algorithms like AIRL. Especially, in tasks that are difficult to encode in a reward funcÂ­
tion, IRL methods show big advantages as they are able to learn meaningful behaviour
from small samples of data. Additionally, the reward function can be analysed to assess
the true objectives of the agents. Also in terms of training times, such an approach is not
worse than a plain RL approach.
The summary of this is, that offline IL methods like BC should be used if a lot of highÂ­quality
expert data is available. RL methods are the right choice if no expert data is available or
the task is very simple. IRL methods like AIRL excel in the area in between. It is most
suitable for cases where either only insufficient expert data is available, a policy shall be
further optimised w.r.t. unseen scenarios or a definition of a reward function proves to be
to difficult for a human.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 126

10 Conclusion
This chapter concludes this thesis by first revisiting the objectives and achievements of
the project. The limitations of the proposed approaches and methods are summarized
and suggestions for future work are provided. Finally, a conclusion is drawn.
10.1 Achievements
This thesis set out to explore the potential of IRL for decision making in ASN, using AISÂ­
data to understand if the IRL methodology is a viable approach to remove the human
definition of reward functions on the path towards full autonomy on the sea. More specifÂ­
ically, this work aimed at meeting three objectives:
1. Formulation of the ASN problem as an IRL problem using AISÂ­data
2. Development of IRLÂ­based solutions for ASN
3. Evaluation of developed IRLÂ­based solutions for ASN
The first objective has been met as the thesis proposes and evaluates different possibiliÂ­
ties to translate AISÂ­data into a format suitable for IRLÂ­based learning algorithms, which is
a key novelty of this thesis. Among the proposed methods, two proved to be most effecÂ­
tive. Transferring the environmental information into a graphÂ­based description proved to
be computationally efficient and allowed policies and reward functions to learn meaningful
behaviours. The actions, which were originally not a part of the data, could be extracted
efficiently and accurately from the AISÂ­data by applying a KalmanÂ­filter.
The second objective has also been achieved by developing three different algorithms Â­
BC, AIRL and AVRIL Â­ for two maritime navigation tasks. BC served as performance baseÂ­
line, while AIRL and AVRIL are reward learning methods. The two maritime navigation
tasks were the simple GOÂ­Navigation and the more advanced EAÂ­Navigation tasks.
The last objective aimed at evaluating the policies and reward functions of the developed
solutions. In GOÂ­Navigation, all three methods were able to learn policies that reached
a defined goalÂ­position. Evaluation of the policies demonstrated that the IRL methods,
AIRL and AVRIL, outperformed the baseline BC approach. In the more advanced task of
EAÂ­Navigation the baseline BC approach was superior in COLREGÂ­compliance and enÂ­
vironmental awareness compared to AVRIL and AIRL. However, AIRL still provides room
for further improvement through extended training and better choice of parameters. AVRIL
can be further improved by avoiding local minima of the lossÂ­function. Additionally, the
learned reward functions were analysed using suitable visualisation tools. The reward
function learned by AIRL proved to be interpretable but mainly encoded the waypointÂ­
following objective, while other navigational motives of the experts were less represented
in the reward. AVRIL on the other hand, produced reward distributions with large unÂ­
certainties in the GOÂ­Navigation task, making them not interpretable. In EAÂ­Navigation
the reward distributions collapsed to a standard normal distribution, indicating that further
optimisations are necessary.
Achieving these three objectives demonstrated the viability of IRLÂ­based solutions for
ASN tasks, bringing the field of ASN closer to fully autonomous and COLREGÂ­compliant
navigation systems without the need to manually define reward functions. However, some
limitations still need to be addressed on this journey to overcome some of the issues
encountered in this project.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 127

10.2 Limitations and Future Work
Some limitations of the taken approaches and chosen methods have also been identiÂ­
fied and function as foundation for future work. The following sections introduce those
limitations and propose ideas for future improvements.
Data limitations
The quantity of available AISÂ­data was sufficient for this project. However, many valid
trajectories feature merely linear motion without considerable turning or accelerating acÂ­
tions. This dataset imbalance encourages policies to prioritise just moving straight. T o
avoid this issue a closer inspection of the AISÂ­data is necessary to achieve a more equal
distribution between scenarios with linear movements and scenarios with actual evasive
actions.
Additionally, trajectories that contained planned encounters with pilot vessels were part
of the dataset. In hindsight, those trajectories should have been removed from the data,
since it is hard to infer the vessel the pilot vessel is trying to approach.
Data representation limitations
Extracting and representing the information from the AISÂ­data in suitable format is also
subject to some limitations. Especially the observation definition has aspects that might
require improvement.
A fix of the introduced error in relation to the inference of actions using the KalmanÂ­filter
is the number one priority when continuing with this work. Not having the unreasonably
large yaw rates in the data, should allow the policies and reward functions to learn better
and faster. Nevertheless, the error did not have a significant effect on the results of this
thesis.
The share of trajectories with groundings was considerably high for all policies trained
for the EAÂ­Navigation task. This indicates that the representation of depths in the graphÂ­
based observation is not ideal to learn behaviours that avoid shallow waters. One apÂ­
proach to improve on this, is to describe the seabed as a twoÂ­dimensional grid around
the OVs position, where each cell expresses the depth at that specific location. Instead
of using a GNN as feature extractor a CNN could be used. This would provide a denser
representation of the seafloor.
In the current definition of the graphÂ­based observation OGRA, the proprioceptive features
about the OV itself are part of the vessel graph. When the graph is processed by the
GNNs, this information dissipates through aggregation with the nodes and edges from
the TVs. This is evident in policies trained by AIRL which tend to apply unreasonable
speeds and yaw rates to the OV. Therefore, it is sensible to add a separate feature vector
to the observation that just contains proprioceptive information like the OVs COG or SOG.
The mentioned points are limitations of the current graphÂ­based observation definition.
However, during the project some more ideas have been developed w.r.t. the observation
definition that might be worth investigating in the future to improve performance of policies.
The graphÂ­based observation OGRA consists of a separate graph for each feature class of
the environment (e.g. vessels or buoys). Instead of keeping these separate, a heterogeÂ­
nous graph that models the dependencies and interactions of all those elements might be
an interesting opportunity to enhance performance.
Another option to enhance the information contained in the graphÂ­based observation
OGRA, is to incorporate human domain knowledge. For example, adding a feature inÂ­
dicating a collision or running aground of the OV should ease learning collision avoiding
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 128

behaviours in the IRL setting. Similarly, adding features encoding the obligations of the
COLREGs might also be a valuable addition.
Algorithmic limitations
Some limitations and improvements are also associated with the chosen algorithms and
their implementation.
First, the two IRL algorithms evaluated in this thesis are only a small share of available
algorithms from the literature. Evaluating different algorithms might provide further imÂ­
provements in policy performance and reward interpretability. Especially the VILD [90]
and IRLEED [91] algorithms are promising as they allow to determine different expertise
levels of demonstrators. This aspect is interesting to define factors contributing to good
seamanship.
Not only the choice of algorithms has been restricted in the thesis. Also the choice of netÂ­
work architectures provides further avenues for exploration. First, different graph layers
and configurations of those graph layers are worth exploring to further optimise perforÂ­
mance. Different architectures per feature extractor instead of the same for every feature
extractor can also improve performance through enhanced representational power. HowÂ­
ever, this also increases the number of parameters. Second, the current architectures
base their prediction just on a state from one timestep. However, navigational decisions
are best based on a sequence of previous states. T o model this, recurrent network archiÂ­
tectures operating on the concatenated outputs of the feature extractors could be used to
model temporal dependencies. Long Short T erm Memorys (LSTMs) are a good starting
point for such investigations.
AIRL is a powerful method, but to fully excel in the EAÂ­Navigation task some improveÂ­
ments are needed. As this algorithm is mainly restricted by training times and hyperpaÂ­
rameter choices, improving those aspects may lead to great performance improvements.
In particular, training the policies and reward functions for more epochs should lead to
convergence to better results. Additionally, optimising the hyperparameters in less trainÂ­
ing time restrictive ranges can lead to considerable improvements.
The main training time constraint originates from the RL algorithm used. PPO requires
iterative sampling from the environment and the expensive learning of a value function to
compute advantage estimates. By choosing an offÂ­policy RL algorithm like SAC compuÂ­
tation times can be improved. However, training stability might be compromised.
The AIRL method can also be modified by augmenting the reward function. Reward terms
that are easily encodable by human experts can be added to the reward function. Those
terms could either penalise collisions or reward compliance to the COLREGs. This apÂ­
proach might ease the learning of expressive reward functions and therefore well perÂ­
forming policies.
Last, the main limitation of AVRIL is that the learned reward distribution does not conÂ­
tain any meaningful information due to its high variance or collapsing towards a standard
normal distribution. T o address this issue, a different hyperparameter optimisation objecÂ­
tive can be chosen in order to achieve higher importance of the reward learning term of
the lossÂ­function in conjunction with expressive policies. Additionally, changing the coÂ­
efficients of the lossÂ­function during training via scheduler can help to escape the local
minima of the reward function. Ultimately, this should lead to better policies and more
expressive reward distributions. An additional area for research would be the use of a difÂ­
ferent prior in the regularisation objective of AVRIL. This might also help to learn aligning
the reward distribution to something more informative.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 129

10.3 Conclusion
Overall, this thesis demonstrates that IRL methods are a promising and viable approach
to solve the ASN problem in an endÂ­toÂ­end fashion. Policies trained by IRL methods are
able to outperform baseline BC approaches in simple navigational tasks and the learned
reward functions prove to be a powerful tool to understand the navigational motives. HowÂ­
ever, some more improvements to the preprocessing of the data, the observation definiÂ­
tions and the algorithms are needed to also excel in more advanced navigational tasks
and to achieve robust, safe and COLREGÂ­compliant autonomy at sea using IRL methods.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 130

Bibliography
[1] The Metro in Copenhagen is driverless . en. URL: https://intl.m.dk/about- the-
metro/facts/trains/ (visited on 02/17/2025).
[2] Waymo Â­ SelfÂ­Driving Cars Â­ Autonomous Vehicles Â­ RideÂ­Hail . en. URL: https :
//waymo.com/ (visited on 02/17/2025).
[3] Christopher Whitt et al. â€œFuture Vision for Autonomous Ocean Observationsâ€. EnÂ­
glish. In: Frontiers in Marine Science 7 (Sept. 2020). DOI: 10.3389/fmars.2020.
00697.
[4] The Future of Maritime Autonomous Surface Ships (MASS) . T ech. rep. IALA (InÂ­
ternational Organization for Marine Aids to Navigation), 2024. URL: https://www.
iala.int/content/uploads/2024/02/The-Future-of-Mass-2024-Portrait-simples-pages-
for-website-corrected.pdf (visited on 02/17/2025).
[5] Autonomous ships The next step . T ech. rep. RollsÂ­Royce plc, 2016. URL: https:
//www.rolls- royce.com/~/media/Files/R/Rolls-Royce/documents/%20customers/
marine/ship-intel/rr-ship-intel-aawa-8pg.pdf (visited on 02/17/2025).
[6] Convention on the International Regulations for Preventing Collisions at Sea, 1972
(COLREGs). URL: https://www.imo.org/en/About/Conventions/Pages/COLREG.
aspx (visited on 02/17/2025).
[7] I. B. Hagen et al. â€œMPCÂ­based Collision Avoidance Strategy for Existing MaÂ­
rine Vessel Guidance Systemsâ€. en. In: 2018 IEEE International Conference on
Robotics and Automation (ICRA) . Brisbane, QLD: IEEE, May 2018, pp. 7618â€“
7623. DOI: 10.1109/icra.2018.8463182.
[8] BjornÂ­Olav H. Eriksen and Morten Breivik. â€œMPCÂ­Based midÂ­level collision avoidÂ­
ance for asvs using nonlinear programmingâ€. en. In: 2017 IEEE Conference on
Control Technology and Applications (CCTA). Mauna Lani Resort, HI, USA: IEEE,
Aug. 2017, pp. 766â€“772. DOI: 10.1109/ccta.2017.8062554.
[9] Mohamed Abdelaal, Martin FrÃ¤nzle, and Axel Hahn. â€œNonlinear Model Predictive
Control for trajectory tracking and collision avoidance of underactuated vessels
with disturbancesâ€. en. In: Ocean Engineering 160 (July 2018), pp. 168â€“180. DOI:
10.1016/j.oceaneng.2018.04.026.
[10] Thanapong Phanthong et al. â€œApplication of A* algorithm for realÂ­time path reÂ­
planning of an unmanned surface vehicle avoiding underwater obstaclesâ€. en. In:
Journal of Marine Science and Application 13.1 (Mar. 2014), pp. 105â€“116. DOI:
10.1007/s11804-014-1224-3 .
[11] S. Campbell and W. Naeem. â€œA RuleÂ­based Heuristic Method for COLREGSÂ­
compliant Collision Avoidance for an Unmanned Surface Vehicleâ€. en. In: IFAC
Proceedings Volumes 45.27 (2012), pp. 386â€“391. DOI: 10.3182/20120919- 3- it-
2046.00066.
[12] Yoshiaki Kuwata et al. â€œSafe Maritime Autonomous Navigation With COLREGS,
Using Velocity Obstaclesâ€. In: IEEE Journal of Oceanic Engineering 39.1 (Jan.
2014), pp. 110â€“119. DOI: 10.1109/joe.2013.2254214.
[13] D. K.M. Kufoalor, E. F . Brekke, and T . A. Johansen. â€œProactive Collision AvoidÂ­
ance for ASVs using A Dynamic Reciprocal Velocity Obstacles Methodâ€. In: 2018
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) .
Oct. 2018, pp. 2402â€“2409. DOI: 10.1109/iros.2018.8594382.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 131

[14] BjornÂ­Olav H. Eriksen et al. â€œRadarÂ­based maritime collision avoidance using dyÂ­
namic windowâ€. en. In: 2018 IEEE Aerospace Conference . Big Sky, MT : IEEE,
Mar. 2018, pp. 1â€“9. DOI: 10.1109/aero.2018.8396666.
[15] Einvald Serigstad, BjÃ¸rnÂ­Olav H. Eriksen, and Morten Breivik. â€œHybrid Collision
Avoidance for Autonomous Surface Vehiclesâ€. en. In: IFACÂ­PapersOnLine 51.29
(2018), pp. 1â€“7. DOI: 10.1016/j.ifacol.2018.09.460.
[16] Roman Smierzchalski. â€œEvolutionary trajectory planning of ships in navigation trafÂ­
fic areasâ€. en. In: Journal of Marine Science and Technology 4.1 (Sept. 1999),
pp. 1â€“6. DOI: 10.1007/s007730050001.
[17] T omasz Praczyk. â€œNeural antiÂ­collision system for Autonomous Surface Vehicleâ€.
en. In: Neurocomputing 149 (Feb. 2015), pp. 559â€“572. DOI: 10.1016/j.neucom.
2014.08.018.
[18] Agnieszka Lazarowska. â€œShipâ€™s Trajectory Planning for Collision Avoidance at Sea
Based on Ant Colony Optimisationâ€. en. In: The Journal of Navigation 68.2 (Mar.
2015), pp. 291â€“307. DOI: 10.1017/s0373463314000708.
[19] Yanzhuo Xue et al. â€œAutomatic simulation of ship navigationâ€. en. In: Ocean EngiÂ­
neering 38.17Â­18 (Dec. 2011), pp. 2290â€“2305. DOI: 10.1016/j.oceaneng.2011.10.
011.
[20] ClÃ©ment PÃªtrÃ¨s, MiguelÂ­Angel RomeroÂ­Ramirez, and FrÃ©dÃ©ric Plumet. â€œReactive
path planning for autonomous sailboatâ€. In: 2011 15th International Conference
on Advanced Robotics (ICAR) . June 2011, pp. 112â€“117. DOI: 10.1109/icar.2011.
6088585.
[21] Qingyang Xu, Chengjin Zhang, and Li Zhang. â€œDeep convolutional neural network
based unmanned surface vehicle maneuveringâ€. In: 2017 Chinese Automation
Congress (CAC). Oct. 2017, pp. 878â€“881. DOI: 10.1109/cac.2017.8242889.
[22] Yuanyuan Qiao et al. Survey of Deep Learning for Autonomous Surface Vehicles
in the Marine Environment . Jan. 2023. DOI: 10.48550/arXiv.2210.08487.
[23] Joohyun Woo, Chanwoo Yu, and Nakwan Kim. â€œDeep reinforcement learningÂ­
based controller for path following of an unmanned surface vehicleâ€. en. In: Ocean
Engineering 183 (July 2019), pp. 155â€“166. DOI: 10.1016/j.oceaneng.2019.04.099.
[24] Le Pham Tuyen et al. â€œDeep reinforcement learning algorithms for steering an
underactuated shipâ€. en. In: 2017 IEEE International Conference on Multisensor
Fusion and Integration for Intelligent Systems (MFI) . Daegu: IEEE, Nov. 2017,
pp. 602â€“607. DOI: 10.1109/mfi.2017.8170388.
[25] Joel Jose, Md Shadab Alam, and Abhilash Sharma Somayajula. Navigating the
Ocean with DRL: Path following for marine vessels . Oct. 2023. DOI: 10 . 48550 /
arXiv.2310.14932.
[26] Andreas Bell Martinsen. â€œEndÂ­toÂ­end training for path following and control of maÂ­
rine vehiclesâ€. eng. MA thesis. Ntnu, 2018. URL: https://ntnuopen.ntnu.no/ntnu-
xmlui/handle/11250/2559484 (visited on 02/18/2025).
[27] Andreas B. Martinsen and Anastasios M. Lekkas. â€œStraightÂ­Path Following for UnÂ­
deractuated Marine Vessels using Deep Reinforcement Learningâ€. en. In: IFACÂ­
PapersOnLine 51.29 (2018), pp. 329â€“334. DOI: 10.1016/j.ifacol.2018.09.502.
[28] Andreas B. Martinsen and Anastasios M. Lekkas. â€œCurved Path Following with
Deep Reinforcement Learning: Results from Three Vessel Modelsâ€. en. In:
OCEANS 2018 MTS/IEEE Charleston . Charleston, SC: IEEE, Oct. 2018, pp. 1â€“8.
DOI: 10.1109/oceans.2018.8604829.
[29] Yin Cheng and Weidong Zhang. â€œConcise deep reinforcement learning obstacle
avoidance for underactuated unmanned marine vesselsâ€. en. In: Neurocomputing
272 (Jan. 2018), pp. 63â€“73. DOI: 10.1016/j.neucom.2017.06.066.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 132

[30] Joohyun Woo and Nakwan Kim. â€œCollision avoidance for an unmanned surface
vehicle using deep reinforcement learningâ€. en. In: Ocean Engineering 199 (Mar.
2020), p. 107001. DOI: 10.1016/j.oceaneng.2020.107001.
[31] Ingunn Johanne Vallestad. â€œPath Following and Collision Avoidance for Marine
Vessels with Deep Reinforcement Learningâ€. eng. MA thesis. Ntnu, 2019. URL:
https : / / ntnuopen . ntnu . no / ntnu - xmlui / handle / 11250 / 2625707 (visited on
02/18/2025).
[32] Siyu Guo et al. â€œAn Autonomous Path Planning Model for Unmanned Ships Based
on Deep Reinforcement Learningâ€. en. In: Sensors 20.2 (Jan. 2020), p. 426. DOI:
10.3390/s20020426.
[33] Thomas Nakken Larsen et al. RiskÂ­based implementation of COLREGs for auÂ­
tonomous surface vehicles using deep reinforcement learning . Nov. 2021. DOI:
10.48550/arXiv.2112.00115.
[34] Eivind Meyer et al. Taming an autonomous surface vehicle for path following and
collision avoidance using deep reinforcement learning . Dec. 2019. DOI: 10.48550/
arXiv.1912.08578.
[35] Eivind Meyer et al. COLREGÂ­Compliant Collision Avoidance for Unmanned SurÂ­
face Vehicle using Deep Reinforcement Learning . June 2020. DOI: 10 . 48550 /
arXiv.2006.09540.
[36] Wei Guan, Zhewen Cui, and Xianku Zhang. â€œIntelligent Smart Marine Autonomous
Surface Ship Decision System Based on Improved PPO Algorithmâ€. en. In: SenÂ­
sors 22.15 (Jan. 2022), p. 5732. DOI: 10.3390/s22155732.
[37] Luman Zhao and MyungÂ­Il Roh. â€œCOLREGsÂ­compliant multiship collision avoidÂ­
ance based on deep reinforcement learningâ€. en. In: Ocean Engineering 191 (Nov.
2019), p. 106436. DOI: 10.1016/j.oceaneng.2019.106436.
[38] Thomas Nakken Larsen et al. â€œComparing Deep Reinforcement Learning AlgoÂ­
rithmsâ€™ Ability to Safely Navigate Challenging Watersâ€. English. In: Frontiers in
Robotics and AI 8 (Sept. 2021). DOI: 10.3389/frobt.2021.738113.
[39] Alexandra Vedeler and Narada Warakagoda. â€œGenerative Adversarial Immitation
Learning for Steering an Unmanned Surface Vehicleâ€. en. In: Proceedings of the
Northern Lights Deep Learning Workshop 1 (Feb. 2020), p. 6. DOI: 10.7557/18.
5147.
[40] Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation Learning. June
2016. DOI: 10.48550/arXiv.1606.03476.
[41] John Schulman et al. Trust Region Policy Optimization. Apr. 2017. DOI: 10.48550/
arXiv.1502.05477.
[42] Pieter Abbeel and Andrew Y . Ng. â€œApprenticeship learning via inverse reinforceÂ­
ment learningâ€. en. In: TwentyÂ­first international conference on Machine learning Â­
ICML â€™04. Banff, Alberta, Canada: ACM Press, 2004, p. 1. DOI: 10.1145/1015330.
1015430.
[43] Mao Zheng et al. Research on autonomous collision avoidance of merchant
ship based on inverse reinforcement learning . en. 2020. DOI: 10 . 1177 /
1729881420969081.
[44] T akefumi Higaki, Hirotada Hashimoto, and Hitoshi Yoshioka. â€œInvestigation and
Imitation of Human Captainsâ€™ Maneuver Using Inverse Reinforcement Learningâ€.
en. In: Journal of the Japan Society of Naval Architects and Ocean Engineers 36
(2022), pp. 137â€“148. DOI: 10.2534/jjasnaoe.36.137.
[45] Brian D Ziebart et al. â€œMaximum Entropy Inverse Reinforcement Learningâ€. In:
AAAI Conference on Artificial Intelligence (2008). DOI: 10.5555/1620270.1620297.
URL: https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 133

[46] T akefumi Higaki and Hirotada Hashimoto. â€œHumanÂ­like route planning for autoÂ­
matic collision avoidance using generative adversarial imitation learningâ€. en. In:
Applied Ocean Research 138 (Sept. 2023), p. 103620. DOI: 10.1016/j.apor.2023.
103620.
[47] John Schulman et al. Proximal Policy Optimization Algorithms . Aug. 2017. DOI:
10.48550/arXiv.1707.06347.
[48] Piyabhum Chaysri et al. â€œUnmanned surface vehicle navigation through generaÂ­
tive adversarial imitation learningâ€. en. In: Ocean Engineering 282 (Aug. 2023),
p. 114989. DOI: 10.1016/j.oceaneng.2023.114989.
[49] Lingyu Li, Yong Ma, and Defeng Wu. â€œUnderactuated MSV path following control
via stable adversarial inverse reinforcement learningâ€. en. In: Ocean Engineering
299 (May 2024), p. 117368. DOI: 10.1016/j.oceaneng.2024.117368.
[50] Justin Fu, Katie Luo, and Sergey Levine. Learning Robust Rewards with AdversarÂ­
ial Inverse Reinforcement Learning . Aug. 2018. DOI: 10.48550/arXiv.1710.11248.
[51] Python 3.12 documentation . en. URL: https : / / docs . python . org / 3/(visited on
05/01/2025).
[52] Jason Ansel et al. â€œPyT orch 2: Faster Machine Learning Through Dynamic Python
Bytecode Transformation and Graph Compilationâ€. en. In: Proceedings of the 29th
ACM International Conference on Architectural Support for Programming LanÂ­
guages and Operating Systems, Volume 2 . La Jolla CA USA: Acm, Apr. 2024,
pp. 929â€“947. DOI: 10.1145/3620665.3640366.
[53] Mark T owers et al. Gymnasium: A Standard Interface for Reinforcement Learning
Environments. Nov. 2024. DOI: 10.48550/arXiv.2407.17032.
[54] Matthias Fey and Jan Eric Lenssen. Fast Graph Representation Learning with
PyTorch Geometric. Apr. 2019. DOI: 10.48550/arXiv.1903.02428.
[55] Charles R. Harris et al. â€œArray Programming with NumPyâ€. In: Nature 585.7825
(Sept. 2020), pp. 357â€“362. DOI: 10.1038/s41586-020-2649-2 .
[56] Sean Gillies et al. Shapely. Apr. 2025. URL: https://github.com/shapely/shapely.
[57] John D. Hunter. â€œMatplotlib: A 2D Graphics Environmentâ€. In: Computing in SciÂ­
ence & Engineering 9.3 (May 2007), pp. 90â€“95. DOI: 10.1109/mcse.2007.55.
[58] Lukas Biewald. Experiment Tracking with Weights and Biases . Software available
from wandb.com. 2020. URL: https://www.wandb.com/.
[59] Richard S. Sutton and Andrew Barto. Reinforcement learning: an introduction . en.
Second edition. Adaptive computation and machine learning. Cambridge, MasÂ­
sachusetts London, England: The MIT Press, 2020. ISBN: 978Â­0Â­262Â­03924Â­6.
[60] Nathan Gavenski et al. A Survey of Imitation Learning Methods, Environments and
Metrics. July 2024. DOI: 10.48550/arXiv.2404.19456.
[61] Hado van Hasselt. Lecture 9: Policy Gradients and Actor Critics . en. 2021. URL:
https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/
Lecture % 209 - %20Policy % 20gradients % 20and % 20actor % 20critics . pdf(visited on
06/08/2025).
[62] Trust Region Policy Optimization â€” Spinning Up documentation . URL: https://
spinningup.openai.com/en/latest/algorithms/trpo.html (visited on 03/28/2025).
[63] Proximal Policy Optimization â€” Spinning Up documentation . URL: https : / /
spinningup.openai.com/en/latest/algorithms/ppo.html (visited on 03/28/2025).
[64] Timothy P . Lillicrap et al.Continuous control with deep reinforcement learning. July
2019. DOI: 10.48550/arXiv.1509.02971.
[65] Deep Deterministic Policy Gradient â€” Spinning Up documentation . URL: https :
//spinningup.openai.com/en/latest/algorithms/ddpg.html#references (visited on
03/28/2025).
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 134

[66] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing Function ApproxiÂ­
mation Error in ActorÂ­Critic Methods . Oct. 2018. DOI: 10.48550/arXiv.1802.09477.
[67] Twin Delayed DDPG â€” Spinning Up documentation . URL: https : / / spinningup .
openai.com/en/latest/algorithms/td3.html (visited on 03/28/2025).
[68] Tuomas Haarnoja et al. Soft ActorÂ­Critic: OffÂ­Policy Maximum Entropy Deep ReinÂ­
forcement Learning with a Stochastic Actor . Aug. 2018. DOI: 10.48550/arXiv.1801.
01290.
[69] Soft ActorÂ­Critic â€” Spinning Up documentation . URL: https://spinningup.openai.
com/en/latest/algorithms/sac.html (visited on 03/28/2025).
[70] Boyuan Zheng et al. Imitation Learning: Progress, Taxonomies and Challenges .
Oct. 2022. DOI: 10.48550/arXiv.2106.12177.
[71] Michael Bain and Claude Sammut. â€œA framework for behavioural cloningâ€. en. In:
Machine Intelligence 15 . Oxford University PressOxford, Jan. 2000, pp. 103â€“129.
DOI: 10.1093/oso/9780198538677.003.0006.
[72] Dean A. Pomerleau. â€œALVINN: An Autonomous Land Vehicle in a Neural NetÂ­
workâ€. In: Proceedings of the 2nd International Conference on Neural InformaÂ­
tion Processing Systems . Nipsâ€™88. Morgan Kaufmann, 1988, pp. 305â€“313. DOI:
10.5555/2969735.2969771.
[73] Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. A Reduction of ImitaÂ­
tion Learning and Structured Prediction to NoÂ­Regret Online Learning . Mar. 2011.
DOI: 10.48550/arXiv.1011.0686.
[74] Mark Beliaev et al. Imitation Learning by Estimating Expertise of Demonstrators .
June 2022. DOI: 10.48550/arXiv.2202.01288.
[75] Siddharth Reddy, Anca D. Dragan, and Sergey Levine. SQIL: Imitation Learning
via Reinforcement Learning with Sparse Rewards . Sept. 2019. DOI: 10 . 48550 /
arXiv.1905.11108.
[76] Ian J. Goodfellow et al. Generative Adversarial Networks . June 2014. DOI: 10 .
48550/arXiv.1406.2661.
[77] Chelsea Finn et al. A Connection between Generative Adversarial Networks, InÂ­
verse Reinforcement Learning, and EnergyÂ­Based Models . Nov. 2016. DOI: 10.
48550/arXiv.1611.03852.
[78] Faraz T orabi, Garrett Warnell, and Peter Stone. Generative Adversarial Imitation
from Observation. June 2019. DOI: 10.48550/arXiv.1807.06158.
[79] Yunzhu Li, Jiaming Song, and Stefano Ermon. InfoGAIL: Interpretable Imitation
Learning from Visual Demonstrations. Nov. 2017. DOI: 10.48550/arXiv.1703.08840.
[80] Stephen Adams, Tyler Cody, and Peter A. Beling. â€œA survey of inverse reinforceÂ­
ment learningâ€. en. In: Artificial Intelligence Review 55.6 (Aug. 2022), pp. 4307â€“
4346. DOI: 10.1007/s10462-021-10108-x .
[81] Andrew Y . Ng and Stuart J. Russell. â€œAlgorithms for Inverse Reinforcement LearnÂ­
ingâ€. In: Proceedings of the Seventeenth International Conference on Machine
Learning. Icml â€™00. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.,
June 2000, pp. 663â€“670. DOI: 10.5555/645529.657801.
[82] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. â€œMaximum margin
planningâ€. en. In: Proceedings of the 23rd international conference on Machine
learning Â­ ICML â€™06 . Pittsburgh, Pennsylvania: ACM Press, 2006, pp. 729â€“736.
DOI: 10.1145/1143844.1143936.
[83] Umar Syed and Robert E Schapire. â€œA GameÂ­Theoretic Approach to ApprenticeÂ­
ship Learningâ€. In: Advances in Neural Information Processing Systems . Vol. 20.
Curran Associates, Inc., 2007. URL: https://proceedings.neurips.cc/paper_files/
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 135

paper / 2007 / hash / ca3ec598002d2e7662e2ef4bdd58278b - Abstract . html(visited on
06/07/2025).
[84] Gergely Neu and Csaba Szepesvari. Apprenticeship Learning using Inverse ReinÂ­
forcement Learning and Gradient Methods . June 2012. DOI: 10.48550/arXiv.1206.
5264.
[85] Deepak Ramachandran and Eyal Amir. â€œBayesian Inverse Reinforcement LearnÂ­
ingâ€. en. In: Proceedings of the 20th International Joint Conference on Artifical
Intelligence. Ijcaiâ€™07 (2007), pp. 2586â€“2591. DOI: 10.5555/1625275.1625692.
[86] Jaedeug Choi and KeeÂ­eung Kim. â€œMAP Inference for Bayesian Inverse ReinÂ­
forcement Learningâ€. In: Advances in Neural Information Processing Systems .
Vol. 24. Curran Associates, Inc., 2011. URL: https://papers.nips.cc/paper_files/
paper / 2011 / hash / 3a15c7d0bbe60300a39f76f8a5ba6896 - Abstract . html(visited on
06/06/2025).
[87] Alex J. Chan and Mihaela van der Schaar. Scalable Bayesian Inverse ReinforceÂ­
ment Learning. Mar. 2021. DOI: 10.48550/arXiv.2102.06483.
[88] Abdeslam Boularias, Jens Kober, and Jan Peters. â€œRelative Entropy Inverse ReÂ­
inforcement Learningâ€. en. In: Proceedings of the Fourteenth International ConÂ­
ference on Artificial Intelligence and Statistics . JMLR Workshop and Conference
Proceedings, June 2011, pp. 182â€“189. URL: https://proceedings.mlr.press/v15/
boularias11a.html (visited on 06/07/2025).
[89] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided Cost Learning: Deep
Inverse Optimal Control via Policy Optimization . May 2016. DOI: 10.48550/arXiv.
1603.00448.
[90] Voot T angkaratt et al. VILD: Variational Imitation Learning with DiverseÂ­quality
Demonstrations. Sept. 2019. DOI: 10.48550/arXiv.1909.06769.
[91] Mark Beliaev and Ramtin Pedarsani. Inverse Reinforcement Learning by EstimatÂ­
ing Expertise of Demonstrators . Dec. 2024. DOI: 10.48550/arXiv.2402.01886.
[92] Ilya Kostrikov et al. DiscriminatorÂ­ActorÂ­Critic: Addressing Sample Inefficiency and
Reward Bias in Adversarial Imitation Learning . Oct. 2018. DOI: 10.48550/arXiv.
1809.02925.
[93] AIS transponders . URL: https://www.imo.org/en/OurWork/safety/navigation/ais.
aspx (visited on 05/18/2025).
[94] Ties Emmens et al. â€œThe promises and perils of Automatic Identification System
dataâ€. en. In: Expert Systems with Applications 178 (Sept. 2021), p. 114975. DOI:
10.1016/j.eswa.2021.114975.
[95] Revised Guidelines For The Onboard Operational Use Of Shipborne Automatic
Identification Systems (Ais) . Dec. 2015. URL: https : / / wwwcdn . imo . org /
localresources/en/OurWork/Safety/Documents/AIS/Resolution%20A.1106(29).pdf
(visited on 05/18/2025).
[96] Abbas HaratiÂ­Mokhtari et al. â€œAutomatic Identification System (AIS): Data ReliabilÂ­
ity and Human Error Implicationsâ€. en. In: Journal of Navigation 60.3 (Sept. 2007),
pp. 373â€“389. DOI: 10.1017/s0373463307004298.
[97] Automatic Identification System (AIS) Overview | Navigation Center . URL: https:
/ / www . navcen . uscg . gov / automatic - identification - system - overview (visited on
04/05/2025).
[98] Main Ports (Locations Only) . en. URL: http://marine-analyst.eu/dev.py?N=simple&
O = 570 & titre _ chap = &titre _ page = portlocations & maxlat = 58 & maxlon = 13 . 16 &
minlon=6.94&minlat=54.25&visit=570 (visited on 04/05/2025).
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 136

[99] Thor I. Fossen. â€œKinematicsâ€. en. In: Handbook of Marine Craft Hydrodynamics
and Motion Control . John Wiley & Sons, Ltd, 2011, pp. 15â€“44. ISBN: 978Â­1Â­119Â­
99413Â­8. DOI: 10.1002/9781119994138.ch2.
[100] Sebastian Thrun, Wolfram Burgard, and Dieter Fox. Probabilistic robotics . eng.
The MIT Press, 2006. ISBN: 0Â­262Â­20162Â­3 0Â­262Â­30380Â­9 978Â­0Â­262Â­20162Â­9
978Â­0Â­262Â­30380Â­4.
[101] Diederik P . Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization .
Jan. 2017. DOI: 10.48550/arXiv.1412.6980.
[102] Peter Nicholas Hansen. â€œSituational Awareness for Autonomous Marine Vesselsâ€.
en. Doctoral Thesis. Kongens Lyngby: T echnical University of Denmark, 2023.
[103] Benjamin SanchezÂ­Lengeling et al. â€œA Gentle Introduction to Graph Neural NetÂ­
worksâ€. en. In: Distill 6.9 (Sept. 2021), e33. DOI: 10.23915/distill.00033.
[104] Justin Gilmer et al. Neural Message Passing for Quantum Chemistry . June 2017.
DOI: 10.48550/arXiv.1704.01212.
[105] Weihua Hu et al. Strategies for PreÂ­training Graph Neural Networks . Feb. 2020.
DOI: 10.48550/arXiv.1905.12265.
[106] Petar VeliÄkoviÄ‡ et al. Graph Attention Networks . Feb. 2018. DOI: 10.48550/arXiv.
1710.10903.
[107] Shengyi Huang et al. The 37 Implementation Details of Proximal Policy OptimizaÂ­
tion. T ech. rep. 2022. URL: https : / / iclr - blog - track . github . io / 2022 / 03 / 25 / ppo -
implementation-details/ (visited on 04/15/2025).
[108] John Schulman et al. HighÂ­Dimensional Continuous Control Using Generalized
Advantage Estimation. Oct. 2018. DOI: 10.48550/arXiv.1506.02438.
[109] Aasa Feragen. Lecture notes on explainable AI: Saliency maps . Lecture Notes.
T echnical University of Denmark, 2024.
[110] GINEConv â€” pytorch_geometric documentation . URL: https://pytorch-geometric.
readthedocs . io / en / latest / generated / torch _ geometric . nn . conv . GINEConv . html #
torch_geometric.nn.conv.GINEConv (visited on 06/22/2025).
[111] NNConv â€” pytorch_geometric documentation . URL: https://pytorch- geometric.
readthedocs.io/en/latest/generated/torch_geometric.nn.conv.NNConv.html#torch_
geometric.nn.conv.NNConv (visited on 06/22/2025).
[112] GATConv â€” pytorch_geometric documentation . URL: https://pytorch-geometric.
readthedocs . io / en / latest / generated / torch _ geometric . nn . conv . GATConv . html #
torch_geometric.nn.conv.GATConv (visited on 06/22/2025).
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 137

A AISÂ­Data
Instance Description Data Type Values
Scenario ID Unique scenario identifier string
Start Time Start time of scenario in UTC string
End Time End Time of Scenario in UTC string
Start Epoch Start UTCÂ­timestamp of scenario float
End Epoch End UTCÂ­timestamp of scenario float
Sam. Time Sampling time of vessel state integer 10
Lat. Limits Latitudinal range of scenario (float, float)
Lon. Limits Longitudinal range of scenario ( float, float)
Num. Vessels Number of vessels in scenario integer 1Â­13
T able A.1: Scenario metadata features
Vessel Metadata
Instance Description Data Type Values
MMSI MMSI of vessel int
Ship Type AISÂ­conform ship type string
Width Vessel width in meters float 2.0Â­62.0
Length Vessel length in meters float 8.0Â­446.92
Draught Vessel draught in meters float 0.1Â­16.5
Nav. Status AISÂ­conform nav. status int 0Â­15
Vessel State (Trajectory is list of states)
Instance Description Data Type Values
Time UTCÂ­timestamp of state float
Lat. Latitudinal position of vessel float
Lon. Longitudinal position of vessel float
SOG SOG of vessel in knots float 2.0Â­40.4
COG COG of vessel in degree float 0.0Â­360.0
T able A.2: Vessel trajectory features
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 138

Instance Description Data Type Values
Multiple instances per Seachart
Bounds Scenario bounds in (Lat.,Lon.) polygon
Depth Depth polygon in (Lat.,Lon.) polygon
Depth range in meters (float, float) 0 .0Â­100.0
Route line in (Lat.,Lon.) lineRoute
Route code string
Buoy Buoy point in (Lat.,Lon.) point
Buoy code string
Land Land polygon in (Lat.,Lon.) polygon
T able A.3: Seachart features
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 139

B Observation Parameters
Observation specific
Symbol Parameter Value
mL Number of ShoreÂ­Nodes per Polygon 5
mD Number of DepthÂ­Nodes per km 2 0.1
Scaling
Symbol Parameter Value
Ïhd Vessel draught offset 10 m
Ï‡hd Vessel draught scaling 10 m
Ïu Surge velocity offset 0 kn
Ï‡u Surge velocity scaling 25 kn
Ïd Distance offset 10 km
Ï‡d Distance scaling 10 km
Ïl Vessel length offset 200 m
Ï‡l Vessel length scaling 200 m
Ïw Vessel width offset 50 m
Ï‡w Vessel width scaling 50 m
Ïurel Rel. velocity offset 25 kn
Ï‡urel Rel. velocity scaling 25 kn
Ïhw Depth offset 50 m
Ï‡hw Depth scaling 100 m
minv Minimum of Vessel Type 0
maxv Maximum of Vessel Type 11
mins Minimum of Navigation Status 0
maxs Maximum of Navigation Status 15
T able B.1: Parameters of graphÂ­based observation OGRA
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 140

C Action Parameters
Action specific specific
Symbol Parameter Value
Ïƒ2
N North position variance 50 m
Ïƒ2
E East position variance 50 m
Ïƒ2
u Surge velocity variance 0.01 m sâˆ’1
Ïƒ2
Ïˆ Course variance 0.01Â°
Ïƒ2
Ë™u Acceleration variance 0.3 m sâˆ’2
Ïƒ2
r Yaw rate variance 7.5 Â° sâˆ’1
Scaling
Symbol Parameter Value
Ï‡ Ë™u Acceleration scaling 0.3 m sâˆ’2
Ï‡r Yaw rate scaling 7.5 Â° sâˆ’1
T able C.1: Parameters of accelerationÂ­yaw rate action Akf
Please note, that these parameters correspond to the erroneous version used throughout
the report.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 141

D Scaling functions
Linear scaling
Ë†v = clip
 v âˆ’ Ï
Ï‡ , âˆ’1, 1

(D.1)
where Ï is the offset and Ï‡ is the scaling factor.
MinÂ­Max scaling
Ë†v = clip

2 Â·
 v âˆ’ min
max âˆ’ min

âˆ’ 1, âˆ’1, 1

(D.2)
where max and min are the maximum and minimum values of the feature.
Power scaling
Ë†v = sign

clip
 v
Ï‡ , âˆ’1, 1

Â·

abs

clip
 v
Ï‡ , âˆ’1, 1
0.5
(D.3)
where Ï‡ is the scaling factor.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 142

E Graph layers
This section shall provide further insights into the definitions of the graph layers used in this
report. For a general understanding of GNNs please refer to [ 103]. The implementation
uses pytorchÂ­geometric [ 54].
GINEConv
xâ€²
i = hÎ˜
0
@(1 + Ïµ) Â· xi + +
X
jâˆˆN (i)
ReLU(xj + ej,i)
1
A (E.1)
where xi is the node feature vector, ei,j is the edge feature vector, hÎ˜ is a neural network
and Ïµ is a learnable parameter. The definition is based on [ 110].
NNConv
xâ€²
i = lÎ¸(xi) +
X
jâˆˆN (i)
xj Â· hÎ˜(ei,j) (E.2)
where xi is the node feature vector, ei,j is the edge feature vector, hÎ˜ is a neural network
and lÎ¸ is a linear transformation. The definition is based on [ 111].
GATConv
xâ€²
i = Î±i,i ls(xi) +
X
jâˆˆN (i)
Î±i,j lt(xi) (E.3)
Î±i,j = exp
 
LeakyReLU(aT
s ls(xi) + aT
t lt(xj) + aT
e le(ei,j)

P
kâˆˆN (i)âˆª{i} exp
 
LeakyReLU(aTs ls(xi) + aT
t lt(xk) + aTe le(ei,k)
 (E.4)
where xi is the node feature vector, ei,j is the edge feature vector, ls, lt and le are linear
transformations and as, at and ae are attention weights. The definition is based on [ 112].
ConcatConv
xâ€²
i = lÎ¸(xi) + aggregationjâˆˆN (i)(hÎ˜(concat(ei,j, x j))) (E.5)
where xi is the node feature vector, ei,j is the edge feature vector, hÎ˜ is a neural network,
lÎ¸ is a linear transformation and aggregation is a aggregation function of type max, mean
or sum. This layer is a custom implementation.
Inverse Reinforcement Learning for Decision Making in Autonomous Ship Navigation 143

T echnical
University of
Denmark
Ã˜rsteds Plads, Building 343
2800 Kgs. Lyngby
Tlf. 4525 1700
electro.dtu.dk",
Algorithms for Text Indexing,"Technical University of Denmark
Department of Applied Mathematics and Computer Science, DTU Compute
MASTER THESIS
Exploring Anchored Text Indexing
Andreea Matei
Supervised by:
Philip Bille and Inge Li GÃ¸rtz
June 2025",,,,"6 Algorithm 3
6.1 Data Structure
The composite data structure of algorithm 3 includes again the two augmented compact tries
and akd-tree (withk = 2) over the set of reduced bd-anchors.
The nodes in the tries still store the same satellite data (ranks and ranges) as presented in
algorithms 1 and 2, but with a minor modification in the way the dictionary holds its elements.
Rather than using the entire edge string as the key (as presented in algorithms 1 and 2), the
dictionary now stores only the first character. Furthermore, the values are no longer limited
to only child nodes. Additionally, the values in the dictionary store the remaining edge strings
without their first character. Figure 10 shows this idea.
The 2d-tree remains the same as the one in algorithm 2 from section 5.1.
N1
N2
aici
N3
casa
. . .
. . . . . . . . . . . .
Key V alue
â€˜aâ€™ â€œiciâ€,N2
â€˜câ€™â€œasaâ€,N3
Figure 10: Changes in the dictionary found in the trie nodeN1 for algorithm 3. This serves
as an extension of figure 8 from algorithm 1. For clarity reasons, the satellite data, Rank and
Range, are omitted now in the current figure. The dictionary ofN1 now stores as key the first
characters â€˜aâ€™ and â€˜câ€™ of its edge strings, while the dictionary values store the rest of the edge
labels, â€œiciâ€ and â€œasaâ€, besides the corresponding children,N2 and N3.
6.2 Preprocessing
Preprocessing the data structure of the third algorithm does not change much from the process
presented for algorithm 2 in section 5.2. The only difference lies in the way the compact tries
store internally the edge information.
Once again, it implies computing the reduced bd-anchors and creating thex_array and
y_array by sorting these selected positions. The arrays are then used as input not only to
create thex_trie andy_trie, but also the set of points required for the2d-tree.
6.3 Query
Algorithm 3 queries the data structure by first traversing the triesx_trie andy_trie, respec-
tively, followed by a traversal of the2d-tree.
Querying the compact triesx_trie andy_trie requires following a path from top to bottom
which coincides with the online patternsPx or Py. However, in comparison with algorithms
1 and 2, the next branch to follow in the trie is chosen based on one dictionary lookup for
the first character. This quick retrieval helps determine whether there even exists a (partially)
13"
Locally Consistent Parsing,"1 Introduction and motivation
We collect and store data like never before. New technologies make creating data easier than ever,
while others enable us to save vast quantities of data. But even though the amount of data is rising,
the information it conveys might not be. Consider, for instance, the number of pixels in an average
digital photo today compared to five years ago. Although the image may be sharper and more detailed,
it also contains a lot of repetitive information, like areas of similar colour or texture.
This phenomenon is not limited to images, as the same can be observed with text data. Extensive
collections of text data, such as sensor logs, server records, large version-controlled text files, and bio-
logical databases storing data like DNA, are growing due to more precise, fine-grained data collection
tools and better storage availability. However, much of this increase comes from repetitive, recurring
patterns rather than new information. Sensor logs may include frequent status updates that differ
little over time, version-controlled text might have some parts which are rarely changed, and DNA
datasets often consist of overlapping or repeated sequences.
Recognising this, we can exploit repetition and redundancy to compress data effectively. Instead
of working with massive original datasets, we can identify patterns and encode them, preserving
essential information while significantly reducing the data sizes we are working with. This is the core
idea behind locally consistent parsing. It is a method that enables us to compress data in a way that
allows minimal access to the original dataset, performing most operations on a compact, pattern-based
representation.
1.1.1 Locally consistent parsing
Given a string, a parsing (also called a partitioning) is a way of splitting the string into non-overlapping
blocks. What makes a parsing locally consistent is that two â€œlong enoughâ€ and identical substrings
will be made out of the same blocks, except for â€œborderâ€ elements (placed amongst the leftmost and
rightmost indices of the substrings).
Some of the intuition behind what makes a parsing locally consistent is that it can be used to
construct a new, shorter version of the original string, where each block of the parsing is assigned a
unique symbol, which it shares only with blocks with exactly the same elements. Then, if we want to
compare two substrings of a big string or perform some other string query on it, we can do most of
the work on the new, smaller string created from the blocks because the blocks of the substrings will
(for the most part) be completely identical.
1.1.2 Starting point of the thesis
The foundation of this thesis is four publications by Birenzwige, Golan, and Porat (2020, 5), Kempa
and Kociumaka (2019, 22), Christiansen and Ettienne (2017, 8), and Christiansen, Ettienne, Kociu-
maka, Navarro, and Prezza (2021, 9). They each build a data structure with local properties and all
6

Locally Consistent Parsing, 2025
work towards the goal of a sub-linear working space. The ( Ï„, Î´)-partitioning set is introduced for the
first time in 2018 by Birenzwige et al. [5], later came theÏ„-synchronising set by Kempa and Kociumaka
[22] in 2019. The signature grammar was introduced by Christiansen and Ettienne [8] in 2017 and
revisited in greater detail by Christiansen et al. [9] in 2018.
1.1.3 Related works
Searching for patterns in compressed strings has been studied intensely, but we will in this related works
section restrict ourselves to publications using locally consistent methods that have been published
after the four previously mentioned papers [5, 8, 9, 22].
On the practical side, Dinklage, Fischer, Herlez, Kociumaka, and Kurpicz (2020, 13) evaluates
the practical performance of synchronising sets [22] and (and indirectly partitioning sets [5]) for LCE
queries, showing that these theoretically motivated structures outperform naive approaches even for
moderate-length queries on real-world repetitive data. Claude, Navarro, and Pacheco (2020, 10)
propose a grammar-compressed self-index with space O(G log n) bits, where G is the size of the
grammar, and search time O((m2 + occ) logG). They also implemented their results in practice and
tested them in highly repetitive text collections.
Another development has been made by Ayad, Loukides, and Pissis (2024, 1), who introduce
the concept of sample-based locally consistent anchors . They use them for efficient text indexing
when the queried patterns exceed a known lower bound in length. These locally consistent anchors
have average-case guarantees, and the contribution also offers a practical implementation of the data
structure.
Kociumaka, Navarro, and Prezza (2020, 24) propose a unified framework for measuring repetitive-
ness, Î´ â‰¤ Î³ (where Î³ is the size of the smallest attractor), in strings by introducing and relating several
compression measures, including attractors, LZ77, and grammar-based size, thereby offering a basis
for comparing compressed indexes. Using this Î´ and combining two distinct locally consistent parsing
techniques1 Kociumaka, Navarro, and Olivares (2022, 25) obtain search time O(m + (occ + 1) logÎµ n)
in tight attractor-bounded space O(Î´ log(n/Î´)).
Kempa and Kociumaka (2022, 23) solves the dynamic suffix array problem using a new type of
dynamic locally consistent parsing. They also offer a dynamic construction of string synchronising
sets [22]. Lastly, the survey by Navarro (2021, 26) is worth mentioning as it provides a comprehensive
portrayal of compressed text indexing. However, his focus is largely on classical data structures and
does not focus very much on recent developments in locally consistent parsing.
1.1.4 The organisation of the thesis
In the remaining of Chapter 1, we introduce the notation and algorithmic concepts used throughout
the thesis.
Chapter 2 examine the partitioning of a string such that it has some local properties. We will look
at a general index-based locally consistent partitioning called ( Î±, Î², Ï„, Ï)-locally consistent set. This
partitioning is a generalisation of two other partitionings: (Ï„, Î´)-partitioning set [5] and Ï„-synchronising
set [22]. The idea of keeping local properties has also been introduced in a slightly different setting,
namely in the signature grammar by Christiansen and Ettienne [8]. In their paper [8], a grammar with
local properties is created, and the construction strongly resembles that of the ( Ï„, Î´)-partitioning set.
We will show how the construction from the signature grammar can also be turned into partitioning
and how it behaves in relation to the other partitionings.
In Chapter 3, two different forms of locality are introduced. In order to highlight the parallels and
differences between the different partitionings. The two localities are index-based and block-based lo-
cality. These different locality concepts are applied to the partitioningsâ€™ (and their constructions), and
1Based on the grammar introduced by Christiansen et al. [9] and another type of locally consistent parsing called
recompression by Jez (2014, 20).
7

Locally Consistent Parsing, 2025
in Chapter 4 we uncover the relationship between the ( Ï„, Î´)-partitioning set and the Ï„-synchronising
set.
In Chapter 5, we look at the signature grammar and turning the construction of a partitioning
set into a grammar construction. This grammar is investigated. Chapter 6 focuses on some of the
problems local data structures can solve and uses their runtimes to argue for similarities between the
different locally consistent data structures.
Chapter 7 contains the final remarks and is followed by the appendix and the references.
1.1.5 The contribution of this thesis
This thesis introduces a more general construction, the ( Î±, Î², Ï„, Ï)-locally consistent set, and uses it
to show the similarities between the ( Ï„, Î´)-partitioning set and the Ï„-synchronising set.
Furthermore, the two concepts of locality, index-based and block-based locality, are introduced to
compare the different locally consistent partitionings and grammars. Moreover, this thesis shows that
when working with non-periodic strings, a partitioning and synchronising set can be reduced to one
another. In the general case, there exists a reduction from a Ï„-synchronising set to a (Ï„, Ï„ )-partitioning
set.
Lastly, in the problem section, this thesis shows an example of a pattern occurrence, which could
be missed by the string indexing query by Christiansen and Ettienne [8]. We propose a small alteration
that does not influence the queryâ€™s complexity and we prove that with this change the query solves
the text indexing problem in sub-linear space.
1.2 Notation, definitions, and general concepts
In this section, we introduce the general notation and concepts used throughout the thesis.
We operate within the standard word RAM model, where a machine word has size w â‰¥ log n, with
n = |S|. We denote the set of integers {1, . . . , n} by [ n]. We consider strings over a finite ordered
alphabet Î£ = [0 . . . Ïƒ âˆ’ 1], where the alphabet size Ïƒ satisfies Ïƒ = nO(1). A string S of length n is a
sequence of symbols from Î£, that is, S = S[1] . . . S[n] âˆˆ Î£n.
S[i . . . j] denotes the substring of S that starts at index i and ends at index j (both inclusive),
while S[i . . . j) denotes the substring starting at i and ending at j âˆ’ 1. We define the prefix of a string
S of length â„“ as the substring S[1 . . . â„“], and the suffix of length â„“ as the substring S[n âˆ’ â„“ + 1 . . . n].
An integer Ï âˆˆ [1 . . . |S|] is called a period of the string S if S[i] = S[i + Ï] for all i âˆˆ [1 . . . |S| âˆ’ Ï].
The smallest such Ï is called the principal period of S, and is denoted by per( S). For a substring to
be periodic, its principal period must occur at least twice. A substring Sâ€² of S is called a ( d,Ï)-run if
|Sâ€²| â‰¥ d and per(Sâ€²) â‰¤ Ï.
Definition 1. The succeeding element of element i in a set X is denoted by succ X (i).
We introduce the Karp-Rabin fingerprint, a random rolling hash function. It has the property
that the fingerprints of all the prefixes of a string can be precomputed in O (n), and afterwards the
fingerprint of any substring can be computed in constant time. Using a definition by Bille et al. [4]
and the result introduced by Karp and Rabin [21] we get the following definition:
Definition 2. Let S be a length n string, p is a prime number, and r âˆˆ Zp. Then we can define
the Karp-Rabin fingerprint function is defined in the following way
Ï•(S) =
nX
i=1
S[i] Â· riâˆ’1 mod p.
8

Locally Consistent Parsing, 2025
1.2.1 The id-function
In the randomised constructions of both the ( Ï„, Î´)-partitioning set and Ï„-synchronising set, the id-
function is used to determine which indices should be in the partitionings. It is a function which maps
a substring to a positive integer. We will define this function by the properties it needs to have, and
then briefly introduce how such a function can be implemented in practice.
Definition 3 (The id-function). Given a string S of length n, then id : [1 . . . nâˆ’ Ï„ + 1] â†’ R The
following property defines the id-function:
â€¢ id(i) = id(j) if and only if S[i . . . i+ Ï„) = S[j . . . j + Ï„).
â€¢ Given a length Ï„ interval [i . . . i + Ï„) where S[i . . . i + Ï„) contains no ( Ï„,Ï„)-runs then the
probability that any element in [ i . . . i+ Ï„) has minimal id-value is O
  1
Ï„

. An index j âˆˆ
[i . . . i+ Ï„] has minimal value if id(j) is smaller than id(k) for any k âˆˆ [i . . . i+ Ï„] where
k Ì¸= j.
â€¢ Computing id(i) for all i âˆˆ [1 . . . n] in a string of length n takes O (n).
One way to implement the id-function in practice is by using a fingerprint function, like the Karp-
Rabin function Ï•, and afterwards hashing the fingerprint.
Birenzwige et al. [5] hashes their fingerprint with a min-wise hash-function h from a family of  1
2 , Ï„

-min-wise hash functions. This family of hash functions has the property that, given a set of
fingerprints X (a subset of all possible fingerprints) with |X | < Ï„ , the behaviour of hash values can be
characterized probabilistically. Specifically, the probability that any element x /âˆˆ X has a hash value
smaller than any value in the interval is 3/2
|X |+1.
This is a brief explanation on one possible way to create an id function with the properties of
Definition 3, and it is more thoroughly described in section 2 (preliminaries) of [5]. The following
definition is used for comparing the similarity of two substringsâ€™ parsing.
Definition 4 (BX (i, j)). Let S be a string and let X be a set of indices of this string. Then
BX (i, j) = {x âˆ’ i : i â‰¤ x â‰¤ j and x âˆˆ X }. The set BX (i, j) is the set of indices of a substring
S[i . . . j] which are part of X and then subtracted by the first index of the substring.
An example of the set BX (i, j) is illustrated in Figure 1.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
S[9 . . .20]
2 4 5 7 11
BX (9, 20)
Figure 1: An example with a string S of length 32. Every cell indicates a character, and
all the marked cells have indices which are elements of the set X . In this case X =
{3, 6, 8, 11, 13, 14, 16, 20, 22, 24, 26, 27, 30}. The darker cells are the elements of X which are also el-
ements of the interval [9 . . .20]. The set BX (9, 20) = {2, 4, 5, 7, 11} consists of the distance between
the indices of those elements and the beginning of the substring we are looking at. In this case, the
distance between the indices {11, 13, 14, 16, 20} and index 9.
9

Locally Consistent Parsing, 2025
1.2.2 Tries
This brief introduction to tries is inspired by the lecture notes on the subject of GÃ¸rtz [17]. Given
a collection of strings S = {S1, . . . , Ss} over an alphabet Î£ of size Ïƒ, a trie is a rooted, ordered tree
that represents all strings in S such that each root-to-leaf path corresponds to one of the strings in
the collection. Common prefixes are shared among the paths, and edges from each node are labelled
by distinct characters and sorted alphabetically.
A pattern of length m can be searched for in the trie by traversing the pattern character by
character. This search takes O (m) time if the alphabet size is constant. If the alphabet is large, a
balanced binary search tree or hash table can be used at each node to guide the search. In this thesis
we will use a trie to do a deterministic search and therefore we only need the following Lemma on
search using a balance binary tree.
Lemma 1. Given a trie over a collection of strings and a balanced binary search tree at each
node, a pattern of length m can be searched in O (m log Ïƒ) time, where Ïƒ is the alphabet size.
If we were using hash tables to store edges at each node (with expected constant-time access),
then the expected time to search for a pattern of length m is O (m).
1.2.3 Lempel-Ziv 77
This description of the LZ77 parse is based on the one made by Bille, Ettienne, GÃ¸rtz, and VildhÃ¸j (2018, 4).
The Lempel-Ziv77 parse is a compression algorithm from 1977, introduced by Ziv and Lem-
pel (1977, 29), and it plays a vital role in the field of string compression as well as in this thesis.
It is a way of compressing a text using repetition in the string. An LZ77 parse is a partition of a
string, S, of length n, into a sequence Z of z succeeding substrings, called phrases. We say that
S = Z[1]Z[2] . . . Z[z]. Every phrase is usually depicted as a tuple with a start, a length, and a border
symbol, c.
For the ith phrase, Z[i] we will write the tuple as ( si, li, ci), where si is the start index of the
phraseâ€™s source, li is the length of the source, and ci is the border position of Z[i]. The string S can
in this way be described by ( s1, l1, c1) . . .(sz, lz, cz) âˆˆ ([n], [n], Î£)z.
The following provides an informal overview of the encoding process. Imagine that we have already
partitioned S[1 . . . j] into phrases Z[1] . . . Z[i]. Let jâ€² be defined such that S[j+1 . . . jâ€²âˆ’1] is the longest
prefix of the substring S[j + 1 . . . n] that is also a substring of S[1 . . . jâ€² âˆ’ 2]2. Then si+1 will be the
index of the substring in S[1 . . . jâ€² âˆ’2], and li = jâ€² âˆ’1âˆ’j +1 = jâ€² âˆ’j is the length of the substring. The
border point is S[jâ€²]. Then the ( i+1)th phrase describes S[j +1 . . . jâ€²] and Z[i+1] = ( si, jâ€² âˆ’ j, S[jâ€²]).
1.2.4 Weak prefix search with z-fast tries
Moving forward, we discuss the Weak Prefix Search Problem , which will be visited in Chapter 6,
when doing pattern matching using a locally consistent grammar. The problem is defined in the
following way
The Weak Prefix Problem
Input: A lexicographically sorted set of k strings denoted D. A pattern P of length m.
Query: Report the ranks of those strings in D of which P is a prefix. If there is no strings are
prefixes of the pattern, then return arbitrary (and incorrect) ranks.
The weak prefix search is as the name implies a weaker version of prefix search in the sense that if no
string with the pattern, we are searching for, the search might return something completely arbitrary.
2Note that the interval only goes to jâ€² âˆ’ 2 because the source need to be place in the interval [1 . . . j].
10

Locally Consistent Parsing, 2025
In this thesis, we will use a data structure, denoted a z-fast trie, which is described in the following
Lemma by Djamal Belazzougui and Vigna (2010,[14, Appendix H.3]).
Lemma 2 (Djamal Belazzougui and Vigna [14],Appendix H.3). Given a set D of k strings with av-
erage length l, from an alphabet of size Ïƒ, we can build a data structure using O (k(log l + log logÏƒ))
bits of space supporting weak prefix search for a pattern P of length m in O (m log Ïƒ/w + log m)
time where w is the word size.
When looking something up in the z-fast trie, we must hash it with the Rabin-Karp fingerprint
function and then do the query, because the data structure contains Rabin-Karp fingerprints rather
than full-length substrings.
1.2.5 2D range reporting
The 2D range reporting problem is defined as follows
2D Range Reporting
Input: A set P of k points in R2.
Query: Given a query rectangle Q = [x1, x2] Ã— [y1, y2], report all points in P âˆ© Q.
There are standard solutions solving this problem in O (log n + occ) time and O (n log n) space, and
another one in O (âˆšn + occ) time and O (n) space (covered by Schulz [27] using the results of [3, 7,
12, 28]).
Chan, Larsen, and PË‡ atraÂ¸ scu (2011, 6) are responsible for the complexity results used for 2D
range reporting in this thesis, which Christiansen and Ettienne [8] use to get a data structure of size
O (z log(n/z)) and with a query time of O (logÏµ(z log(n/z))) [8, 9].
1.2.6 Distributed 6-colouring a path in O (logâˆ— n)
The algorithm, which in this thesis is referred to as the colouring algorithm, is a version of the Cole-
Vishkin algorithm [11].
Introduced in 1986, the Cole-Vishkin algorithm can be used to compute a proper colouring of
a path using only 6 colours in O(logâˆ— n) rounds in the LOCAL model. It starts from unique node
identifiers and iteratively compresses colours using bitwise string operations. In each round, a node
gets a new colour on the index of the first bit that differs from the colour of its successor, ensuring
that adjacent nodes receive different colours. This process rapidly reduces the colour space from 2 128
(or larger) to 6, after which a simple greedy reduction yields a proper 3-colouring if needed.
In this thesis, a very similar algorithm is used to transform a string over a large alphabet Î£
into a string over a constant-size alphabet of size 6. The character at each position i becomes the
label (or colour) of i, and the transformation preserves local uniqueness. If each character is distinct
from its immediate neighbours in the input, then the corresponding labels will also be distinct. The
transformation runs in c logâˆ— n rounds for a string of length n, and is local; that is, the label at index
i depends only on the original characters in the c logâˆ— n positions to the right of i.
While the original Cole-Vishkin algorithm is defined in the LOCAL model (a model used when
working with distributed computing), the transformation used in this thesis also works naturally in
the word RAM model. Since the label at position i depends only on a window of c logâˆ— n characters
to the right, the transformation can be implemented sequentially in linear time by scanning the input
from right to left, maintaining a sliding window of size O(logâˆ— n). This locality ensures that the core
structure of the algorithm remains efficient even outside the distributed setting.
11",,,,"Locally Consistent Parsing, 2025
6.4.3 Complexity
The queries on the z-fast tries T1 and T2 takes O (log m) time [14]. A query on the 2D-range structure
R takes O (logÏµ(z log(n/z))) this was corrected by Christiansen et al. [9] because Christiansen and
Ettienne [8] claimed that it was only O (logÏµ z). O (log m) queries are done, since P âˆ—
S is size O (log m)3.
Verification of the O (log m) weak prefix queries are done in O
 
log2 m + m

time ([8] by citing Gagie
et al. [16]). Building the signature DAG of P takes O (m). The runtime with reporting is therefore:
O (m + (occ + 1)(log m Â· (log m + logÏµ(z log(n/z))) = O
 
m + (1 + occ) log2 m Â· logÏµ(z log(n/z)

.
Christiansen and Ettienne [8] bound reduces the expression further by differentiating between m â‰¤
log2Ïµ z and m > log2Ïµ z. They show that it is possible to reduce the runtime expression to:
O(m + (1 + occ) logÏµâ€²
(z log(n/z)) where Ïµâ€² > Ïµ.
Let us now examine the space of the data structure. We showed in Lemma 18 that the size of
dag(S) is O (z log(n/z)), and this is also the number of nodes in the DAG. Therefore and because of
Lemma 2 is the size of T1 and T2 at most O (z log(n/z)) (this is elaborated in [8]). Lastly, the 2D-range
data structure uses space linearly in the number of points it contains, and since we store a constant
number of points (on average) per vertex in dag(S), the size of this structure is also O (z log(n/z)).
In total, this data structure uses O (z log(n/z)) space.
6.4.4 Queries for short and semi patterns
The solution for short and the one for semi-short patterns found by Christiansen and Ettienne [8]
are based on the LZ77-parse of S. This results in a runtime of O (m + occ) for short patterns and
O (m + occ(log logn + logÏµ z))) for semi-short patterns, with z being the size of the LZ77 parse of S
and Ïµ is any positive constant < 1
2. Their overall result (independent of the pattern size) is that the
runtime with reporting is O (m + logÏµ(z log(n/z)) + occ(log logn + logÏµ z)) [8, Theorem 2(3)] (revised
by Christiansen et al. [9]).
3The signature grammar has height O (log m) and we only include a constant number of position in P âˆ—
S per level.
61

Chapter 7
Conclusion and future work
7.1 Conclusion on the partitionings
Throughout this thesis, we have drawn many parallels between the ( Ï„, Î´)-partitioning set and the Ï„-
synchronising set. They have very similar randomised constructions, they can both be defined as locally
consistent sets (Definition 5), when working with a string, which has no periodic behaviour. They
can be reduced to each other, and lastly they solve The Longest Common Extension Problem
using a similar setup and query with the same runtimes.
The ( Ï„, Î´)-partitioning set and the Ï„-synchronising set have so much in common that they only
differentiate from each other on the subject of how they treat highly periodic behaviour.
In addition, we have Ï„-local minimum set, which only has expected index-based locality. It can-
not compete with the two other partitionings when solving, for instance, The Longest Common
Extension Problem , but since this thesis is only theoretical, it might work well in practice.
7.2 Conclusion on the grammars
In Chapter 5 we saw that the signature grammar and the grammar derived from the deterministic
(O (Ï„) , O (Ï„ logâˆ— n))-partitioning set have very similar constructions. Despite this fact and the fact
that some properties (like their height) are common to both grammars, the derived grammar does not
prove to be locally consistent (at least not according to Definition 11). This implies that the derived
grammar will not be useful for solving The String Indexing Problem , at least not using the
methods presented in Chapter 6. It might be useful for solving other problems, but this would require
further investigation into these problems and into the compression rate of this derived grammar.
7.3 Conclusions on locality
Having examined the two kinds of locality, index-based and block-based, we observe that each serves
different purposes and are useful in different settings. Index-based locality allows us to work primarily
on the compressed structure, requiring only a limited number of character lookups near the boundaries
of a query. This is particularly useful when storing the entire string externally, and accessing arbitrary
characters is computationally costly.
In contrast, block-based locality enables pattern matching directly on a compressed representa-
tion, even without access to the original string. Although the properties of block-based locality may
appear more subtle or less intuitive than those of index-based locality, they are crucial to the proof of
correctness for the signature grammar pattern matching algorithm, as illustrated in Lemma 25.
62

Locally Consistent Parsing, 2025
7.4 Future work
There are other interesting areas within locally consistent parsing, which are not covered by this thesis,
some of them are:
â€¢ Is there something a Ï„-synchronising set can do that a (Ï„, Î´)-partitioning set cannot?
In this thesis and the literature of the two partitionings, it is still unclear whether the more
strict density condition of the Ï„-synchronising set makes it capable of solving some problems
more efficiently than a partitioning set. The example studied here, The Longest Common
Extension Problem , is solved just as well by a partitioning set, and the same goes for the
Sparse Suffix Tree (which is not described in this work but has been solved by both Biren-
zwige et al. [5] and Kempa and Kociumaka [22] using their respective partitionings). It could
be interesting to try to locate some (maybe more advanced) problems that would get distinctive
performance from the two partitionings.
â€¢ An even more unifying theory. This work has not been possible to make a theory that
unites the locally consistent sets (or grammars) based on different locality measures. It could
be interesting to continue working towards a more general description containing both locality
forms.
â€¢ Looking into other compression measures than the Lempel-Ziv77 parse. This is not
necessarily a new research topic, but Christiansen et al. [9] measured the performance of their
grammar (which is almost identical to the signature grammar they introduced earlier [8]) using
attractors. It could be very interesting to look more into those and how the compression rate of
a locally consistent grammar relates to this concept.
â€¢ Investigation of the grammar derived from the deterministic partitioning set con-
struction. It could also be very interesting to dive further into the properties of the grammar
derived from Construction 2. Studying both the compression rate of the grammar and the prob-
lems, it could be used to provide even better grounds for a comparison between this grammar
and the signature grammar of Christiansen and Ettienne [8].
7.5 Concluding thoughts
This thesis aims to map out the differences and parallels of the different types of locally consistent
parsing and to emphasise the use case and upsides of using locally consistent structures.
In a world of ever-increasing data collection and a growing need to efficiently identify patterns
within such data, the potential of locally consistent parsing seems substantial.
63

Appendix and References
A Appendix
A.1 Explaining Figure 12
1. B5 moves up a level: B5 is of type 1 and is therefore moved to the next level.
2. B5 does not moves up a level: B5 is of type 2 (part of a periodic sequence) but is not the first
block.
3. B5 moves up a level: B5 is the first block in a type 2 sequence ( B4 is of type 1).
4. B5 moves up a level: B5 is the first block in a type 2 sequence.
5. B5 does not moves up a level: B5 is the last block of a type 2 sequence.
6. B5 moves up a level: B5 is the right neighbour of a type 1 block.
7. B5 moves up a level: B4 is the last block in a block 2 sequence thus B5 moves up.
8. B5 does not moves up a level: B4 is the first block after a sequence of type 2 blocks which means
B5 does not move up.
9. B5 does not moves up a level: B3 is the last block in a sequence of type 2 blocks. This means
B4 is the first block after a sequence of type 2 blocks which means B5 does not move up.
10. B5 moves up a level: B4, B5, and B6 are all blocks of type 3, and the label of B5 is the smallest
of the three. Therefore B5 moves up.
11. B5 does not moves up a level: B4, B5, and B6 are all blocks of type 3, and the label of B5 is not
the smallest of the three. Therefore B5 does not move up.
12. B5 does not moves up a level: B5 is the last block of type 3 with a complete label list. Therefore
can the final label of B5 and B6 not be compared and B6 is part of the c logâˆ— n last blocks. Thus
B5 does not move up.
13. B5 moves up a level: B5 is the first of the c logâˆ— n last blocks of a sequence if type 3 (which is
treated like a sequence of type 4). Therefore it is moved up.
14. B5 does not moves up a level: B4 is the first of the c logâˆ— n last blocks, which means B5 should
not be moved up.
15. B5 moves up a level: B5 is neither the first nor the second block in of of the c logâˆ— n last blocks,
|L5| â‰¡ 0 mod 2, and it is not the last (because |L5| â‰¡ 0 mod 2) thus it is moved to the next
level.
16. B5 moves up a level: B5 is the first of the c logâˆ— n last blocks of a sequence if type 3 (which is
treated like a sequence of type 4). Therefore it is moved up.
64

Locally Consistent Parsing, 2025
17. B5 does not moves up a level: B5 is not the first of the c logâˆ— n last blocks of a sequence if type
3 (which is treated like a sequence of type 4) and |L5| Ì¸â‰¡ 0 mod 2 therefore it is not moved up.
65

Locally Consistent Parsing, 2025
IsL5 =âˆ…?
IsL4=âˆ…?
Is|B4|=|B5|? IsL3=âˆ…?
IsL2=âˆ…and is|B2|=|B3|?
Is|L5| â‰¥c Â·logâˆ—n?
Is|L6| â‰¥c Â·logâˆ—n?
Is the value ofB5a minimum
compared toB4andB6?
Is|B3|=|B4|?
IsB5| â‰¥(3/2)Âµ+1?
IsL4=âˆ…?
Is|B4|=|B5|?
B5do not move
up to the next level
B5moves up to
the next level
B5moves up to
the next level
B5moves up to
the next level
B5moves up to
the next level
B5do not move
up to the next level
B5moves up to
the next level
B5do not move
up to the next level
B5do not move
up to the next level
B5moves up to
the next level B5do not move
up to the next level
B5do not move
up to the next level
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes
No
No
No
No
No
No
No
No
No
Yes
No
B1
Current block
No
Is|L3| â‰¥c Â·logâˆ—n?
Is|L5| â‰¡0 mod 2?
Is|L4| â‰¥c Â·logâˆ—n?
Yes
B5moves up to
the next level
Yes
No
No
No
B5do not move
up to the next level
Is|L4| â‰¥c Â·logâˆ—n?
B5moves up to
the next level B5do not move
up to the next level
B5moves up to
the next level
Yes
Yes
No
No
B2 B3 B4 B5 B6
1
2
3
4
5
6
7 8
9
10 11
12
13
14
15
16
17
66

B References
[1] Lorraine A. K. Ayad, Grigorios Loukides, and Solon P. Pissis. Text indexing for long patterns
using locally consistent anchors. Arxiv (cornell University) , 2024. doi: 10.48550/arXiv.2407.
11819.
[2] Maxim Babenko, Pawe l Gawrychowski, Tomasz Kociumaka, and Tatiana Starikovskaya. Wavelet
trees meet suffix trees. Proceedings of the Twenty-sixth Annual Acm-siam Symposium on Discrete
Algorithms, pages 572â€“591, 2015. doi: 10.5555/2722129.2722168.
[3] J. L. Bently and D. F. Stanat. Analysis of range searches in quad trees. Information Processing
Letters, 3(6):170â€“173, 1975. ISSN 00200190, 18726119. doi: 10.1016/0020-0190(75)90034-4.
[4] Philip Bille, Mikko Berggren Ettienne, Inge Li GÃ¸rtz, and Hjalte Wedel VildhÃ¸j. Timeâ€“space
trade-offs for lempelâ€“ziv compressed indexing.Theoretical Computer Science, 713:66â€“77, February
2018. ISSN 0304-3975. doi: 10.1016/j.tcs.2017.12.021. URL http://dx.doi.org/10.1016/j.
tcs.2017.12.021.
[5] Or Birenzwige, Shay Golan, and Ely Porat. Locally consistent parsing for text indexing in small
space. Proceedings of the Annual Acm-siam Symposium on Discrete Algorithms , 2020-:607â€“626,
2020.
[6] Timothy M. Chan, Kasper Green Larsen, and Mihai PË‡ atraÂ¸ scu. Orthogonal range searching on
the ram, revisited. Proceedings of the Annual Symposium on Computational Geometry , pages
1â€“10, 2011. doi: 10.1145/1998196.1998198.
[7] Bernard Chazelle and Leonidas J. Guibas. Fractional cascading: I. a data structuring technique.
Algorithmica, 1(1-4):133â€“162, 1986. ISSN 01784617, 14320541. doi: 10.1007/BF01840440.
[8] Anders Roy Christiansen and Mikko Berggren Ettienne. Compressed indexing with signature
grammars, 2017.
[9] Anders Roy Christiansen, Mikko Berggren Ettienne, Tomasz Kociumaka, Gonzalo Navarro, and
Nicola Prezza. Optimal-time dictionary-compressed indexes. Acm Transactions on Algorithms ,
17(1):3426473, 2021. ISSN 15496333, 15496325. doi: 10.1145/3426473.
[10] Francisco Claude, Gonzalo Navarro, and Alejandro Pacheco. Grammar-compressed indexes with
logarithmic search time. 2020.
[11] R Cole and U Vishkin. Deterministic coin tossing with applications to optimal parallel list ranking
universal algorithm for sequential data compression. Information and Control, 70(1):32â€“53, 1986.
ISSN 18782981, 00199958. doi: 10.1016/S0019-9958(86)80023-7.
[12] Mark De Berg, Otfried Cheong, Marc Van Kreveld, and Mark Overmars.Computational geometry:
Algorithms and applications . Springer Berlin Heidelberg, 2008. ISBN 3540779736, 3540779744,
3642096816, 9783540779735, 9783540779742, 9783642096815. doi: 10.1007/978-3-540-77974-2.
[13] Patrick Dinklage, Johannes Fischer, Alexander Herlez, Tomasz Kociumaka, and Florian Kurpicz.
Practical performance of space efficient data structures for longest common extensions. Leibniz
International Proceedings in Informatics, Lipics , 173:39, 2020. ISSN 18688969. doi: 10.4230/
LIPIcs.ESA.2020.39.
[14] Rasmus Pagh Djamal Belazzougui, Paolo Boldi and Sebastiano Vigna. Fast prefix search in little
space, with applications. volume 6346 of LNCS, pages 427â€“438. Springer Berlin Heidelberg, 2010.
(Appendix H.3 can be found at http://www.itu.dk/people/pagh/papers/prefix.pdf).
67

Locally Consistent Parsing, 2025
[15] Paolo Ferragina and Roberto Grossi. Improved dynamic text indexing. Journal of Algorithms ,
31(2):291â€“319, 1999. ISSN 10902678, 01966774. doi: 10.1006/jagm.1998.0999.
[16] Travis Gagie, Pawe l Gawrychowski, Juha KÂ¨ arkkÂ¨ ainen, Yakov Nekrich, and Simon J. Puglisi. Lz77-
based self-indexing with faster pattern matching. In Alberto Pardo and Alfredo Viola, editors,
LATIN 2014: Theoretical Informatics , pages 731â€“742, Berlin, Heidelberg, 2014. Springer Berlin
Heidelberg. ISBN 978-3-642-54423-1.
[17] Inge Li GÃ¸rtz. Tries and suffix trees. https://www2.compute.dtu.dk/courses/02105/
tries-and-suffix-trees.pdf, n.d. Lecture notes, Technical University of Denmark.
[18] J. Ian Munro, Yakov Nekrich, and Jeffrey S. Vitter. Fast construction of wavelet trees.Theoretical
Computer Science, 638:91â€“97, 2016. ISSN 18792294, 03043975. doi: 10.1016/j.tcs.2015.11.011.
[19] Guy Jacobson. Space-efficient static trees and graphs. Annual Symposium on Foundations of
Computer Science (proceedings), pages 549â€“554, 1989. ISSN 02725428.
[20] Artur Jez. A really simple approximation of smallest grammar. Combinatorial Pattern Matching,
Cpm 2014, 8486:182â€“191, 2014. ISSN 16113349, 03029743.
[21] Richard M. Karp and Michael O. Rabin. efficient randomized pattern-matching algorithms. Ibm
Journal of Research and Development , 31(2):249â€“260, 1987. ISSN 21518556, 00188646. doi:
10.1147/rd.312.0249.
[22] Dominik Kempa and Tomasz Kociumaka. String synchronizing sets: sublinear-time bwt con-
struction and optimal lce data structure. In Proceedings of the 51st Annual ACM SIGACT
Symposium on Theory of Computing , STOC â€™19, page 756â€“767. ACM, June 2019. doi:
10.1145/3313276.3316368. URL http://dx.doi.org/10.1145/3313276.3316368.
[23] Dominik Kempa and Tomasz Kociumaka. Dynamic suffix array with polylogarithmic queries and
updates. Proceedings of the Annual Acm Symposium on Theory of Computing , pages 1657â€“1670,
2022. ISSN 07378017. doi: 10.1145/3519935.3520061.
[24] Tomasz Kociumaka, Gonzalo Navarro, and Nicola Prezza. Towards a definitive measure of repet-
itiveness. Lecture Notes in Computer Science (including Subseries Lecture Notes in Artificial In-
telligence and Lecture Notes in Bioinformatics) , 12118:207â€“219, 2020. ISSN 16113349, 03029743.
doi: 10.1007/978-3-030-61792-9 17.
[25] Tomasz Kociumaka, Gonzalo Navarro, and Francisco Olivares. Near-optimal search time in Î´-
optimal space. Lecture Notes in Computer Science (including Subseries Lecture Notes in Artificial
Intelligence and Lecture Notes in Bioinformatics), 13568:88â€“103, 2022. ISSN 16113349, 03029743.
doi: 10.1007/978-3-031-20624-5 6.
[26] Gonzalo Navarro. Indexing highly repetitive string collections, part ii. Acm Computing Surveys ,
54(2):26, 2021. ISSN 15577341, 03600300. doi: 10.1145/3432999.
[27] AndrÂ´ e Schulz. 6.851: Advanced data structures â€“ lecture 3: Range reporting. https:
//courses.csail.mit.edu/6.851/spring10/, February 2010. Scribed by Jacob Steinhardt and
Greg Brockman.
[28] R WILBER. Lower bounds for accessing binary search-trees with rotations. Siam Journal on
Computing, 18(1):56â€“67, 1989. ISSN 10957111, 00975397. doi: 10.1137/0218004.
[29] J Ziv and A Lempel. Universal algorithm for sequential data compression. Ieee Transactions
on Information Theory , 23(3):337â€“343, 1977. ISSN 15579654, 00189448. doi: 10.1109/TIT.1977.
1055714.
68"
Parallel Compression Algorithms,"2 Introduction
Lossless compression algorithms are algorithms designed to take a large input and compress (shrink)
it in a way where the original input can be restored from just the compressed data. This is in contrary
to lossy compression algorithms, which allows discarding less significant data in order to compress
the data more, but in turn only manages to restore an approximation on the original input.
A common property of lossless compression algorithms is that it takes much longer to compress
data than it takes to decompress it. This is because, roughly speaking, when we compress data we
need to search for patterns in the data (e.g. a sentence in a book repeating multiple times) but
when we decompress data we are given a list of patterns that we just need to execute in reverse (e.g.
replace each instance of a token with the sentence that it represents), which is much faster.
We therefore started looking for ways to speed up the compression process by trying to parallelise
the compression part of a compression algorithm. An easy way to parallelise any compression
algorithm is to split the input data into parts, and compress each of them individually. This, however,
may result in worse compression, since we no longer have a way to share information across the
parts of the input data. It is also technically a different compression algorithm, since the output of
running the algorithm on the entire input data, and on parts of the input data are different. We
therefore specifically wanted to find an parallelisation of a compression algorithm, which returns the
same output as the original algorithm.
In (Shun & Zhao, 2013b) they found a parallelisation of the Lempel-Ziv-77 (LZ77) algorithm,
which achieved high levels of speed-up when run on a multi-core Central Processing Unit (CPU).
One of the steps in that algorithm is to run a parallel all nearest smaller values algorithm (ANSV)
along with answering range minimum queries (RMQ). In this thesis, we focus on improving these
two steps.
For the ANSV problem, they cite (Berkman et al., 1993) which presents an algorithm that can
parallelise on up to n
lg(n) threads. A CPU, however, only has relatively few cores, meaning that it can
only execute a few threads at a time. In contrary, a Graphics Processing Unit (GPU) is specifically
designed to run highly parallelised calculations on many cores,but in return it suffers from limitations
that means that it cannot execute regular algorithms. We therefore focus on converting the algorithm
from (Berkman et al., 1993) into one that can run on a GPU.
(Berkman et al., 1993) also mentions an algorithm for answering RMQ queries in constant time,
using data precomputed from the output of the ANSV algorithm. We therefore decided to also
convert this algorithm to a version capable of running on a GPU.
Page 6 of 57

s184009 25th June 2025
Part I
All Nearest Smaller Values
3 Defining the problem and relevant variables
Before we start explaining the algorithm, we first need to define the problem.
Definition 3.1.Given an array of integersA, find the following for all elements inA:
â€¢ For the element at indexk, find the element at indexi which satisfies, i < k , A[i] < A [k],
âˆ€â„“âˆˆN, i < â„“ < k : A[â„“] â‰¥A [k]. This is referred to as the â€œleft nearest smaller valueâ€, or just
the â€œleft matchâ€.
â€¢ For the element at indexk, find the element at indexj which satisfies, j > k , A[j] < A [k],
âˆ€â„“âˆˆN, k < â„“ < j : A[â„“] â‰¥A [k]. This is referred to as the â€œright nearest smaller valueâ€, or just
the â€œright matchâ€.
It is possible for an element to be missing its left-, right-, or both matches
The other thing we need to explain is some variable names and terms that we will be using in
order to make the explanations more readable. These variable names and terms are chosen to be
consistent with the ones used in (Berkman et al., 1993).
â€¢The index of the â€œmainâ€ element currently focused on by a thread isk.
â€¢The index of an element left ofkisi, with â€œleft ofâ€ meaning thati < k.
â€¢The index of an element right ofkisj, with â€œright ofâ€ meaning thatj > k.
â€¢During the algorithm we splitAinto non-overlapping parts called subsets.
â€¢An elementâ€™s index inAis its global index.
â€¢An elementâ€™s index in its subset is its local index.
â€¢The global index of the minimum element in subsetkisb k.
â€¢The global index of the left match ofbk isl(b k).
â€¢The global index of the right match ofbk isr(b k).
â€¢ A â€œnonlarger valueâ€ is defined similarly to a â€œsmaller valueâ€ but where we look for an element of
equal or smaller value. The definition of the left nonlarger value would e.g. bei < k, A[i] â‰¤A [k],
âˆ€â„“âˆˆN, i < â„“ < k:A[â„“]> A[k].
â€¢The global index of the left- and right nearest nonlarger value ofbk arenl(b k)andnr(b k).
â€¢ The index of the subset containing elementl(bk)is gl(bk). We sometimes replace this withi,
allowing us to writer(bi)instead of r(bgl(bk)). Similarly the subset containingr(bk)is gr(bk)
and we sometimes writel(bj)instead ofl(b gr(bk)).
â€¢In the same way, the subsets containingnl(bk)andnr(b k)aregnl(b k)andgnr(b k).
Page 7 of 57

3.1 Computation model and analysis termss184009 25th June 2025
3.1 Computation model and analysis terms
All the algorithms described in this thesis (both ANSV and RMQ algorithms) share the property that
they split up their calculations into non-overlapping parts, before the assign them to the threads for
parallel execution. This means that while two processors might read overlapping input and calculate
the same variables local to themselves, they each calculate and write non-overlapping parts of the
output array for any given subroutine.
This means that the algorithms are compliant with the Concurrent Read Exclusive Write PRAM
(CREW PRAM) model.
For the analysis of the algorithms, we use the terms work and span. Work is the number of
calculations performed in total across all threads. Span is the maximum number of calculations
performed by any thread.
To compare this to the usual runtime analysis of a sequential algorithm, the span is the actual
runtime of the parallel algorithm, since the algorithm takes only as long as the slowest thread.
The work is how long it would take to run the parallel algorithm if we ran it sequentially, with
one thread performing all the work.
4 The ANSV algorithm from (Berkman et al., 1993)
In short, the algorithm described in (Berkman et al., 1993) works by splittingA into subsets, finding
all matches within the subsets, and then finding all matches across subsets. This way, it can achieve
parallelisation over n
lg(n) threads by splittingAinto n
lg(n) subsets of sizelg(n)each.
Some of the smaller steps in the algorithm are repeated multiple times, why they are split into
subroutines. We will explain the subroutines first, and then the full algorithm.
The standard version of the algorithm assumes that all values in the input arrayAare distinct.
4.1 Basic search procedure
The first subroutine is thebasic search procedure. It works by first constructing a binary tree with
the leaf nodes being the elements ofA and the other nodes having the value of the minimum of its
descendants. Henceforth, we refer to such a tree as a min-tree. Once the min-tree is constructed, we
can find the nearest smaller value to the left of a leaf node with valuea by walking up the tree until
the left sibling is less thana. We then go to the left sibling and walk down the tree going to the
right child if it is less thana, and otherwise going to the left child.
To instead find the right nearest smaller value, swap all references of left and right. You can also
find the nearest nonlarger values by replacing all â€œless thanâ€ checks with â€œless than or equalâ€.
Constructing the min-tree takesO(n)work and O(log(n))span, and running the basic search
procedure is single threaded and runs inO(log(n))time.
4.2 The merging procedure
The second subroutine is themerging procedure. It takes two arraysA1 and A2 as input, and finds
matches for elements across the arrays.
To make the concept of matches make sense, we assume thatA1 and A2 are non-overlapping
sub-arrays originating from the same array, and thatA1 is located left ofA2. This means that we
are looking for right matches for elements inA1 and left matches for elements inA2. We also assume
that all elements in the input arrays are distinct.
Page 8 of 57

4.3 The full ANSV algorithms184009 25th June 2025
The merging procedure starts with calculating the suffix minima ofA1 and naming itC1, then
calculating the prefix minima ofA2 and naming itC2. It then mergesC1 and C2 into a monotonic
increasing listC.
We now notice that for any elemente in C, if we assume thate originates from C1, we can
calculate the number of elements inC2 that are smaller thaneby subtractingeâ€™s index inC1 from
its index inC. It also works the other way ife instead originates fromC2. This means that we can
calculate the matches across the arrays using the following formulas:
â€¢Letâ„“be the length ofC 2.
â€¢Letrrefer to an elementâ€™s own index inC1 orC 2.
â€¢Letmrefer to an elementâ€™s own index inC.
â€¢ If r = m (i.e. the element and all previous elements inC are from the same array), there is no
match.
â€¢If the element originates fromC 1, its right match isâ„“âˆ’(mâˆ’r).
â€¢If the element originates fromC 2, its left match ismâˆ’(â„“âˆ’r).
This obviously runs in linear time.
The reason why we assume that the elements are distinct, can be seen if we place an element
of the same value in each array. If the input isA1 = [1,3]and A2 = [3,2], then the merged list will
be either C = [1,2,3,3]or C = [1,2,3,3], with green denotingA1 and red denotingA2. In either case,
one of the elements of value 3 will get a match of value 3, which is not a smaller value.
4.3 The full ANSV algorithm
With the two subroutines defined, we can now explain the full ANSV algorithm. We start by creating
the min-tree, and splitting the array inton
lg(n) subsets of lengthlg(n)to which we assign one thread
each. The rest of the algorithm is explained as a sequential algorithm from the point of view of
threadkassigned to subsetk. The parallelism then follows naturally, since we have n
lg(n) threads.
Whenever one thread reads a value written by another thread, we implicitly synchronize the
threads, such that they all catch up to each other and the value is written before it is read. In
practice it is enough to synchronize the threads twice. We also implicitly convert all results local to
a subset (such as the index of the minimum element) to global indices before we save them to the
arrays shared between threads. Converting a local index to a global index and back to a local index
can be done by addingklg(n)and subtracting
j
index
lg(n)
k
lg(n).
Subset specific calculations
The first thing a thread does is to perform calculations local to its subset. Specifically:
â€¢Solving the ANSV problem locally using the standard sequential stack-based algorithm.
â€¢Finding the index of the minimum element,bk.
â€¢Calculating the prefix minima.
â€¢Calculating the suffix minima.
Page 9 of 57

4.3 The full ANSV algorithms184009 25th June 2025
Global calculations
The second thing the thread does is to precompute values necessary for solving the ANSV problem
across subsets. Specifically:
â€¢ Finding the nearest smallervalues ofbk,namely l(bk)and r(bk)using the basic search procedure.
â€¢ Calculating the subsets that these elements belong to. That isgl(bk) =
j
l(bk)
lg(n)
k
and gr(bk) =j
r(bk)
lg(n)
k
.
With everything ready, the thread can now use the merging procedure to match elements across
subsets. The thread is responsible for checking four conditions, and depending on which ones are
true, running the merging procedure on the prefix- and suffix minima of various subarrays of the
full array. We always use the suffix minima of the leftmost subarray and the prefix minima of the
rightmost subarray. The four cases are:
1. If gl(bk) = kâˆ’1,we run the merging procedure on subarraysl(bk) . . . klg(n)âˆ’1and klg (n) . . . bk.
2. If gr(bk) = k + 1, we run the merging procedure on subarraysbk . . .(k + 1) lg(n) âˆ’ 1and
(k+ 1) lg(n). . . r(b k).
3. If l(bk)and r(bk)were bothfound,and l(bk) < r(bk),we run the merging procedure on subarrays
l(bj). . . l(b k)andr(b k). . . bj.
4. If l(bk)and r(bk)were bothfound,and l(bk) > r(bk),we run the merging procedure on subarrays
bi . . . l(bk)andr(b k). . . r(b i).
We note that since each of the subarrays is contained within a single subset, and since the start/end
index of each subarray is either the start/end of a subset (depending on if we use the prefix- or suffix
minima) or the nearest smaller value of an element outside the subarray, the prefix-/suffix minima of
the subarray will always be the same as the same indexes of the prefix-/suffix minima of the entire
subset. We can therefore use the prefix-/suffix minima that we calculated beforehand.
The output of the merging procedure is the matches of the elements that are the same in the
subset and in the prefix-/suffix minima of the subset. For all other elements, the fact that they have
a different value in the prefix-/suffix minima means that its nearest smaller value is within its own
subset. We can therefore find the entire ANSV solution by taking all the local ANSV solutions,
and for all the elements that do not have a local match, use the match found via the merging
procedures (which we convert to global indices by adding the start index of the respective subarray).
Any remaining elements that we still have not found a match for simply do not have matches.
Looking at the individual parts of the algorithm, we see that:
â€¢Constructing the min-tree performsO(n)work and hasO(log(n))span on n
lg(n) threads.
â€¢Findingl(b k)andr(b k)is single threaded and takesO(log(n))time.
â€¢ Everything else is single threaded and runs inO(m)time, but is only executed on inputs of
lengthm=O(log(n)), hence running inO(log(n))time.
In total, since each thread performs O(log(n))work and we spawn n
lg(n) threads, the algorithm
performsO(n)work withO(log(n))span on n
lg(n) threads.
Page 10 of 57

s184009 25th June 2025
5 Generalised CPU version
So far we have assumed that all elements in the input array are distinct. The reasons for this are
that otherwise:
1. The merging procedure would return an incorrect output, as seen in the example shown in
section 4.2.
2. The method for selecting which subarrays to run the merging procedure on would no longer
ensure that all elements find their matches.
3. In the original article, it is shown that only one thread will run the merging procedure on each
pair of subarrays.1 While this does not affect the correctness of the algorithm, it would turn
the computation model from CREW to CRCW due to multiple threads potentially writing the
same value.
4. We assume that there is an unique minimum element in each subset.
The method to solve this problem suggested in (Berkman et al., 1993) is to enumerate all values in
the array, turning them into tuples, with the index being a tie breaker to decide which of two identical
elements is â€œsmallerâ€. When we enumerate the values with increasing indices, all right matches will
be correct, but the left matches risk pointing to elements with the same value. We therefore also
reverse the enumeration, since with decreasing indices, it is now then left matches that are correct.
This means that by modifying the input and running the algorithm twice, we can handle duplicate
values.
We will present a modified version of the algorithm which does not require modification of the
input and only needs to be run once, yet still handles duplicate values.
In order to fix finding matches across subsets, we will present a modified merging procedure and
a modified set of rules for which subarrays to run the merging procedure on. The alternative merging
procedure fixes the correctness of the output when the input includes duplicates, while the rules
for which subarrays to run the merging procedure on ensures that we correctly run the merging
procedure on all relevant subarrays while staying within the CREW PRAM computation model.
Finally, in order to handle multiple minima within a single subset, we will present a trick to have
those multiple minima act as a single minimum. Since we explain this last, for the earlier parts of
the algorithm, we assume that each subset has a single minimum element,bk.
5.1 The modified merging procedure
The input to the modified merging procedure is two arraysA1 andA 2.
We start by filtering out all elements that already know their match (right match for elements in
A1 and left match for elements inA2). This can be done by either passing the information along as
extra input, or by calculating the prefix-/suffix minima and only keeping the elements where their
prefix-/suffix minima is equal to itself. The remaining elements are stored in arraysC1 and C2 along
with their indices fromA1 andA 2. We then mergeC1 andC 2 into a monotonic increasing listC.
We can then find the matches by iterating overCand using the following logic:
â€¢Let(val,index)refer to an elementâ€™s value and index fromA1/A2.
â€¢Letabe the element we are currently trying to find a match for.
1Berkman et al., 1993, Lemma 3.1, page 351.
Page 11 of 57

5.2 Selecting subarrays to merges184009 25th June 2025
Value
IndexSubset 0 Subset 1 Subset 2 Subset 3
Figure 1: A visualization of an array where the original ANSV algorithm would fail to find all the
correct matches. The vertical axis represents the value of the elements, the horizontal line represents
the index, and the vertical lines represent the boundaries between subsets. An example of an array
that would fit this illustration is[1,3,5,5,6,8,7,6,5,4,2,0].
â€¢ Let b and c refer to the last two elements with distinct values from the other array. I.e. ifa
is from C1, and we have previously seen(1,1),(2 ,3), and(2 ,4)from C2, then b = (1,1)and
c= (2,4).
â€¢ If the value ofa is greater than the value ofc, the match fora is the index ofc. Otherwise the
match forais the index ofb.
â€¢ If we cannot pick eitherb or c to be the match ofa due to not yet having iterated over elements
from the other array, thena does not have a match to be found in the input to the merging
procedure.
5.2 Selecting subarrays to merge
To know what to modify in the logic of selecting which subarrays to merge, we start by looking at
an example of what situations the original algorithm fails to calculate the correct output.
Looking at fig. 1 we can see three issues:
(a) Since the minimum in subset 1 and 2 are the same value (the red dots), both thread 1 and 2
run the merging procedure on the subarrays coloured blue and green.
(b) The merging procedure is run on the subarrays coloured blue and green, but to get the correct
output, the purple element should have been part of the blue subarray.
(c) No thread runs the merging procedure on the elements of subset 1 and 2, despite the fact that
the merging procedure should be run on the red and black elements.
Informally, the three issues can be solved in the following way:
Page 12 of 57

5.2 Selecting subarrays to merges184009 25th June 2025
(a) In case where the minima of multiple neighbouring subsets are the same, only the leftmost
subset runs the merging procedure.
(b) When using the basic search procedure to find the start and end indices for the subarrays
that should be merged, instead of looking for the nearest smaller value, we look for the nearest
nonlarger value.
(c) When checking if a subset should be merged with its neighbouring subset to the left, instead
of checking if the nearest smaller value is in the subset, check if the nearest nonlarger value is
in it. Note that when comparing to the neighbouring subset to the right, we still check for the
nearest smaller value in order to avoid duplicated work.
In case we merge subarrays from two neighbouring subsets with the same minimum value, we
no longer find the matches of the minimum elements in the direction of the other subset. To
remedy this, we use the basic search procedure.
Formally, the new rules for deciding which subarrays to run the merging procedure on look as
follows:
â€¢ If gnl(bk) = kâˆ’ 1we run the merging procedure on subarrays nl(bk) . . . klog(n) âˆ’ 1and
klog(n). . . b k.
â€¢ If gr(bk) = k + 1, we run the merging procedure on subarraysbk . . .(k + 1) log(n) âˆ’ 1and
(k+ 1) log(n). . . r(b k).
â€¢IfA[b kâˆ’1] =A[b k], find the right match ofbkâˆ’1 using the min-tree.
â€¢IfA[b k+1] =A[b k], find the left match ofbk+1 using the min-tree.
â€¢Ifl(b k)andr(b k)both exist, and bothgl(b k) =gnl(b k)andgnr(b i) =gr(b k), then we:
1. Find the nearest nonlarger element to the right of elementbk in subset j, and denote it
nrj(bk). This element is found using the min-tree as usual, but by starting the search in
element jlognâˆ’ 1while ignoring the value of the element itself and instead searching for
the value of elementbk.
2. Run the merging procedure on subarraysbi . . . nl(bk)andnr j(bk). . . nr(b i).
3. IfA[b i] =A[nr(b i)], find the right match ofbi,r(b i), using the min-tree.
â€¢Ifl(b k)andr(b k)were both found,gr(b k) =gnr(b k),gl(b j) =gl(b k), then we:
1. Find the nearest nonlarger element to the left of elementbk in subset i, and denote it
nli(bk). This element is found using the min-tree as usual, but by starting the search in
element( i + 1)logn while ignoring the value of the element itself and instead searching
for the value of elementbk.
2. Run the merging procedure on subarraysl(bj). . . nl(b k)andnr j(bk). . . bj.
We would like to point out that while the two rules for whenl(bk)and r(bk)are both found look
very similar, their differences are very important. The difference in their usage of smaller values and
nonlarger values is what ensures that two threads do not perform overlapping work. I.e. it is what
guarantees that the algorithm stays in the CREW PRAM computation model.
Page 13 of 57

5.3 Multiple minima within a subsets184009 25th June 2025
5.3 Multiple minima within a subset
So far we have assumed that each subset contains only a single minimum element,bk, to keep the
description simple. We will now change the meaning ofbk depending on the context to handle
multiple minima elements while preserving the correctness of the rest of the algorithm.
We observe the following properties of the algorithm with regards to having multiple minima
elements within a single subset:
â€¢ If two or more minima elements are present in a subset, all elements in-between these elements
(except minima elements) will have their ANSV found by the sequential algorithm.
The logic behind this property is that since these elements have a minimum element both to
the left and to the right of them, they have a smaller element on both sides of them, meaning
that the ANSV algorithm will find a match to both sides.
â€¢ When part of a subset is selected as the left input array to the merging procedure, only the
elements to the right of the rightmost minima element are relevant to the merging.
For this,we point out that since all elements to the left of the rightmost minima element already
have right matches within the subset, these elements cannot find matches in a neighbouring
subset. Likewise, the elements in the neighbouring subset cannot find their match to be the left
of the rightmost minima element. The only exceptions are the other minima elements which
matches must exist outside the subset, but those are handled by another property.
â€¢ The same is true, where the right input array of the merging procedure only needs elements to
the left of the leftmost minima element.
â€¢The left and right matches for all minima elements in a subset will be the same.
The logic behind this property, is that since there are no smaller values in-between the minima
elements, they closest smaller element outside the subset will be the closest smaller element to
all the minima elements.
Using these properties,we declare the following rules aboutwhichminimum elementis represented
byb k:
â€¢ When the right input array for the merging procedure is selected from the subset,bk represents
the leftmost minimum element.
â€¢ When the left input array for the merging procedure is selected from the subset,bk represents
the rightmost minimum element.
â€¢ When we use the basic search procedure to findl(bk)or nl(bk), bk represents the leftmost
minimum element.
â€¢ When we use the basic search procedure to findr(bk)or nr(bk), bk represents the rightmost
minimum element.
This way we have effectively contracted all the minima elements and all the elements in-between into
a single point, while finding matches across subsets. After we are done running the merging procedure
on all relevant subarrays, we execute the following logic on all subsets to finish the algorithm: Take
the right match of the rightmost minimum element and the left match of the leftmost minimum
element and copy those to all the other minima elements. This way, all minima within a subset, find
the same left and right matches, just as they should.
Page 14 of 57

5.4 Algorithmic propertiess184009 25th June 2025
5.4 Algorithmic properties
While the modified algorithm is significantly more complicated due to it needing to handle many
more special cases, it does not affect the algorithmic properties of the algorithm.
The merging procedure still runs in linear time of its input size. The merging procedure is still
only run a constant number of times per thread. The basic search procedure is also still only run
a constant number of times per tread. And finally, copying the ANSV result amongst the multiple
minima elements within a subset, also runs in linear time per thread.
This means that the work is stillO(n), the span is stillO(log(n))and the number of threads
used is also still n
lg(n).
6 The BSZ algorithm
Later, when we compare the performance of our ANSV algorithm to previous implementations,
we use the implementation of the ANSV algorithm made for and used in (Shun & Zhao, 2013b).2
This implementation uses a heuristic to simplify the algorithm so we will explain said heuristic. In
(Sitchinava & Svenning, 2024) they call this the BSZ algorithm, which is the name that we will adopt.
The algorithm starts by splittingA into s subsets and finding the ANSV locally within each
subset using a sequential algorithm. It then constructs the min-tree.
The third step is to find the right matches using the following procedure:
1. Take the rightmost unmatched elementa in a subset and find its right match using the min-tree
(as in the basic search procedure).
2. Take the next unmatched elementb to the left ofa, and find its right match using the min-tree.
This time, however, we do not start the search inb, but instead we start atr(a).
3. Repeat step 2 for all unmatched elements in the subset.
This step is shown in fig. 2.
The fourth and final step is to find all the left matches, using the same method as for the right
matches, but where we search left instead of right.
The reason why the heuristic works can be seen if we look at the example shown in fig. 2. We
know that A[b] â‰¤A [a]since otherwise b would find a to be its right match within the subset. We
also know thatâˆ€jâˆˆN, a < j < r (a) : A[j] â‰¥A [a]since otherwise a would have found a right match
before r(a). By transitivity, we now know thatâˆ€jâˆˆN, a < j < r (a) : A[j] â‰¥A [b], which means that
r(b) â‰¥r (a), so there is no point in searching the part of the array left ofr(a)when we look for the
right match ofb.
In (Sitchinava & Svenning, 2024) they show that for a1 â‰¤sâ‰¤n , this algorithm achieves
O

n

1 + log(n)
k

work and O
 
k
 
1 + log
  n
k

span.3 By setting k = Î˜(log(n))they claim that it
achievesO(1)work andO
 
log(n) log
  n
k

span.
2Shun and Zhao, 2013a.
3Sitchinava and Svenning, 2024, p. 261.
Page 15 of 57

s184009 25th June 2025
subsetk
abc r(a) r(b) r(c)
matcha matchb matchc
Figure 2: Illustration of the heuristic used in (Shun & Zhao, 2013b). In subsetk, the elementsa, b,
and c have yet to find their right matches. The arrows beneath â€œmatchaâ€, â€œmatchbâ€, and â€œmatchcâ€
show the parts of the array that are searched using the min-tree when looking for the right matches
ofa,b, andcrespectively.
7 GPU version
In this section, we will present a version of the ANSV algorithm tailored for running on a GPU. The
modified merging procedure shown in section 5.1 did not turn out to be suitable for a GPU, so we
based it on the version of the ANSV algorithm explain in (Berkman et al., 1993).
7.1 How a GPU works / GPU Terminology
Before we explain our GPU algorithm, we will roughly explain how GPU hardware is designed
and some of the features it has. This will not be a comprehensive explanation, but the aim is to
simplify and explain the parts that are necessary in order to understand the algorithms and the
performance characteristics we will touch on in the discussion. It shall also be noted that much of the
GPU terminology is dependent on which company is designing the GPU, but the actual hardware
is roughly the same just with different marketing. In our case, we will use the terminology used by
AMD.
7.2 Workgroups and kernels
On a GPU we haveworkgroupswhichare sets of32 threads groupedtogether(the exactnumbervaries
depending on the hardware). All the threads in a workgroup sharing the same set of instructions and
are started and stopped together. When running code, each workgroup knows what itsworkgroup
indexis, and each thread in each workgroup knows what itsthread indexis. The threads can therefore
perform independent work by calculating which indices of the input and output arrays that it should
read from and write to.
This is different from how a CPU works, where each thread has its own set of instructions. This
means that while it is possible to run a CPU algorithm on GPU hardware by starting the algorithm
with â€œif thread index is 0, execute thisâ€, this would result in 31 of the 32 threads doing nothing while
waiting for the one thread to finish. Instead GPU algorithms should be designed in such a way that
all 32 threads perform approximately the same amount of work and finish at the same time.
A piece of code designed for running on a workgroup is called akernel. When wedispatch(run)
a kernel, we run many workgroups at a time. The actual hardware that executes the threads in
a workgroup is called aSIMD. A GPU is designed to have as many SIMDs as possible, typically
hundreds.
Page 16 of 57

7.3 Assumptions and trickss184009 25th June 2025
7.2.1 Synchronization
One important consideration when designing GPU algorithms issynchronization. When executing a
kernel across multiple workgroups, we do not have any guarantees on which order the operations are
executed in. This means that if two threads need to collaborate, neither of them can know when the
result from the other thread has been written to an array.
Consider for example if we want to find the minimum of an array. Each workgroup is assigned
part of the array, and each thread in said workgroup is then assigned part of that part. After all the
threads have found the local minima of their part, we still need to somehow find the minimum across
all the parts. Naively, we could just let one thread read all the local minima and find the minima
amongst them, but even this does not work, since we have no way to know when each thread has
written their local minima have been written to a shared array.
The solution to solve this problem is to synchronize the threads and workgroups by utilizing
barriers. Depending on the hardware configuration there exist many types of barriers, but the two
most common ones are the workgroup-wide barrier, which synchronizes the threads within a single
workgroup, and the implicit synchronization that happens after a kernel is done executing.
The workgroup-wide barrier makes it easy and fast to share information between threads in a
workgroup. The synchronization between two kernels being dispatched is useful for implementing
more complex algorithms, where we need synchronization between all threads.
Whenever we explicitly refer to synchronization in the algorithms, we mean workgroup-wide
synchronization, as the kernel synchronization happens implicitly.
7.2.2 Memory and cache layers
While it varies slightly over time and between GPU manufactureres, the general design of a GPUâ€™s
memory hierarchy is as follows:
â€¢Video RAM (VRAM) is the GPUâ€™s main memory.
â€¢ The level 2 cache is the next cache layer with slightly faster cache. Not all SIMDs share the
same level 2 cache.
â€¢ The Local Data Share (LDS) is the smallest and fastest cache on a GPU. Each LDS cache is
shared between four SIMDs.
While a SIMD unit can only execute one workgroup at a time, it is designed to store up to 16
workgroups in its cache. Then, whenever a workgroup needs to access some data not in the LDS
cache, the SIMD can quickly switch to working on another of the 16 workgroups, while it waits for
the data to be retrieved from higher up in the memory hierarchy.
This is one of the tricks a GPU has to speed up its execution, but in return it limits us when
we design our algorithm since we need to be very aware of the cache usage of our algorithm. As if a
workgroup requires too much data to be cached, then the SIMD unit first stops preparing the other
15 workgroups, and if it is even worse, then the other three SIMD units that share the same LDS
cache stop preparing their workgroups as well.
7.3 Assumptions and tricks
We need to point out a two assumptions that we make. They are strictly speaking irrelevant to the
algorithm itself, and lean more towards implementing the algorithm in practice, but they straighten
a couple rough edges in the explanations.
Page 17 of 57

7.4 Common algorithm patterns on GPUss184009 25th June 2025
â€¢ We assume that unless otherwise mentioned, all arrays use unsigned 32 bit integers as their
data type.
The reason why we assume a data type is that we need to know the minimum value and the
maximum value that an input element can have.
â€¢ Keeping track of which elements have found matches, and which have not can be tricky in
GPU code. To reduce the need for a separate array to store this data (which simplifies our
explanations), we define an index being the maximum possible value of an unsigned 32 bit
integer to mean that we have not found a match for said element.
7.4 Common algorithm patterns on GPUs
In order to facilitate simpler descriptions of the GPU kernels, we will start by describing some smaller
algorithms and techniques that are useful on a GPU. These can then be used as parts of GPU kernels
without further explanation.
7.4.1 Tree reduction
Often, when we e.g. want to find the minimum element in an array, we split the array into parts and
assign each part to a thread which performs the comparisons. After this, we now have a minimum for
each thread, but we still need to find the minimum across thread. To accomplish this, it is common
to reduce the thread minima in a binary tree structure. Let the leaves be the threads minima, and
the root being the global minima. For each layer in the tree, we take every node and let the left child
(thread) find the minimum of both the children, and write the result to the node. Once all nodes in
a layer are reduced, we start with the next layer.
The tree structure and thread assignment is described formally in listing 1 and visualised in fig. 3.
This method is typically used for the 32 threads within a workgroup, as the workgroup syn-
chronization operation is fast, so it does not matter if we use it five times. If the method is used
across workgroups, then the upper bound on the number of leafs in the tree is much higher and
synchronization only happens when a kernel finishes running, so we need to dispatch multiple kernels
which is very expensive. In this case, we let each node in the tree have the same number of children
as in the original array parts, to reduce the number of kernels that needs to be dispatched.
If the number of threads is constant, calculating the thread minima results inO(n)work and
span, while the tree has a constant number of nodes and therefore does not affect neither work nor
span.
If instead the size of the part of the array assigned to each thread is constant, calculating the
thread minima results inO(n)work and O(1)span, while the tree now has a linear number of nodes,
so reducing the tree results inO(n)work andO(log(n))span.
We can therefore conclude that performing tree reduction requires O(n)work and O(n)or
O(log(n))span depending on if the number of threads or the number of elements per thread is set
to be constant.
Page 18 of 57

7.4 Common algorithm patterns on GPUss184009 25th June 2025
1t : The index of the current thread
2M : The array c o n t a i n i n g the thread minima
3
4For i =0..âŒˆlg(T)âŒ‰-1:
5d =2 i
6Iftâ‰¡0 moddandt+d < T:
7M [ t ] = min ( M [ t ] , M [ t + d ])
8S y n c h r o n i z e
Listing 1: Tree reduction
Thread 1 Thread 2 Thread 3 Thread 4
Array
Thread minima
Global minima
Figure 3: The bottom array is split into four parts and each part is assigned to a thread. Each thread
then finds the minima in its part, after which the minima across threads is found.
7.4.2 Single workgroup minimum
As explained section 7.4.1, a single workgroup can find the minimum of an array by splitting the
array into 32 parts, having each thread find the minimum of one part, and then performing tree
reduction to find the global minimum.
As we have a constant number of threads, doing this performsO(n)work and hasO(n)span.
7.4.3 Single workgroup prefix- and suffix minima
To find the prefix minima of an array, start by splitting the array into 32 parts and have each thread
find the minimum of their part. We then synchronize to allow all threads to know all the thread
minima. Each thread now finds the minima of all previous threads, and finds the prefix minima of
its own part using a sequential algorithm, but with the initial prefix minima being the minima of
the previous threads. The algorithm is described formally in listing 2.
Since all the algorithm does is that each thread iterates over its part of the array twice, and over
the constant length thread-minima array once, the work and span areO(n).
The suffix minima is found similarly.
1t : The index of the current thread
2e : The number of elements per thread
3A : The input array
4M : The array c o n t a i n i n g the thread minima
5P : The array c o n t a i n i n g the prefix minima
6
Page 19 of 57

7.4 Common algorithm patterns on GPUss184009 25th June 2025
7M [ t ] = min ( A [ t * e ..( t +1) *e -1])
8S y n c h r o n i z e
9p = min ( M [0.. t -1])
10For i =0.. e -1:
11p = min (p , A [ t * e + i ])
12P [ t * e + i ] = p
Listing 2: Prefix minima
7.4.4 Bitonic merge
To emulate the merging procedure on a GPU, we need to be able to merge a monotonic increasing
array and a monotonic decreasing array into a single monotonic increasing array. We let the input be a
single array that is the concatenation of the monotonic increasing array and the monotonic decreasing
array (henceforth namedarray), along with the index of when the array switches from monotonic
increasing to monotonic decreasing (the peak). A sequence that is first monotonic increasing and
then monotonic decreasing is called a bitonic sequence, hence why we name this a bitonic merge.
We add the extra assumption that no value is shared between the monotonic increasing and
monotonic decreasing parts of the input array. That is, we assume that the input is of the form that
we would get by having an array with no duplicate values, and then finding the suffix minima on the
half left of the peak, and prefix minima on the half right of the peak. This assumption is necessary
in order to for there to only exist one valid merged list, which we need in order to avoid the same
issues we discussed regarding the generalized CPU version of the ANSV algorithm.
In a sequential algorithm, we would start at the peak and iterate to the left and right picking
whatever value is the largest and writing that to the output array starting from the end (such that
the start of the output array contains the smallest values). To parallelise this on a GPU we want to
predict which indices the sequential algorithm would compare when deciding what value to write to
a specific index in the merged list. Formally, we want to find the indicesi1 and i2 of the two elements
that are compared when writing indexjof the merged array.
To locate these indices, each thread runs two concurrent binary searches. One locatesi1 in
the monotonic increasing part, i.e.âˆ’1 . . . peakâˆ’ 1, and the other one locatesi2 in the monotonic
decreasing part, i.e.peak . . .|array| (with i1 and i2 being stored as signed integers). It is on purpose
that we allow indices outside the array, since we need these indices to be hardcoded to certain values
based on which binary search is reading them:
â€¢Ifi 1 =peakthenarray[i 1 + 1] =the maximum value storable by an unsigned 32 bit integer.
â€¢Ifi 2 =peakthenarray[i 2 âˆ’1] =the maximum value storable by an unsigned 32 bit integer.
â€¢Ifi 1 =âˆ’1thenarray[i 1] = 0(the minimum value storable by an unsigned 32 bit integer).
â€¢Ifi 2 =|array|thenarray[i 2] = 0.
This way the binary searches can read one index outside the ranges of the monotonic arrays, while
preserving the monotonic property. The reason why this is necessary is that we want to iterate on
both binary searches at the same time until we satisfy the condition
(array[i 1]< array[i 2]< array[i 1 + 1]âˆ¨array[i 2]< array[i 1]< array[i 2 âˆ’1])
âˆ§(peakâˆ’i 1 âˆ’1) + (i 2 âˆ’peak) =k
 |array|
32

withkbeing the thread index. The condition has two parts:
Page 20 of 57

7.4 Common algorithm patterns on GPUss184009 25th June 2025
index
value
i1 i1 + 1 peak i2
array[i 1]
array[i 1 + 1]
array[i 2]
Figure 4: An example of a stabilised set of binary searches for the bitonic merge.
1. By requiring thatarray[i1] < array [i2] < array [i1 + 1], we force the indices to be as seen in
fig. 4, where the element at indexi1 is the first element on the left side of the peak which is
smaller than the element at indexi2 on the right side of the peak.
Likewise, if array[i2] < array [i1] < array [i2 âˆ’ 1]is satisfied instead, i2 is the first element
which is smaller thani1.
2. The condition(peakâˆ’i 1 âˆ’1)+( i2 âˆ’peak) = k
l
|array|
32
m
has the purpose of distributing the work
amongst the threads. Specifically, if we assign
l
|array|
32
m
elements to each of the 32 threads, then
k
l
|array|
32
m
is the offset from the end of the output array at which threadk should start writing
its output.( peakâˆ’i 1 âˆ’ 1) + (i2 âˆ’peak )denotes the number of elements that has already been
merged,whichin this case means thatifwe require that(peakâˆ’i 1âˆ’1)+(i2âˆ’peak) = k
l
|array|
32
m
,
threadkskips the largestk
l
|array|
32
m
elements.
Once the two binary searches are done, threadk knows the state at which the sequential merge
algorithm would have been after mergingk
l
|array|
32
m
elements, and it can therefore start merging the
next
l
|array|
32
m
elements independently of the state of the other threads.
For the sake of using the merged list to calculate the ANSV, instead of storing the values of the
elements, we store the indices that the merged elements had in the input array.
Running two binary searches concurrently only performs at most2lg(n)steps, so the work and
span are dominated by actually merging elements. Since the number of threads is constant, both
work and span areO(n).
Page 21 of 57

7.5 The ANSV algorithms184009 25th June 2025
7.4.5 Rightmost and leftmost smaller value
Another small algorithm is finding the rightmost element in an array that has a value smaller than
a target value. The input is the array and the target value. We start by splitting the array into 32
parts, assigning each part to a thread which finds the index of the rightmost smaller element in the
part sequentially. We then synchronize the threads, and use the tree reduction method to find the
largest index amongst the threads.
We likewise find the leftmost smaller element by finding the leftmost smaller element in the part,
and then finding the smallest index in the array via the tree reduction method.
This obviously runs inO(n)time and withO(n)span.
7.5 The ANSV algorithm
Now for the actual ANSV algorithm. First we need to note a few things regarding the explanation
of the algorithm itself:
â€¢ The algorithm is split into six kernels, which we will present first, after that we will present
how these kernels are executed with regards to input and output. The reason is that we need
to accommodate the limitations of how the GPU synchronizes threads.
â€¢ Each kernel is analysed with regards to the work performed by a single workgroup and the
span of one thread.O(n)denotes linear work relative to the length of the entire array that is
passed as input to the kernel, whileO(m)denotes linear work relative the length of the part
of the array that is assigned to the specific workgroup (typically a subset).
â€¢ We assume that all the input elements are distinct, since the algorithm is based on the ANSV
algorithm from (Berkman et al., 1993).
7.5.1 Kernel 1: Minimum within subset
The firstkernelâ€™s jobis to calculate the minimum within eachsubset,withthe inputto eachworkgroup
being an array containing the subset. This is exactly what is explained in section 7.4.2, so like what
is explained there, the work isO(m)and the span is alsoO(m).
7.5.2 Kernel 2: ANSV of subset minima
The second kernelâ€™s job is to take an array of all the subset minima found by the first kernel, and
each workgroup then finds the ANSV of a single element of that array.
We do this by assigning each element to a workgroup. That workgroup then runs the rightmost
smaller value algorithm described in section 7.4.5 on the part of the array that is left of the element.
It then likewise runs the leftmost smaller value algorithm on the part of the array that is right of
the element. This gives us the left and right matches of the element assigned to the workgroup.
As the leftmost- and rightmost smaller value algorithms both perform linear work with linear
span, the kernels performsO(n)work and its span isO(n).
7.5.3 Kernel 3: Local ANSV within subset
The third kernelâ€™s job is to solve the ANSV problem locally within each subset.
Each workgroup is assigned a subgroup and runs the entire parallel CPU ANSV algorithm as
described in section 4.3 over the 32 threads in the workgroup.
Page 22 of 57

7.5 The ANSV algorithms184009 25th June 2025
1,4,3,8
5,1,9,11 2,7,4,16 14,13,15,3 8,6,12,10
Figure 5: An illustration of the two-layer min-tree withâˆšn elements per node, derived from the
array [5,1,9,11,2,7,4,16,14,13,15,3,8,6,12,10].
We do make a slight modification though: Instead of the min-tree being a binary tree withlg(m)
layers, it is now a two-layer tree with the root node havingâˆšm children and each node containingâˆšmelements. Such a tree can be seen in fig. 5.
Running the basic search procedure using the two-layer min-tree takesO(âˆšm)time, instead of
the usual O(log(m))time with the regular min-tree. But in return, we only needm + âˆšm space to
store the two-layer min-tree, instead of the2mspace it takes to store the regular min-tree.
Usually, this trade-off wouldnâ€™t be worth it, but since the cache available to each workgroup is
very limited, in practice the number of elements in each subset is only a few hundred. This means
that the difference betweenâˆšm and lg(m)is small, while the space difference is very significant as
it allows the subset size to be larger, which reduces the constant factor of time spent executing the
kernel amortized over the number of elements in the subset.
The work done by the kernel isO(m), since we treat the 32 threads as a constant number, and
all parts of the parallel CPU ANSV algorithm performs at most linear work. The only change is the
two-layer min-tree, but performingâˆšmwork 32 times is still withinO(n).
As for the span of each thread, since we performO(m)work spread out over a constant number
of threads, the span must also beO(m).
7.5.4 Kernel 4: Global ANSV of the subset minima
Kernel 1 found the subset minima, and kernel 2 found the ANSV of the subset minima array. The
job of kernel 4 is to find the nearest smaller values of the subset minima elements in the original
array.
The ANSV of the subset minima array tells us which subsets contain the nearest smaller values
of the subset minima elements. That is, if e.g. the minima element of subset 4,b4, found its left
nearest smaller value in the subset minima array to beb2, then its nearest smaller value,l(b4), in
the original array would be in subset 2.
The reason why this is true can be seen if we continue the example. Lets say that contradictory
to our claim,l(b4)was located in subset 3. Thenb3 â‰¤l (b4) < b 4, which means thatb3 would be the
nearest smaller value in the subset minima array, notb2.
With this knowledge, workgroupk can find the left match ofbk by lettingi be left match ofbk in
the subset minima array. It then runs the rightmost smaller value algorithm described in section 7.4.5
on subsetito find the left match ofbk.
Similarly, we letj be the right match ofbk in the subset minima array and find the right match
ofb k using the leftmost smaller value algorithm on subsetj.
Since all we do is read two values from an array and run the leftmost- and rightmost smaller
value algorithms with the input array being a subset of sizem, the work and span of the algorithm
isO(m).
Page 23 of 57

7.5 The ANSV algorithms184009 25th June 2025
7.5.5 Kernel 5: Calculate sizes of arrays to merge
Kernel 6 is tasked with executing the merging procedure, but since it is very cache-constrained,
we have decided to move the calculations of what subarrays to run the merging procedure on to a
separate kernel.
There are four possible pairs of subarrays to run the merging procedure on, so we denote which
subarrays should be merged by writing the length of the subarrays. If the length is zero, this means
that we shouldnâ€™t merge this subarray. The four pairs are:
1. Subarraysb i . . . l(bk)andr(b k). . . r(b i)should be merged ifA[b i]> A[b j].
2. Subarraysl(b j). . . l(b k)andr(b k). . . bj should be merged ifA[bi]< A[b j].
3. Subarraysl(b k). . .(i+ 1)Â·mâˆ’1andkÂ·m . . . b k should be merged ifi=kâˆ’1.
4. Subarraysb k . . .(k+ 1)Â·mâˆ’1andjÂ·m . . . r(b k)should be merged ifj=k+ 1.
Thread t of workgroupw handles calculating the eight subarray lengths related tobk with k = 32w+t.
Since all the relevant variables have already been calculated during previous kernels, this kernel
performs a constant amount of calculations per thread. The work and span are therefore bothO(1).
7.5.6 Kernel 6: Merging subarrays
The sixth and final kernel handles finding matches across subsets via the merging procedure.
Each workgroup is assigned a set of four potential pairs of subarrays to run the merging procedure
on,as calculated by kernel 5. For each pair where the length of the subarray is not zero,the workgroup
knows the indices of the subarray, since it knows the start index and the length. We then use the
algorithm described in section 7.4.3 to calculate the suffix minima of the left subarray and the prefix
minima of the right subarray. We ensure that the suffix- and prefix minima are written to the same
array, such that once we synchronize the threads, we are ready to run the bitonic merge algorithm
described in section 7.4.4. The output of the bitonic merge is the elementsâ€™ indices from the array
containing both the suffix- and prefix minima, which means that we can know from which subarray
an element originated by checking if the index is less than or greater than the length of the left
subarray.
Now that we have the merged list, we can use the formulas described in the original merging
procedure from section 4.2 to calculate the left and right matches of the elements in the subarrays.
As in the parallel CPU ANSV algorithm, we only write the result if there has not already been found
a match for the element. That is, if kernels 3 or 4 have not already found a match.
The suffix- and prefix minima algorithm and the bitonic merge algorithm all performO(m)work
and their span areO(m). The kernel runs these algorithms up to three times followed by calculating
the indices of the matches, which is alsoO(m)work and span. The total work performed by the
kernel is thereforeO(m)and the span is alsoO(m).
7.5.7 Running the entire algorithm
To round off explaining the algorithm, we will summarise what the kernels calculate, and how they
are to be executed with regards to input and output.
Kernel 1 finds the subset minima. Kernel 2 then calculates the ANSV amongst the subset minima.
Kernel 3 finds all the matches local to the subsets. Kernel 4 finds the ANSV of the subset minima.
Kernel 5 figures out which subarrays to merge. Kernel 6 finds the remaining ANSV using the merging
procedure.
Page 24 of 57

7.5 The ANSV algorithms184009 25th June 2025
The algorithm is therefore run as follows:
1. Calculate/define the subset size,m, that we split the array into.
2. Dispatch kernel 1 withâŒˆn
m âŒ‰workgroups, one for each subset.
subset_min, subset_min_index = kernel1(data)
3. Dispatch kernel 2 withâŒˆn
m âŒ‰workgroups, one for each element in thesubset_minarray.
subset_min_ansv = kernel2(subset_min)
4. Dispatch kernel 3 withâŒˆn
m âŒ‰workgroups, one for each subset.
ansv_partial = kernel3(data)
5. Dispatch kernel 4 withâŒˆn
m âŒ‰workgroups, one for each subset.
subset_min_global_ansv = kernel4(data, subset_min, subset_min_ansv)
6. Dispatch kernel 5 withâŒˆn
m âŒ‰workgroups, one for each subset.
merge_indices = kernel5(data, subset_min_index, subset_min_ansv,
subset_min_global_ansv)
7. Dispatch kernel 6 withâŒˆn
m âŒ‰workgroups, one for each subset.
ansv = kernel6(data, merge_indices, ansv_partial, subset_min_index,
subset_min_ansv, subset_min_global_ansv)
In practice, kernel 4 and 6 puts hard upper limits on the size of the subsets due to cache sizes,
and while kernel 2 is flexible with regards to the size of its input, it becomes exponentially slower the
more subsets there are. This problem can be solved, however, due to the fact that all kernel 2 does is
solving the ANSV problem for a separate array. We can therefore pick the subset sizes to optimize
for the cache sizes available to kernel 4 and 6, while we replace the call to kernel 2 with a recursive
call to the entire algorithm. Once the recursion causes the number of subsets to be sufficiently small,
we call kernel 2 in order to end the recursion.
7.5.8 Correctness
The correctness of the algorithm follows from the fact that it is just a reimplementation of the
CPU version, but split into smaller pieces and with some parts replaced with versions that are more
suitable for running on a GPU, but that calculate the same output.
The CPU version starts by finding the ANSV local to the subsets, the subset minima, and the
prefix- and suffix minima of each subset.
In the GPU version, kernel 1 finds the subset minima, kernel 2 finds the ANSV local to the
subsets, and kernel 6 calculates the prefix- and suffix minima.
The CPU version then uses the basic search procedure to findl(bk)and r(bk), and then calculates
gl(bk)andgr(b k).
The GPU version uses kernel 3 to findgl(bk)and gr(bk), and then uses that in kernel 4 to find
l(bk)andr(b k).
Finally, the CPU version determines which subarrays to run the merging procedure on, and then
runs the merging procedure on said subsets to find the remaining matches for the ANSV output.
In the GPU version, kernel 5 calculates the subarrays to run the merging procedure on, after
which kernel 6 runs the merging procedure and writes the ANSV output.
Page 25 of 57

7.5 The ANSV algorithms184009 25th June 2025
As can be seen from the above, the CPU and GPU versions of the algorithm calculate the same
information, they just do it in different ways and in a different order. Therefore, since the input to
the merging procedure and the merging procedure itself are the same, along with the both version
calculating the same ANSV local to the subsets, the resulting output of the algorithms must also be
the same, proving the correctness of the GPU version of the algorithm.
7.5.9 Work and span
Work
When we analyse the algorithm in terms of work, we need to consider both the work performed by a
single workgroup when running a kernel, and how many workgroups are dispatched.
The work performed by each kernel is:
â€¢ Kernel 1 dispatchesâŒˆ n
m âŒ‰ workgroups performing O(m)work each resulting inO(n)total work.
â€¢ Kernel 2 dispatchesâŒˆ n
m âŒ‰ workgroups, and with the input array beingâŒˆ n
m âŒ‰ long, each workgroup
performsO
  n
m

work resulting inO
  n
m
2
total work.
â€¢ Kernel 3 dispatchesâŒˆ n
m âŒ‰ workgroups performing O(m)work each resulting inO(n)total work.
â€¢ Kernel 4 dispatchesâŒˆ n
m âŒ‰ workgroups performing O(m)work each resulting inO(n)total work.
â€¢ Kernel 5 dispatchesâŒˆ n
m âŒ‰ workgroups performing O(1)work each resulting in O( n
m)total work.
â€¢ Kernel 6 dispatchesâŒˆ n
m âŒ‰ workgroups performing O(m)work each resulting inO(n)total work.
We notice that all kernels, except for kernel 2, performO(n)work. If we then replace kernel 2
with a recursive call, as mentionad in section 7.5.7, and only terminate the recursion oncen is less
than a constant, then kernel 2 performs constant work, and we instead get the recurrence relation
T(n) =T
  n
m

+cn, which describes the total work performed by the algorithm.
If we then assume the subset size to bem = lg(n), we can show that the GPU algorithm is work
optimal, by guessing that the work isT(n)â‰¤2cnand plugging that into the recurrence.
Base case(n= 4)
T(4) =cby definition
2cn= 8câ‰¥c=T(4) =T(n).
Induction step
AssumeT(â„“)â‰¤2câ„“for4â‰¤â„“ < n.
T(n) =T
 n
lg(n)

+cn
â‰¤2c n
lg(n) +cn
=cn
 2
lg(n) + 1

â‰¤2cn=O(n).
Page 26 of 57

7.5 The ANSV algorithms184009 25th June 2025
Span
If we instead look at the span of each kernel we see that the total span isO(m)except for kernel 2.
â€¢Kernel 1 has each workgroup process a lengthmsubset, so its span isO(m).
â€¢Kernel 2 has each workgroup process a lengthâŒˆn
m âŒ‰subset, so its span isO
  n
m

.
â€¢Kernel 3 has each workgroup process a lengthmsubset, so its span isO(m).
â€¢Kernel 4 has each workgroup process two lengthmsubsets, so its span isO(m).
â€¢Kernel 5 only performs a constant number of calculations, so its span isO(1).
â€¢ Kernel 6 has each workgroup merge a constant number of lengthm subsets, so its span is
O(m).
If we then again replace kernel 2 by a recursive call, we get the recurrenceT (n) = T
  n
m

+ cm. We
then again assume that the subset size isn = lg(n)and guess that the span isT (n) â‰¤clg 2(n)which
we plug into the recurrence:
Base case(n= 4)
T(4) =cby definition
clg 2(n) =clg 2(4) = 4câ‰¥c=T(4) =T(n)
Induction step
AssumeT(â„“)â‰¤clg 2(â„“)for4â‰¤â„“ < n.
T(n) =T
 n
lg(n)

+clg(n)
â‰¤clg 2
 n
lg(n)

+clg(n)
=c

lg2
 n
lg(n)

+ lg(n)

=c

(lg(n)âˆ’lg(lg(n))) 2 + lg(n)

=c
  
lg2(n)âˆ’2 lg(n) lg(lg(n)) + lg 2(lg(n))

+ lg(n)

=clg(n)

lg(n)âˆ’2 lg(lg(n)) + lg2(lg(n))
lg(n) + 1

â‰¤clg 2(n) =O(log 2(n)).
The span of the algorithm is thereforeO(log2(n))if we set the subset size tolg(n)and use n
lg(n)
threads.
Page 27 of 57

s184009 25th June 2025
8 Performance optimizations and comparisons
In this section we will discuss implementation, optimization and performance comparisons between
the algorithms.
8.1 Implementation
For our benchmarks, we prepared three implementations of the ANSV algorithm available.
1. The original CPU algorithm from (Berkman et al., 1993).
2. Our modified CPU algorithm that handles duplicate values.
3. The BSZ CPU algorithm from (Shun & Zhao, 2013b).
4. Our GPU algorithm.
We started by implementing the CPU algorithm from (Berkman et al., 1993). We picked the Rust
programming language, since it is fast and because the Rust compiler checks for concurrency bugs
at compile time (instead discovering them at runtime), making it a great programming language for
implementing parallel algorithms. In addition, we used the Rayon library to handle thread pools and
distributing work across threads. This implementation was compiled with Rustâ€™s--release option.
We also easily implemented our modified CPU algorithm by modifying the implementation of
the original CPU algorithm.
We found the BSZ implementation used in (Shun & Zhao, 2013b) on GitHub.4 This implemen-
tation is written in C++ and uses OpenMP to implement their parallelism. It was compiled with
GCCâ€™s-O2 -DOPENMPoptions.
When implementing GPU algorithms in algebraic research, the conventional choice is to use
CUDA, since Nvidia (the makers of CUDA) has the best support and tooling in the industry. We
did, however, have access to a mix of Nvidia and AMD GPUs, with the most accessible GPU being
an AMD GPU. With CUDA only being supported on Nvidia GPUs, this was no longer the optimal
choice. Instead, we decided to target Vulkan, which is an open API that works on both Nvidia and
AMD GPUs. For code to run via Vulkan, it needs to be compiled to SPIR-V which is an intermediary
language. We decided to use the RustGPU project, which compiles Rust code to SPIR-V kernels.
While RustGPU is admittedly still very early in debelopment and buggy, it provides the benefit
of allowing us to reuse parts of our codebase, since everything is written in Rust. To execute the
SPIR-V kernels, we used wgpu which is a library based on the WebGPU API that support running
kernels via Vulkan.
This implementation was compiled with Rustâ€™s--releaseoption.
One important difference between the implementation are the data types. The BSZ implemen-
tation assumes the input is signed 32 bit integers. Our implementations of the CPU algorithms are
made to accept any data type as input, but we tested them with signed 32 bit integers to make it as
comparable as possible.
For our GPU implementation, however, we decided to let the input be unsigned 32 bit integers,
since it made a complex codebase slightly simpler.
4Shun and Zhao, 2013a.
Page 28 of 57

8.2 Test systemss184009 25th June 2025
8.2 Test systems
The hardware thatwe hadavailable forourtests was a mix ofourpersonalhardware andthe hardware
on DTUâ€™s HPC cluster.
Our personal hardware includes a six core Ryzen 5 7600x CPU, an AMD Radeon RX 7900XTX
GPU.
For the systems on DTUâ€™s HPC cluster we used
â€¢A system with a 12 core Xeon E5-2650 v4 CPU.
â€¢A system with a 16 core Xeon Gold 6226R CPU.
â€¢A system with a Nvidia Tesla V100 GPU.
â€¢A system with a Nvidia Tesla A100 GPU.
This means that we have three CPUs and three GPUs available for our tests. It should be noted
that this hardware was released up to six years apart (the Xeon E5-2650 v4 is from 2016, and the
Ryzen 5 7600x is from 2022), so we need to be cautious if we compare benchmarks across different
hardware.
All the systems were Linux systems, with the systems with the Xeon CPUs and the Nvidia GPUs
running RHEL Linux on kernel 6.1 LTS, and our personal system running Arch Linux on kernel
6.15.3.
The Nvidia GPUs were using Nvidiaâ€™s proprietary driver.
The AMD GPU was tested twice, once with the open source RADV Vulkan driver, and the other
with the proprietary AMDGPU Pro driver.
8.3 Test setup
When deciding on which data we would run the benchmarks on, we made four observations:
â€¢ Performing the merging procedure is a significant part of the computations done, so to present
a worst case scenario, we want the subarrays that are merged to be as long as possible.
â€¢ If we make the input too regular, it becomes easier for the CPUâ€™s and GPUâ€™s branch prediction
and memory prefetching hardware to optimise the code execution.
â€¢ It is obvious, that with a small dataset the CPU algorithms will be faster, since dispatching
GPU kernels is slow,and the main benefit of a GPU is that it has many cores that can parallelise
over large datasets.
â€¢ When connecting to a GPU, the WebGPU API includes a process for negotiating which limits
are supported. One such limit is the maximum buffer size, which is the maximum size of an
array that we can use as an argument for a kernel. With our hardware, the maximum supported
limit is approximately 536 million 32 bit elements.
For these reason we decided on two datasets.
The first dataset is an array of 500 million elements constructed in order to maximise the time
spent running the merging procedure. Specifically, we arrange the elements such that if we iterate
over the elements from smallest to largest, we would alternatively pick a value from the start of the
array and the end of the array. An array of six elements would therefore look like [0,2,4,5,3,1].
Page 29 of 57

8.4 Resultss184009 25th June 2025
Since it was designed to potentially force a worst case runtime, we refer to this as the â€œworst caseâ€
dataset.
The second dataset is an array of 500 million random distinct elements. The purpose of this
dataset is to not be biased in any way, and to serve as a reference point compared to our first dataset.
We refer to this as the â€œrandomâ€ dataset.
For the tests themselves, we ran each algorithm five times to warm up the caches followed by 30
runs of which we saved the average times.
Specific to the CPUs, since we had a mix of 6, 12, and 16 core CPUs, and each CPU core
implements multithreading (having one core run two threads at once), we decided to get data points
at 1, 6, 12, 16, 24, and 32 threads, with each CPU being tested up to twice its core count.
Specific to the GPUs, we had to make a choice regarding what we include in the benchmark time.
Since we both generate the data and read the output on the CPU, one could argue that the GPU
algorithm needs to include transferring the data to the GPU and transferring the output back to the
CPU. This makes sense since eventually, the data would start and end at the CPU. We, however,
would argue that transferring data back and forth does not belong in the benchmark. Our argument
is that while it is true that the data starts and ends on the CPU, our algorithm is supposed to run
in the larger context of a full compression algorithm. So while transferring data back and forth is
part of the actual runtime, it would be amortized between all parts of the compression algorithm,
and we have no way of knowing how much of the runtime would be spent on the ANSV algorithm.
We therefore do not include the time spent transferring data in our benchmarks.
8.4 Results
The data that we collected can be see in table 1, table 2, table 3, table 4, table 5, table 6, and table 7
in the appendix. For the CPU benchmarks, we have split the data by the input dataset and visualised
it in fig. 6 and fig. 7.
The patterns that we see in the figures are:
â€¢ Our implementations are slightly slower but overall similar to the performance of the BSZ
implementation.
â€¢ When the CPUs start using multithreading (i.e. when they spawn more threads than there are
cores on the CPU), the BSZ implementation using OpenMP continues to speed up, while our
implementation using Rust and Rayon stops scaling.
â€¢ Our implementation of the original CPU algorithm is slightly slower than our modified algo-
rithm.
While our modified algorithm was targeted at handling generalised data, our modified merging
procedure also has the added benefit of not needing to calculate the prefix- and suffix minima
of the data. This, along with the merging procedure maybe being more efficient are the only
twospecificparts of the changes that we can point to which would cause a speed-up. The other
changes (notably the new logic for which subarrays to run the merging procedure on) are too
complex and situational for us to know if they even affect the runtime practice.
â€¢ Overall, the algorithms ran faster on the worst case dataset than they did on the random
dataset.
This suggests that we were right in worrying about the branch prediction and memory prefetch-
ing hardware being very efficient at tackling the worst case dataset.
Page 30 of 57

8.4 Resultss184009 25th June 2025
	0
	1000
	2000
	3000
	4000
	5000
	6000
	1 	6 	12 	16 	24 	32
Ryzen-5-7600x-bsz
Ryzen-5-7600x-original
Ryzen-5-7600x-our-version
Xeon-E5-2650-v4-bsz
Xeon-E5-2650-v4-original
Xeon-E5-2650-v4-our-version
Xeon-Gold-6226R-bsz
Xeon-Gold-6226R-original
Xeon-Gold-6226R-our-version
Time	[ms]
Threads
ANSV	benchmarks	on	the	random	dataset
Figure 6: A plot of the data from table 1, table 2, and table 3. To not clump the lines too much, we
have cut off the data points for when the benchmarks are run on one thread.
	0
	1000
	2000
	3000
	4000
	5000
	6000
	1 	6 	12 	16 	24 	32
Ryzen-5-7600x-bsz
Ryzen-5-7600x-original
Ryzen-5-7600x-our-version
Xeon-E5-2650-v4-bsz
Xeon-E5-2650-v4-original
Xeon-E5-2650-v4-our-version
Xeon-Gold-6226R-bsz
Xeon-Gold-6226R-original
Xeon-Gold-6226R-our-version
Time	[ms]
Threads
ANSV	benchmarks	on	the	worst	case	dataset
Figure 7: A plot of the data from table 4, table 5, and table 6. To not clump the lines too much, we
have cut off the data points for when the benchmarks are run on one thread.
Page 31 of 57

8.5 Optimising the CPU algorithms184009 25th June 2025
Overall, it seems like our modified CPU ANSV algorithm is competitive in terms of performance,
although at the cost of being more complex compared to the other algorithms. Where is really shines
is that it does not need to run twice in order to handle data with duplicate values.
The fastest benchmark run of any CPU implementation was the BSZ algorithm on the Xeon
Gold 6226R CPU. This finished in 510ms on the random dataset and in 466ms on the worst case
dataset.
If we compare it to the GPU benchmarks in table 7, we see the fastest GPUs are faster, with the
fastest being the AMD Radeon RX 7900 XTX (using the RADV driver) which finished in 98ms on
the random dataset and 118ms on the worst case dataset. If we compare all the data, we make the
following observations:
â€¢ The GPUs are ranked in speed as RX 7900XTX > Tesla A100 > Tesla V100, which coincides
with them being released in 2022, 2020, and 2017 (in the same order).
â€¢ The RX 7900XTX has huge differences in performance between which driver is used, with
the RADV driver being 4-5 times faster than the AMDGPU Pro driver. What this suggests
is that the Nvidia GPUs might also have potential for some additional large performance
optimisations.
â€¢ In the GPU benchmarks, the worst case dataset was slower than the random dataset (with one
exception), which is the opposite as what we saw for the CPU benchmarks.
This suggests thatthe CPUs are more relianton eitherbranchprediction ormemory prefetching,
compared to GPUs.
We would also like to point out that since the Xeon Gold 6226R was released in 2020, the AMD
Radeon RX 7900XTX in 2022, and the Tesla A100 in 2020, this performance difference between the
CPU and GPU algorithm is not a result carried by one set of hardware being significantly newer
(and therefore faster) than the other. Similarly, the Xeon CPU has an Manufacturer Suggested Retail
Price (MSRP) of $1500, while the AMD GPU has a MSRP of $1000, so it also is not a problem of
some hardware being â€œa higher tierâ€ of product than the rest.
8.5 Optimising the CPU algorithm
There is one significant difference between the description of the ANSV algorithms and our imple-
mentations. For the analysis of both ANSV the original ANSV algorithm and our modified version,
it is optimal to have the subset size belg(n), such that we can utilize n
lg(n) threads. In practice, this
results in very poor performance sincelg(n)is tiny, so the overhead per thread is large, and we do
not even have hardware capable of anything close ton
lg(n) threads simultaneously. Instead, we put a
minimum on the subset size, so it is at least 8192 elements.
After fixing that, our preliminary testing showed that our CPU implementations were slightly
slower than the BSZ implementation. When we tried profiling our code to find potential speed-ups,
the profiling tools claimed that most of the time was spent during Linux system calls and during
codepaths of the Rayon library. In other words, it was hard to figure out what the bottlenecking code
was. For that reason, we tried replacing the Rayon library with a manual handling of threads using
the std::thread library from Rustâ€™s standard library, but this just resulted in the code running
much slower. We also tried just manually looking for places to optimise our code, but we didnâ€™t have
much luck.
So while our code is slightly slower than the BSZ implementation, we believe it is mostly related
to our choice of algorithm, programming language, and (if we believe the profiling output) library
selection.
Page 32 of 57

8.6 Optimising the GPU algorithms184009 25th June 2025
8.6 Optimising the GPU algorithm
We had a better time optimising the GPU algorithm. As the GPU we had the most readily available
was an AMD Radeon RX 7900XTX we optimized for that GPU. We used the RadeonGPUProfiler
software to visualize the time spent during different parts of the algorithm. Sadly, this software only
runs using the AMDGPU Pro diver, so we do not include profiling of the RADV driver. As seen
in fig. 8, the majority of the time is spent running kernel 3 and 6, so we focused our optimization
efforts there.
Additionally, there is also a lot of time spent seemingly doing nothing without any indication
of what is happening. Based on our experience of looking at the profiling output of different GPU
algorithms, and the fact that during the recursive part of the algorithm between kernel 1 and 3 the
time spent doing nothing is relatively small, we guess that it might be related to synchronizing large
amounts of workgroups and writing their outputs to memory, although we have no way to confirm
or deny our guess. It might also just be an artefact of the profiling process.
The first thing we notice regarding kernel 3 and 6 is that the yellow bar is very small. The yellow
bar signifies how â€œbusyâ€ the GPU is, that is how many workgroups are currently being processed at a
time. There is, however, the small detail that what it actually describes is how many workgroups are
loaded in the SIMDâ€™s cache. As we mentioned in section 7.2.2, each SIMD loads multiple workgroups
into its cache, but only processes one of them at a time. This means that the yellow bar being low is
a sign of being cache constrained, but not necessarily a sign that the hardware is inactive. But while
the hardware might not be inactive, it is still a sign that we are cache constrained, so we focus our
effort on optimising the cache usage of kernel 3 and 6.
We first tried to just write better code. This is what caused kernel 3 to implement the min-tree
using a two-layer tree instead of a binary tree (see section 7.5.3). The second step was to optimise the
subset size. Just like with the CPU algorithm, we do not have hardware capable of running anything
close to n
lg(n) threads simultaneously, so our initial implementation was set to have a large subset size.
What this resulted in was the GPU crashing. After some testing, we found that using the AMDGPU
Pro driver runs out of cache (and crashes) if we set the subset size to anything higher than 1000
elements. We then tried different subset sizes, and found anything aroudn 400 to be optimal.
While the number 400 is not special we can still reason why anything close to it would be optimal.
For smaller workgroup sizes, we need to remember that in kernel 6 we run two binary searches per
thread. Since the input to the kernels is two subarrays, the binary search uses up toâŒˆlg(2 Â·400)âŒ‰ = 10
steps, which means that each thread performs at least something multiplied by 10 operations. When
we then split the 800 elements between the 32 threads in a workgroup, we see that
 800
32

= 25which
means that each thread only calculates 25 elements of the output. If the subset size was any smaller,
the time spent running the binary search amortized over the number of output elements per thread
would be even higher.
The reason why we do not increase the workgroup size beyond 400 is because of another thing
we mentioned in section 7.2.2: That multiple SIMDs share the same LDS cache. So while wecould
increase the workgroup size, it would result in some SIMDs not having enough cache space to store
its input. Those SIMDs would therefore do nothing, resulting in actual hardware sitting idle (unlike
when we just couldnâ€™t load multiple workgroups into each SIMD).
The third thing we tried, was to replace the RADV driver with the AMDGPU Pro driver. While
the profiling tool only supports the AMDGPU Pro driver due to both being made by AMD, the
RADV driver is made by the community and it has a reputation of being much better, so it was an
obvious last thing to try. We first found that the RADV only crashes when the subset size becomes
2500 elements. This is 2.5 times larger than with the AMDGPU Pro driver. We still, however, found
Page 33 of 57

8.6 Optimising the GPU algorithms184009 25th June 2025
the optimal subset size to be around 400. While we canâ€™t confirm it due to the profiling tools not
supporting and RADV driver, this increase in the maximum subset size suggests that the RADV
driver is somehow much more cache-efficient compared to the AMDGPU Pro driver. But whether it
is due to cache-efficiency or not, the data in table 7 clearly shows a massive speed-up by switching
drivers.
Copy data to GPU
Kernel 1
Kernel 3
Kernel 4
Kernel 6
Figure 8: A screenshot of part of the output when profiling the GPU ANSV algorithm using the
RadeonGPUProfiler software. The horizontal axis represents the time spent, and the vertical axis
represents how â€œbusyâ€ the GPU is measured as the number of workgroups actively running on the
hardware. The unmarked parts are the recursive call replacing kernel 2 in-between kernel 1 and 3,
and kernel 5 which is inbetween kernel 4 and 6.
Page 34 of 57

s184009 25th June 2025
Part II
Range Minimum Query
Now that we have an efficient GPU algorithm for solving the ANSV problem, it would be interesting
to see if we could utilize it to solve another problem. One candidate for such a problem is the
range minimum query problem (henceforth RMQ). RMQ is used in the parallelised Lempel-Ziv-77
algorithm5, and it is mentioned in (Berkman et al., 1993) that the ANSV of an array can be used to
solve RMQ.6
Definition 8.1(Range Minimum Query).Given an arrayAofnintegers, answer questions of the
form RMQ(i, j) = arg minA[k]for iâ‰¤kâ‰¤j . In words, find the minimum element in the range[i; j]
ofA.
The way to solve RMQ via ANSV is by turning it into a Cartesian tree, and then answering
Lowest Common Ancestor queries (henceforth LCA) in the Cartesian tree.7
Definition 8.2(Cartesian Tree).Given an array A of n distinct numbers, the Cartesian tree ofA
is defined using the following recursive procedure:
1. Find the smallest element inA. This is the root element,r.
2. Split A into two subarrays. The left subarray isA[0 . . . râˆ’ 1]. The right subarray isA[r +
1. . . nâˆ’1].
3. Construct the Cartesian tree of both subarrays.
4. Place an edge betweenr and the root of the Cartesian tree constructed from the left subarray.
5. Likewise, place an edge betweenr and the root of the Cartesian tree constructed from the right
subarray.
Definition 8.3(Lowest Common Ancestor).Given a tree T and two nodesa and b. The lowest
common ancestor ofa and b is the shared ancestor ofa and b which is the furthest away from the
root ofT.
It is very easy to convert ANSV into a Cartesian tree, and it is also trivial to parallelise. All we
have to do it iterate over the ANSV array, and for each element, pick whichever of its left and right
nearest smaller values that is the largest.
The question therefore becomes how to find the LCA in constant time.
9 Answering LCA in constant time
We have decided to use the method described in (Schieber & Vishkin,1988) to solve the LCA problem.
For proof that it works, and an explanation of why, we refer to the article. The article describes a
preprocessing stage where four values are calculated for each node in the Cartesian tree. Along with
a lookup table, these four values enables answering LCA queries in constant time.
The four precomputed values are:
5Shun and Zhao, 2013b, p. 126.
6Berkman et al., 1993, p. 364.
7Berkman et al., 1993, p. 364.
Page 35 of 57

9.1 How to parallelise the precomputations184009 25th June 2025
â€¢ Preorder: This is simply the index of the node in the preorder traversal of the tree, with the
root node having index one.
â€¢Level: This is the nodeâ€™s distance from the root node.
â€¢ Inlabel: This is whatever preorder value amongst the node itself and its children that has the
most consecutive zeroes in its binary representation counting from the right side.
So if a node has e.g. preorder value 3 and it has 4 children, then the preorder of those would
be 3 through 7 and we would look at the values 011, 100, 101, 110, and 111, of which 100 has
the most consecutive zeroes from the right side. Node 3â€™s inlabel would therefore be 100.
â€¢ Ascendant: This value stores the inlabel of all of its ancestors in a single integer. It does
this by taking the rightmost one-bit from each of its ancestors (including itself), and otherwise
letting all other bits be zero.
The lookup table is calledheadand it maps each inlabel value to whichever node containing said
inlabel has the lowest level. Due to the inlabel being the preorder value of a descendant node, the
nodes sharing the inlabel value form a path, meaning that â€œwhichever node containing said inlabel
has the lowest levelâ€ is a single unique node.
9.1 How to parallelise the precomputation
Parallelising computing all of the four precomputed values relies on parallelising the list ranking
problem.
Definition 9.1(List Ranking).Given a linked list, determine the rank of each element in the list,
that is the distance from the first element.
For now, we assume that we already have a parallel list ranking algorithm. The list ranking
problem has a set of implicit weights, in that the distance between each element is one. We make
these weights explicit by assigning a weight to each element denoting the distance to the next element.
The tree that we receive is defined by each node having an edge to its parent. The first thing we
do is convert this tree to an euler tour, by adding an edge in the opposite direction. We then define
two arrays,W1 and W2, which contains the weights of edge.W1[e] = 1if e is directed away from the
root node, whileW1[e] = 0when it is directed towards the root node.W2[e] = 1if e is directed away
from the root node, whileW2[e] =âˆ’1when it is directed towards the root node.
We then run the list ranking algorithm twice using these weights to get the distancesD1 and
D2. The preorder of a node, u, is the first distance of the edge from u to its first child, v, so
preorder[u] = D1[uâ†’v ]+1. Thelevelistheseconddistancetothefirstchild,so level[u] = D2[uâ†’v ].
If we letp be the parent node ofu, we can calculate the inlabel asinlabel[u] = 2i
j
D1[uâ†’p]+1
2i
k
with
i=âŒŠlg(D 1[uâ†’v] xor(D 1[uâ†’p] + 1))âŒ‹(withxorbeing the bitwise xor operator).
We now define a new set of weightsW3 where, for each node that is not the root node and
where inlabel[u] Ì¸= inlabel[p], then W3[pâ†’u ] = 2 i and W3[uâ†’p ] = âˆ’2i, with i being the index
of the rightmost one-bit in inlabel[u]. The weights of all other edges are zero. We run the list
ranking algorithm withW3 and find the distancesD3. We can now calculate the ascendant values
as ascendant[u] = D3[uâ†’v ] + 2l with l being the number of bits required to store the number of
nodes in the input tree.
The final thing we need to calculate is the head table. We do this by having each node write
head[inlabel[u]] = u if inlabel[u] Ì¸= inlabel[p], or if it is the root node. This is trivial to parallelise
as all the calculations are independent of each other.
Page 36 of 57

9.2 How to answer LCA queriess184009 25th June 2025
9.2 How to answer LCA queries
As we mentioned earlier, the nodes that share the same inlabel value form a path. The logic behind
answering LCA queries relies on this fact, as what we do is first find he inlabel value of the LCA,
and then figuring out exactly which node is the LCA.
When we try to answer the LCA query for two nodesx and y, the first thing we check is if they
share the same inlabel value. If that is the case then one of them must be the ancestor of the other,
so the LCA must be whichever one has the lowest level value. If it is not the case, we try to determine
the inlabel of the LCA, which we will refer to asz.
We remind the reader thatl is the number of bits required to store the number of nodes in the
input three. This also means thatl bits is large enough to store either of preorder, level, inorder, and
ascendant for any given node, which is why we assume that all the values arel bit integers. It is find
in practice to use more thanl bits if we just pad the leftmost bits with zeroes, in which case the
reader should replace any reference to e.g.lâˆ’i with the number of bits used minusi. Back to the
explanation.
We first letibe the maximum of the following three indices:
â€¢The index of the leftmost bit in whichinlabel[x]andinlabel[y]differ.
â€¢The index of the rightmost one-bit ininlabel[x].
â€¢The index of the rightmost one-bit ininlabel[y].
We then letj be the index of the rightmost one-bit in amongst the leftmostlâˆ’i + 1bits of
ancestor[x] &ancestor [y](with & being the bitwise and operator).inlabel[z]is then thelâˆ’j leftmost
bits ofinlabel[x]followed by one one-bit andjzero-bits.
The next step is forx (and y) to find its lowest ancestor which has the same inlabel value asz,
named xV. We do this by lettingk be the leftmost one-bit in the rightmostj bits of ascendant[x]. xV
is then the parent of the head of the path of nodes with their inlabel being thelâˆ’k leftmost bits of
inlabel[x]followed by one one-bit andk zero-bits. It is also possible thatx (or y) already has the
same inlabel asz, in which case we skip this calculation and letxV=xdirectly.
Once xVand yVhave been found,z, that is the LCA ofx and y, is just whichever one of them has
the lowest level value.
To better understand this process, lets look at fig. 9, which is an example of a tree with its
precomputed values. If we want to find the LCA of node 3,x, and node 6, y, (named by their
preorder value), we start by seeing that they do not share an inlabel value. Therefore we calculate
the following:
â€¢i= 2, sinceinlabel[x]andinlabel[y]differ in their third least significant bit.
â€¢j = 2,since the third least significant bit is the rightmost one-bit inancestor[x] &ancestor [y] =
011 & 110 = 100.
â€¢ Regarding xV, the leftmost one-bit amongst the rightmostj bits of ascendant[x]is the bit with
index 0, sok = 0. We therefore look at the head node of the inlabel path with value011, which
isxitself. Its parent is node 2, which isxV.
â€¢ Regarding yV, the leftmost one-bit amongst the rightmostj bits of ascendant[x]is the bit with
index 1, sok = 1. We therefore look at the head node of the inlabel path with value110, which
is node 5. Its parent is node 1, which isyV.
â€¢ Since, level[xV] = 1 > 0 = level[yV], then yV(i.e. node 1) must be the LCA of node 3 and node 6.
Page 37 of 57

9.3 Parallel list rankings184009 25th June 2025
1,0
100,100
2,1
100,100
3,2
011,101
4,2
100,100
5,1
110,110
6,2
110,110
7,2
111,111
Figure 9: An example of a tree, with each node having the four values preorder, level, inlabel, and
ancestor. Inlabel and ancestor are given in binary form. Each inlabel path is given a colour.
9.3 Parallel list ranking
To solve the list ranking problem in parallel, and with an algorithm suitable for a GPU, we found
Rehman et al., 2009 which describes the Recursive Helman-JÃ¡JÃ¡ algorithm.
To explain the Helman-JÃ¡JÃ¡ algorithm, it is easier to first look at a parallel prefix sum algorithm.
9.3.1 Parallel prefix sum
The prefix sum calculation can be solved in four easy steps.
1. We split the array into equal sized parts, one for each thread.
2. Each thread calculates the sum of the elements in its part.
3. One thread calculates the prefix sum of the part sums.
4. Thread k calculates the prefix sum ofpartk,butinsteadofinitializing the prefix sum calculation
withzero,itinitializesitwiththe kâ€™thvaluefrom theprefixsum ofthepartsums. Thiseffectively
adds the sum of all the elements earlier in the array to the prefix sum of the part itself.
To parallelise the algorithm further, step 3 can be replaced with calling the algorithm recursively.
9.3.2 Helman-JÃ¡JÃ¡
The concept behind the Helman-JÃ¡JÃ¡ algorithm is the same as that of the parallel prefix sum
algorithm.
1. Split the linked list into smaller parts.
2. Calculate the sum of each part.
3. Construct a linked list of the part sums.
4. Solve the list ranking problem for the part sums.
Page 38 of 57

s184009 25th June 2025
5. Solve the list ranking problem for each part, but add the rank of the part sum to all the
elements.
The difference here is that since the input is a linked list instead of an array, it is no longer
possible to split the list into equal sized parts. What we do instead is to randomly decide on elements
that form the start of each part. We then assign a thread to start at each start-element, and as it
follows the linked list to calculate the prefix sum of its part, when it encounters another start-element,
it stops.
This is a way to split the linked list into parts, but it has two downsides. The first downside is
that we cannot rely on randomly picking the first element of the linked list to be a start-element.
There are multiple ways to solve this problem, but in our case, we rely on the fact that it is easy to
find the last element of the list, since it is the only element that does not point to the list, and that
since the input is an Euler tour that we construct ourselves, we can just cleverly construct it such
that we can find the first element given the last element. This will be explained later.
The second downside is that fact that we do not have a guarantee for the length of each part of
the linked list, and it might therefore end up running in linear time despite our best efforts.
10 LCA on a GPU
Now that we have a method for answering LCA queries, we will start describing the individual parts
necessary for an GPU implementation, after which we will assemble them together into an algorithm.
10.1 Kernel 1: Convert ANSV into a Cartesian tree
To convert the ANSV output into a Cartesian tree, all we have to do is to have each element point to
whichever of its nearest smaller values has the largest value. For this we require the input to be the
indices of the left and right matches from the ANSV algorithm along with the original array itself,
so that we can read the values at those indices.
Since each element is independent of each other, we will not group the threads by workgroups.
Instead thread t in workgroup k calculates its global index, that is32k + t, and handles that ele-
ment. Henceforth, whenever we refer to threadsâ€™ global indices, it is because their calculations are
independent of each other, so we ignore their workgroups.
This kernel obviously performsO(n)work and hasO(1)span.
10.2 Kernel 2: Count children part 1
All Cartesian trees share two properties:
â€¢Each node has at most two children.
â€¢ If a node has two children, one child is to the left of the node and the other is to the right of
the node.
Since each child of a node has the edge to the node, it knows both its own index and the index of
the node. It therefore knows if it is the left child or the right child of the node.
This kernel sets a flag for each child of a node, which enables us to count the number of children.
The input is the Cartesian tree.
We initialise an array of zeroes containing two elements per node in the Cartesian tree. Threadt
(global index) checks if nodet is the left or right child of its parent,p. If it is the left child, it writes
a one to index2p. If it is the right child it writes a one to index2p+ 1.
This obviously performsO(n)work and hasO(1)span.
Page 39 of 57

10.3 Kernel 3: Count children part 2s184009 25th June 2025
10.3 Kernel 3: Count children part 2
This kernel counts the number of children based on the information provided by kernel 2.
The input is the arraychild f lagsproduced by kernel 2.
We initialise an array of the same size as the Cartesian tree. Threadt (global index) calculates
child f lags[2t]+ child f lags[2t+1]and writes it to indext in the output array. This array now contains
the number of children of each node.
This obviously performsO(n)work and hasO(1)span.
10.4 Kernel 4: Uniform add
An uniform add kernel is a kernel that adds a predetermined value to all elements in an array.
For our purpose, it is convenient to implement it such that instead of the value being a single
constant, we let each workgroup add an unique value to the elements in its assigned part of the array.
The input is therefore the data array and an array containing the values to add. The size of the
part assigned has so far been given implicitly to all kernels, so we do not consider it part of the
explicit input.
This kernel obviously performsO(m)work and hasO(m)span.
10.5 Kernel 5: Workgroup-local prefix sum
To implement a prefix sum algorithm on a GPU as described in section 9.3.1, all we need is a kernel
that calculates the prefix sum of individual parts of an array, along with an uniform add kernel (as
described previously).
For this kernel, workgroupk is assigned part k which it splits into 32 subparts. Threadt in
the workgroup then calculates the sum of subpartt and saves it to indext in a small 32-element
long array shared only locally between the threads in the workgroup. The threads then synchronize.
Thread t then calculates the sum of sums 0 totâˆ’ 1, and uses the result as the initial value for
calculating the prefix sum of subpartt.
The input to this kernel is an array of data.
This kernel obviously performsO(m)work and hasO(m)span.
10.6 Kernel 6: Convert Cartesian tree to Euler tour
This kernel has the task of converting an array containing a Cartesian tree into an array containing
an Euler tour of said tree.
The properties we mentioned in kernel 2 are also essential here in order to deterministically create
an array describing an Euler tour through a Cartesian tree.
The array containing the Cartesian tree is actually a set of edges, since if the value at indexu is
v, then there must be an edgeuâ†’v . For an Euler tour of a tree, this means that each node has an
edge down to each of its children along with an edge back up to its parent. A node withc children
therefore needsc+ 1elements in the Euler tour array.
Since the Cartesian tree array is constructed such that a child knows its parent but the parent
does not know its child, we need a way to deterministically construct the edges from the parent down
to its child parallel. Since each node has at most two children, we need to handle three cases for how
to write the edges:
â€¢Zero children:The thread assigned to the node itself writes to its first element the index of
its parent.
Page 40 of 57

10.6 Kernel 6: Convert Cartesian tree to Euler tours184009 25th June 2025
Parent/root
Node
Left child Right child
Figure 10: Illustration of the number of the Euler tour of a Cartesian tree with four nodes. This
shows how many elements (squares) are assigned to each node based on the number of children, and
how these elements are connected by indices (arrows) pointing to the next element to form an Euler
tour.
â€¢ One child:The thread assigned to the child writes to the nodeâ€™s first index the index of the
child.
The thread assigned to the node itself writes to its second element the index of its parent.
â€¢ Two children:The thread assigned to the left child writes to the nodeâ€™s first index the index
of the child. It knows that it is the left child since the index of the child is less that the index
of the node.
The thread assigned to the right child writes to the nodeâ€™s second index the index of the child.
It knows that it is the right child since the index of the child is greater that the index of the
node.
The thread assigned to the node itself writes to its third element the index of its parent.
There is an additional set of rules for when we mention â€œthe index of its parentâ€ depending on how
many children the parent has.
â€¢One child:â€œThe index of its parentâ€ is the parentâ€™s second index.
â€¢Two children:â€œThe index of its parentâ€ is the parentâ€™s third index.
If we apply these rules, we achieve an Euler tour such as e.g. the one shown in fig. 10, where
the parent has one child, the â€œnodeâ€ has two children denoted left and right child, and the â€œchildrenâ€
each have no children of their own.
Now that we have a method to construct an Euler tour from a Cartesian tree, we just need to
show how to get the relevant information needed. That is the number of children of each node, and
the index in the Euler tour array at which the edges related to a specific node is written.
The input to the algorithm is the Cartesian tree and the prefix sum of an array containing
the number of edges going out of each node (henceforthpref ixsumedges). Node i has its desig-
nated elements in the Euler tour starting from indexpref ixsumedges[i]. The number of children
that node i has can be calculated aspref ixsumedges[i] âˆ’pref ixsumedges [iâˆ’ 1]if i > 0, and as
pref ixsumedges[i]ifi= 0.
For the CPU to be able to tell the GPU to initialise the Euler tour array, we need to know the
length of the array. To avoid having to transfer data from the GPU to the CPU, we note that the
number of elements (edges) in the Euler tour array must be twice the number of elements in the
Cartesian tree minus one. The reason we know this is that since the Euler tour is based on a tree,
each node has one edge from its parent and one edge back to its parent. This accounts for all edges
Page 41 of 57

10.7 Kernel 7: Find starting edge in Euler tours184009 25th June 2025
in the Euler tour. The only exception is the root node, which does not have a parent, but still has
one edge pointing to outside the array, signalling that it is the end of the path.
While the logic behind the method might be complex, the work needed to be done is a con-
stant amount of calculations and comparisons and it is independent between threads. We therefore
implement it with the threads using their global index.
This results in the work performed by a workgroup beingO(n)and the span beingO(1).
10.7 Kernel 7: Find starting edge in Euler tour
This kernel has the job of finding the index of the first edge in the Euler tour. The input is the
Cartesian tree andpref ixsumedges.
All we do in this kernel is have each thread (using global thread indexes) search for the edge that
points outside the array, which must be the root node in the Cartesian tree. Given that the root
node has indexi, the starting edge of the Euler tour must have indexpref ixsumedges[iâˆ’ 1]if i > 0
and index 0 ifi= 0.
This kernel obviously performsO(n)work and hasO(1)span.
10.8 Kernel 8: Initialise weightsW 1 andW 2
Kernel 8 initialises an array with the weights we need for list ranking to calculate preorder, level,
and inlabel. For that we need to know which edges go away from the root and which go towards the
root.
The input to the kernel ispref ixsumedges along with the two values that should be assigned to
the edges going away from and towards the root node.
Thread t (global index) finds that the index of nodet is idx = pref ixsumedges[tâˆ’ 1]if t > 0
and idx = 0if t = 0, and that the number of edges that nodet has is |edges| = pref ixsumedges[i] âˆ’
pref ixsumedges[iâˆ’ 1]if i > 0and |edges| = pref ixsumedges[i]if i = 0. All the edges except the last
one go down to the children, so the thread writes the down-value to elementsidx to idx + |edges| âˆ’ 2
in the output array, and the up value to elementidx+|edges| âˆ’1.
Since the number of edges out of each node is at most three, each thread performs constant work.
The work is thereforeO(n)and the span isO(1).
10.9 Kernel 9: Splitting the linked list
Kernel 9 handles splitting the linked list for the list ranking algorithm into parts.
The input is the index of the start-element of the linked list.
Each thread uses its global index to determine which part of the array containing the linked list
that it is responsible for. It then selects a random number in that range, which is the start-element
of the part. That number is then written to an output array.
The only exception is when the threadâ€™s part contains the start-element for the entire linked list,
then it specifically does not pick that element. Thread 0 then ignores its own part, and picks the
start-element of the linked list. This ensures that the entire linked list is going to be processed by the
list ranking algorithm, and it makes it easier to know the index of the start element for any recursive
calls to the linked list algorithm.
This kernel performsO(n)work and hasO(1)span.
Page 42 of 57

10.10 Kernel 10: List ranking - calculating part weightss184009 25th June 2025
10.10 Kernel 10: List ranking - calculating part weights
Kernel 10 handles the first half of the list ranking algorithm. Its job is to run through each part of
the linked list, calculate its total weight and create a new linked list with each part contracted into
one element.
The input is the linked list, weights for the edges, and the indices of the start-elements of each
part.
Thread t (global index) starts at start-elementt of the linked list and calculates the sum of the
weights along the linked list until it reaches another start-element. It then writes the total weight
of the part to the output array, along with the index of the other start-element that it found at the
end of the part.
This kernel performsO(n)work, and the span is the length of the longest part.
10.11 Kernel 11: List ranking - calculating element ranks
Kernel 11 handles the second half of the list ranking algorithm. Its job is to take the global rank of
the start-element of the part, and calculate the global ranks of all elements in the part.
The input is the linked list, weights for the edges, ranks of the start-elements of the parts, and
the indices of the start-elements of each part.
Thread t (global index) starts at start-elementt and writes its rank to the output. It then follows
the linked list and calculates the ranks of the elements in the part by adding their weights to the
rank of the start-element.
This kernel performsO(n)work, and the span is the length of the longest part.
10.12 Kernel 12: Calculate preorder, level, and inlabel
The input to this kernel ispref ixsumedges, D1, and D2, which it uses to calculate preorder, level,
and inlabel using the formulas described in section 9.1.
This kernel performsO(n)and has O(1)span, since the calculations are independent between
the nodes and it performs a constant number of calculations per node.
10.13 Kernel 13: Initialise weights for calculating ascendant
Kernel 13 initialises an array with the weightsW3.
The input is the Cartesian tree, the Euler tour,pref ixsumedges, and the inlabel array.
It implements the weights as describes by the logic in section 9.1. Since the work is independent
between threads, we use the global thread index. Threadt uses the Cartesian tree to find the parent
p of node t. It then compares the inlabels of nodest and p to know what values should be set as the
weights of the edges fromp to t and back. The only thing left is to find index the edge fromp to t
in the Euler tour.
Due to the way we constructed the Euler tour, we have obtained the property that end index of
the edge from a node to its parent is exactly one larger than the index of the parent to the node.
This means that to find the index of the edge from the parent to the node, we use the formula
eulertour[pref ixsumedges[t]âˆ’1]âˆ’1.
The only exception is ift is the root node, but we detect that when we look for the parent using
the Cartesian tree. In this case we just do nothing since there are no edges that need weights.
Since we perform a constant number of operations per thread, the work isO(n)and the span is
O(1).
Page 43 of 57

10.14 Kernel 14: Calculate ascendants184009 25th June 2025
10.14 Kernel 14: Calculate ascendant
Kernel 14 calculates the ascendant values of the nodes.
It needs three input values:pref ixsumedges, D3, and2 l which the CPU has precomputed using
its knowledge about the size of the Cartesian tree.
Thread t (global index) calculates ascendant[t] = D3[pref ixsumedges[tâˆ’ 1]] + 2l if t > 0or
ascendant[0] =D 3[0] + 2l ift= 0.
This kernel obviously performsO(n)work and its span isO(1).
10.15 Kernel 15: Calculate head
Kernel 15 fills the head table.
Its input is the Cartesian tree and the inlabel array.
Thread t (global index) checks if nodet is not the root node, and if its parent has a different
inlabel that itself. In this case it writestat indexinlabel[t]in the head table.
This kernel obviously performsO(n)work and its span isO(1).
10.16 Running the entire algorithm
Running the RMQ algorithm is a little more complex than running the ANSV algorithm, since it
has more kernels and multiple parts are run recursively. The algorithm is run as follows:
1. Dispatch kernel 1 with
 n
32

workgroups forntotal threads.
cartesian_tree = kernel1(ansv)
2. Dispatch kernel 2 with
 n
32

workgroups forntotal threads.
childflags = kernel2(cartesian_tree)
3. Dispatch kernel 3 with
 n
32

workgroups forntotal threads.
num_children = kernel3(childflags)
4. Dispatch kernel 4 with
 n
32

workgroups forntotal threads.
We initialise an arrayalloneswith
 n
32

ones.
num_edges = kernel4(num_children, allones)
5. Kernel 5 calculates the workgroup-local prefix sum. To calculate the global prefix sum, we
use the method described in section 9.3.1. We only show it with one level of recursion, but it
should continue until one workgroup handles the entire prefix sum at the innermost layer of
the recursion.
For one level of recursion where each workgroup handlesm1 elements, we:
â€¢Dispatch kernel 5 with
l
n
m1
m
workgroups, withm < nâ‰¤m 2
1.
â€¢Dispatch kernel 5 with one workgroup.
â€¢Dispatch kernel 4 with
l
n
m1
m
workgroups.
prefix_sum_num_edges_1 = kernel5(num_edges)
prefix_sum_num_edges_2 = kernel5(prefix_sum_num_edges_1)
prefix_sum_num_edges = kernel4(prefix_sum_num_edges_1, prefix_sum_num_edges_2)
6. Dispatch kernel 6 with
 n
32

workgroups forntotal threads.
euler_tour = kernel6(cartesian_tree, prefix_sum_num_edges)
Page 44 of 57

10.16 Running the entire algorithms184009 25th June 2025
7. Dispatch kernel 7 with
 n
32

workgroups forntotal threads.
euler_tour_root = kernel7(cartesian_tree, prefix_sum_num_edges)
8. Dispatch kernel 8 with
 n
32

workgroups forntotal threads.
W1 = kernel8(cartesian_tree, prefix_sum_num_edges, 0, 1)
W2 = kernel8(cartesian_tree, prefix_sum_num_edges, -1, 1)
9. Dispatch kernel 9 with
l
n
m2
m
workgroups for
l
n
m2
m
total threads.
The threads implicitly knowsm2, so they can calculate indices of the part that it splits without
needing further input.
splits = kernel9(euler_tour_root)
10. Kernel 10 and 11 performs the list ranking algorithm described in section 9.3.2. We only show
it with one level of recursion, but the recursion should continue until one workgroup calculates
the ranks for the entire list.
For one level of recursion where each workgroup handlesm2 elements, we:
â€¢Dispatch kernel 10 with
l
n
m2
m
workgroups, withm < nâ‰¤m 2
2.
â€¢Dispatch kernel 10 with one workgroup.
â€¢Dispatch kernel 11 with
l
n
m2
m
workgroups.
We initialise an arrayallzeroeswith
l
n
m2
m
zeroes.
part_linked_list, part_weights = kernel10(splits, W1, euler_tour,
euler_tour_root)
TheCPUcalculatesthestart-element/rootof part_linked_listfrom mand euler_tour_root.
part_splits = kernel9(part_linked_list_root)
part_ranks = kernel11(part_splits, part_weights, part_linked_list, allzeroes)
D1 = kernel11(splits, W1, linked_list, part_ranks)
All of this is then run twice. Once with the weightsW1, and once withW2 in order to calculate
D2.
11. Dispatch kernel 12 with
 n
32

workgroups forntotal threads.
preorder, level, inlabel = kernel12(prefix_sum_num_edges, D1, D2)
12. Dispatch kernel 13 with
 n
32

workgroups forntotal threads.
W3 = kernel13(cartesian_tree, euler_tour, prefix_sum_num_edges, inlabel)
13. Repeat step 10. but with the weightsW3in order to calculateD3.
14. Dispatch kernel 14 with
 n
32

workgroups forntotal threads.
The CPU calculates2l using its knowledge ofn.
ascendant = kernel14(prefix_sum_num_edges, D3, 2^l)
15. Dispatch kernel 15 with
 n
32

workgroups forntotal threads.
head = kernel15(cartesian_tree, inlabel)
Despite us describing that for many of the kernels each thread thread performs the calculations
related to one node in the Cartesian tree, in practice we actually have each thread handle a constant
number of calculations in order to reduce the overhead of spawning a thread.
Page 45 of 57

10.17 Correctnesss184009 25th June 2025
10.17 Correctness
The method we use to answer LCA queries and the list ranking algorithm are both described and
proven to be correct in their respective articles. Instead, we need to show that our many steps of
running small kernels actually run those algorithms.
The necessary argument for each kernel are already given in the description of the kernels, so
here we will describe what the kernels do when put together.
Kernel 1 converts the ANSV result to a Cartesian tree as described in (Berkman et al., 1993).
Kernels 2 and 3 counts the number of children of each node as we showed in the description of
kernel 2.
Kernel 4 then adds one to the number of children to get the number of edges out of a node,
according to the description of kernel 6.
Kernel 5 is then used together with kernel 4 to calculate the prefix sum of the number of edges
out of each node. This data can then be used in a kernel to calculate both the number of edges out
from a node along with those edgesâ€™ along with unique indices for each of those edges.
Kernel 6 then constructs an Euler tour using the prefix sum of the number of edges out of each
node.
So far kernel 1 through 6 has been used to calculate the Cartesian tree and an Euler tour through
said tree. (Schieber & Vishkin, 1988) then describes that to answer LCA queries, we need to set up
specific weights for the edges in the Euler tour and run a list ranking algorithm using those weights.
Kernel 8 sets the first two sets of weights, and kernels 9, 10, and 11 then runs the recursive
Helman-JÃ¡JÃ¡ algorithm as described in (Rehman et al., 2009) in order to solve the list ranking
problem.
Kernel 12 can then calculate the preorder, level, and inlabel arrays as described in (Schieber &
Vishkin, 1988).
Kernel 13 can then use the inlabel array to set up the third set of weights, after which we once
more run the list ranking algorithm.
Kernel 14 and 15 then calculates the ascendant array and the head table as described in (Schieber
& Vishkin, 1988).
This shows that while the algorithm has been split into many small parts in order to fit with the
architecture of a GPU, it is still the same algorithm and it is therefore still correct.
10.18 Work and span
Before we analyse the algorithm, we would like to point out that almost the entire algorithm can run
on n
log(n) threads performing O(n)work with O(log2(n))span, just like the ANSV algorithm. There
is an exception, however, which is when we run the list ranking algorithm. In (Helman & JÃ¡JÃ¡, 2001)
Helman and JÃ¡JÃ¡ analysed the list ranking algorithm and their analysis only holds when the total
number of threads,p, satisfiesn > p 2 ln(n).8 That is ifp <
q
n
ln(n).
We will therefore limit the analysis to using
q
n
ln(n) âˆ’ 1threads, despite most parts supporting
further parallelism.
Furthermore, the analysis assumes that the array is split intos parts, with each thread handling
s
p parts.sneeds to beâ‰¥pln(n) + 1. So withp=
q
n
ln(n) âˆ’1we the lower bound onsis
sâ‰¥pln(n) + 1
8Helman and JÃ¡JÃ¡, 2001, Lemma 1, p. 270.
Page 46 of 57

10.18 Work and spans184009 25th June 2025
=
r n
ln(n) âˆ’1

ln(n) + 1
=
p
nln(n)âˆ’ln(n) + 1.
We decide ons=
p
nln(n)for our analysis of the list ranking algorithm.
Work
Kernels 1, 2, 3, 6, 7, 8, 12, 13, 14, and 15 are all only run once on an input of lengthO(n). Since the
amount of work performed per input element isO(1), the total work isO(n). The remaining kernels
are therefore 4, 5, 9, 10, and 11.
Kernel 4 is first run once to convertnum_children into num_edges, during which it performs
O(n)work.
Kernel 4 and 5 are then run recursively. Given an input size ofn and that we assignm elements
to each workgroup, each iteration of the recursion will reduce the input size of the next iteration to n
m

. Since we dispatch
 n
m

workgroups per iteration, and each workgroup performsO(m)work in
each of the two kernels, we end up with the recurrence relationT (n) = T
  n
m

+ cn describing the
total work performed during the recursion.
If we assume that we havep =
q
n
ln(n) âˆ’ 1threads, and split n evenly between workgroups, we
get m = nq n
ln(n) âˆ’1

Â·32
elements per workgroup. Just like in section 7.5.9, we guessing that the work
isT(n)â‰¤2cnand plugging it into the recurrence.
Base case(n= 137)
T(137) =cby definition
2cn= 4câ‰¥c=T(137) =T(n).
Induction step
AssumeT(â„“)â‰¤2câ„“for137â‰¤â„“ < n.
T(n) =T
 n
m

+cn
=T
ï£«
ï£¬ï£¬ï£­
n
nq n
ln(n) âˆ’1

Â·32
ï£¶
ï£·ï£·ï£¸ +cn
â‰¤2c n
nq n
ln(n) âˆ’1

Â·32
+cn
= 2c
r n
ln(n) âˆ’1

Â·32 +cn
=c
 
64
âˆšnp
ln(n)
âˆ’64
!
+cn
â‰¤2cn=O(n)
We note that we assumed thatnâ‰¥137in order to make the inequality64
âˆšnâˆš
ln(n) âˆ’64â‰¤2nhold.
Page 47 of 57

10.18 Work and spans184009 25th June 2025
Theotherkernelsare9,10,and11,whichexecutetherecursiveHelman-JÃ¡JÃ¡listrankingalgorithm.
When we run kernel 9, we split the array into parts, and pick one start-element from each part. As we
mentioned above, we decided to set the number of parts tos =
p
nln(n). All three kernels perform
O(n)work per iteration, so we need solve the recurrence relationT (n) = T (s) + cn. We guess the
solution to beT(n)â‰¤2cnand try to solve it.
Base case(n= 9)
T(9) =cby definition
2cn= 4câ‰¥c=T(9) =T(n).
Induction step
AssumeT(â„“)â‰¤2câ„“for9â‰¤â„“ < n.
T(n) =T(s) +cn
â‰¤2cs+cn
= 2c
p
nln(n) +cn
â‰¤2c n
2 +cn
= 2cn
We note that we assumed thatnâ‰¥9in order to make the inequality
p
nln(n)â‰¤ n
2 hold.
To summarise, the work performed by the entire algorithm isO(n), meaning that it is work
optimal.
Span
Kernels 1, 2, 3, 6, 7, 8, 12, 13, 14, and 15 are all only run once on an input of sizeO(n), and all have
O(1)span assuming that we assigned one thread to each element. This means that since we haveq
n
ln(n) âˆ’ 1threads, if we distribute the elements equally between threads, fornâ‰¥ 9each thread
would handle nq n
ln(n) âˆ’1
elements, resulting in the span being
nq
n
ln(n) âˆ’1
â‰¤ 2nq
n
ln(n)
= 2n
âˆšnâˆš
ln(n)
= 2
p
nln(n)
=O
p
nlog(n)

.
The remaining kernels are once again 4, 5, 9, 10, and 11.
Kernel 4 is first run once withO(1)span on n threads, orO
p
nlog(n)

span on p threads, just
as the above mentioned kernels.
Page 48 of 57

10.18 Work and spans184009 25th June 2025
The recursion of kernel 4 and 5 performsO(m)work per workgroup, and since each iteration
reduces the input size to
 n
m

we get the recurrence T (n) = T
  n
m

+ cm. If we split the input
array equally, thenm = nq n
ln(n) âˆ’1

Â·32
and if we guess thatT (n) â‰¤ 64c
p
nln(n), we can solve the
recurrence.
Base case(n= 1905)
T(1905) =cby definition
64c
p
nln(n) = 64c
p
1905 ln(1905)â‰¥c=T(1905) =T(n)
Induction step
AssumeT(â„“)â‰¤64c
p
â„“ln(â„“)for1905â‰¤â„“ < n.
T(n) =T
 n
m

+cm
=T
ï£«
ï£¬ï£¬ï£­
n
nq n
ln(n) âˆ’1

Â·32
ï£¶
ï£·ï£·ï£¸ +cm
=T
r n
ln(n) âˆ’1

Â·32

+c
r n
ln(n) âˆ’1

Â·32
â‰¤64c
sr n
ln(n) âˆ’1

Â·32 ln
r n
ln(n) âˆ’1

Â·32

+c
r n
ln(n) âˆ’1

Â·32
â‰¤64c
r n
4 ln(n) + 32c
p
nln(n)
= 64c
p
nln(n) =O(
p
nlog(n))
We note that we assumed thatnâ‰¥ 1905in order to make the inequality
q
n
ln(n) âˆ’1

Â· 32 â‰¤ n
4 hold,
which at the same time also gives us the inequality
q
n
ln(n) âˆ’1â‰¤
p
nln(n).
For the analysis of the span of the recursion performed by kernels 9,10,and 11,we use the analysis
of Helman and JÃ¡JÃ¡ which states that when we split the linked list intos parts, with each of thep
threads handling s
p parts, then each thread handles at mostÎ±(s) n
p elements with high probability
(meaning with probability greater than1âˆ’n âˆ’Ïµ for someÏµ > 0),for some functionÎ± of s, Î±(s) â‰¥ 2.62.9
While Î±(s)does not look like a constant, Helman and JÃ¡JÃ¡ use the fact that each thread handles
Î± n
p elements to show that the computation time of the threads to traverse their respective parts of
the linked list isO

n
p

. We therefore assume thatÎ±(s)is a constant.
Explained in simpler terms, this all means that each thread handles approximately the same
number of elements.
Since we split the array intos parts, the span of the recursive list ranking algorithm can be
explained by the recurrence relationT (n) = T (s) + cÎ± n
p. If we guess thatT (n) = 2Î±c
p
nln(n) we
can solve the recurrence.
9Helman and JÃ¡JÃ¡, 2001, Lemma 1, p. 270.
Page 49 of 57

s184009 25th June 2025
Base case(n= 68)
T(68) =cby definition
2Î±c
p
nln(n) = 2Î±c
p
68 ln(68)â‰¥c=T(68) =T(n)
Induction step
AssumeT(â„“)â‰¤2Î±c
p
â„“ln(â„“)for68â‰¤â„“ < n.
T(n) =T(s) +cÎ± n
p
â‰¤2Î±c
p
sln(s) +cÎ± n
p
= 2Î±c
qp
nln(n) ln(
p
nln(n)) +cÎ± nq
n
ln(n) âˆ’1
= 2Î±c 4
p
nln(n)
q
ln(
p
nln(n)) +cÎ± nq
n
ln(n) âˆ’1
â‰¤2Î±c 4
p
nln(n)
p
ln(n) +cÎ± np
nln(n)
â‰¤2Î±c
âˆšn
2
p
ln(n) +c âˆšn Î±p
ln(n)
â‰¤2Î±c
p
nln(n) =O(
p
nlog(n))
We note that we assumed thatnâ‰¥ 1905in order to make the inequality4
p
nln(n)â‰¤
âˆšn
2 hold, which
at the same times also gives us the inequality
q
n
ln(n) âˆ’1â‰¤
p
nln(n).
Usingp=
q
n
ln(n) âˆ’1threads, the total span of the algorithm becomesO(
p
nlog(n)).
In (Rehman et al., 2009) it is claimed that if we letp = n
log(n) and split the linked list into inp
parts, the expected span can be expressed by the recurrence relationT (n) = T (p) + O

n
p

. Using
this, we would find the span of the recursive Helman-JÃ¡JÃ¡ algorithm to beO(log2(n)), which means
the entire algorithm would performO(n)work and haveO(log2(n))span on n
lg(n) threads.
In the article they do not, however, explain how they handle the fact that the lengths of the parts
of the linked list are dependent on each other. If one part is shorter than the average length, then
those missing elements must be present in other parts, making those longer than the average length.
Since they do not explain how they handle this issue, or even mention its existence, we decided to
not rely on this result.
11 Generalised RMQ
While the explanation of the algorithm itself is done, we have one final addition. If we use the ANSV
of an array with duplicated value as input to the RMQ algorithm, we do not necessarily find the
correct output. The reason is that we might end up with the LCA of two nodesa and b being outside
the interval[ a; b]. An example of this case can be seen in fig. 11, where the black and blue edges
form the Cartesian tree, so it can easily be seen that the LCA ofa and b is p, which is left of both
of them.
Page 50 of 57

11.1 Implementation and test setups184009 25th June 2025
Value
Index
p
a
c
b
d
Figure 11: The black and blue edges form the tree found when trying to construct a Cartesian tree by
running an ANSV algorithm. If we replace the blue edge with the green edge, we find a tree capable
of answering RMQ queries.
This issue only occurs when two nodesa and b, share their value, share a parentp, and they are
either both left or both right of the parent. In this case, when we want to answer the RMQ query for
a descendant ofa and a descendant ofb (or a and b themselves), the answer will always be the value
of a (or b since they are the same value), but the LCA that we find isp. To make the LCA become
a, we remove the edge fromb to p, and instead insert an edge fromb to a, letting a be the parent of
b. In fig. 11, this is when we replace the blue edge with the green edge.
Luckily, constructing such a Cartesian tree is easy. As we mentioned in section 5, (Berkman
et al., 1993)â€™s suggested method of solving the generalised ANSV problem is by enumerating all
the elements and using the index as a tie-breaker for whenever two element share the same value.
Since this would cause neighbouring elements which share the same value to find each other as the
nearest smaller value, but only in the direction of the lowest index, the article suggested running
the algorithm twice, once where the indices are increasing and once where they are decreasing. This
issue, however, is perfect for solving the generalised RMQ.
If we enumerate the elements, and only run the ANSV algorithm once, neighbouring elements
with the same value will form a directed path, and only one of them will have their parent be the
element that is the actual nearest smaller value of all the elements. Explained in another way, we
would get the tree from fig. 11 but with the green edge instead of the blue edge, just as we wanted. It
is therefore easy to solve the generalised RMQ using the algorithms that we have described so far.
11.1 Implementation and test setup
For our benchmarks we only have two implementations:
1. Our GPU algorithm.
2. Therange_minimum_queryRust crate (Rust terminology for package).10
We would have liked to implement the RMQ algorithm ourselves, but we ran out of time. This
serves as a substitution, since it claims to utilise a Cartesian tree to answer RMQ queries in
constant time. Sadly, it is not a parallelised algorithm.
When implementing our GPU algorithm, we had trouble implementing kernel 9. Specifically,
since RustGPU is still early in development, we couldnâ€™t find a way to generate the random numbers
10Petri, 2024.
Page 51 of 57

11.2 Test setups184009 25th June 2025
needed for kernel 9. Generating random number is a common feature available when using other GPU
code compilers, so it is not an issue with the algorithm itself. It is only with our implementation.
What we decided to do instead is to â€œrandomlyâ€ pick the first element of each part to be the
start-element of the part, with the second element being the backup start-element in case the global
start-element of the linked list is the first element in some part.11
11.2 Test setup
For our RMQ benchmarks we will be utilising the same datasets as we did for the ANSV benchmarks,
but with the number of elements decreased from 500 million to 50 million in order to accommodate
the larger memory usage of the RMQ algorithm.
For the GPU algorithm, since its input is the output of the ANSV algorithm, we first run the
GPU ANSV algorithm on the dataset, and then we run the GPU RMQ algorithm.
The input to the CPU algorithm is just an array of data, so we just input the dataset directly.
While the datasets might seem randomly chosen and no longer hand picked, they each still serve
two distinct functions.
â€¢ Since we couldnâ€™t randomly pick the start-elements in kernel 9, we could reintroduce the
randomness by letting the data itself be random. While we canâ€™t control how running the
ANSV algorithm followed by constructing a Cartesian tree and an Euler tour affects the
randomness of the input, we can at make it as random as possible by letting the input be
random data, i.e. the random dataset.
â€¢ The worst case dataset for the ANSV algorithm will actually be close to a best case dataset
for the RMQ algorithm. Due to the way the worst case dataset is constructed, it is actually a
bitonic array. The Cartesian tree of a bitonic array forms a single path, with the root element
being either the leftmost or rightmost element, and then the next element in the path is either
the rightmost or the leftmost element that is not already in the path.
When we then construct an Euler tour, all nodes in the Cartesian tree except the leaf node
will have two outgoing edges in the Euler tour, while the leaf node will have one outgoing edge.
Due to the exact way we construct the Euler tour (see section 10.6), each on the path from the
root to the leaf node, all even indices of the Euler tour in the range

0; n
2

and all odd indices
in the range
 n
2 ;n

will be traversed, with the indices from each range being traversed in sorted
order. On the path back from the leaf to the root, it is the odd indices in the range

0; n
2

and
the even indices in the range
 n
2 ;n

that are traversed.
Now, since we place the start-elements at regular intervals through the array, the maximum
distance between two start-elements will be twice the average. The reason is that if the space
between the start-elements is an even number of elements, then we will encounter all start-
elements in the range

0; n
2

on the path from the root to the leaf node, and then we encounter
all the start elements in the range
 n
2 ;n

on the path back to the root. If the space between
the start-elements is an odd number, then we encounter half of the start-elements from each
range on the path to the leaf node, and the other half on the path back to the root.
The hardware that we utilise is the AMD Ryzen 5 7600x CPU (since it is the fastest single-
threaded CPU we have available), and all the GPUs that we used for the ANSV benchmarks.
11https://xkcd.com/221/
Page 52 of 57

11.3 Resultss184009 25th June 2025
11.3 Results
The data that we have collected can be seen in table 8. We first notice that the RADV driver is no
longer much faster than the AMDGPU Pro driver. The relative performance difference between the
GPUs, however, is still the same as for the ANSV benchmarks, which tells is that the AMDGPU
Pro driver was just particularly bad at something that we did for the implementation of the ANSV
algorithm.
Since the purpose of the RMQ algorithm was to utilise the ANSV output to easily solve another
problem, this GPU implementation is a bit of a failure. Compared to running the GPU ANSV
algorithm, it takes about 7 to 11 times longer to process the ANSV output for answering RMQ
queries. As we will mention in the optimisations section, there are a few optimisations that we didnâ€™t
have time to implement, but overall, it might just be better to design an algorithm that solves the
RMQ problem from scratch, rather than trying to utilise the already computed ANSV output.
Additionally, if we compare it to the CPU version, the GPU algorithm is only 3 to 6 times faster.
And since the CPU version is sequential, it should be relatively easy to make it faster than the GPU
algorithm by parallelising it with the algorithm that we based our GPU algorithm on.
Our results reveal one good thing though. We analysed the algorithm using the assumption that
n > p 2 ln(n). The AMD Radeon RX 7900XTX GPU has 96 compute units, each with 4 SIMDs,
which handles workgroups with 32 threads. This means that in total, we have 12288 threads. This
means that the inequality if only satisfied whennâ‰ˆ 3,310,000,000, or6 .6times larger than the
maximum buffer size the GPU supports (not including the fact that the Euler tour buffer is up3n
long). Yet somehow, the data showed no significant difference in performance between the random
dataset and the worst case dataset (which is actually the best case dataset) where we do not rely
on the randomness analysis. This means that in practice, the recursive Helman-JÃ¡JÃ¡ list ranking
algorithm should scale much better than what we found theoretically.
11.4 Optimisations
When we tried optimising the RMQ GPU algorithm, we once again used the RadeonGPUProfiler.
As seen in fig. 12, it is very obvious that if this algorithm is become competitive with the CPU
algorithm, it must optimise the time it takes to perform list ranking.
The we started by optimising the size of the parts that the linked list are split into, and found
that 300 elements was close to optimal, but with a big exception. Since it is slow to dispatch GPU
kernels, we found that it was worth it to end the recursion when the length of the part-sums array
became 15,000 elements, and then have just a single thread compute the ranks sequentially. It might
be worth testing if it is faster to copy the last part-sums array to the CPU and have it compute the
ranks instead. We sadly didnâ€™t have time to test this.
Another easy optimisation is that the first two times we run the list ranking algorithm, first using
weights W1 and then usingW2, it might be possible to combine these into one. The reason why this
might be efficient, is that the main bottleneck for the list ranking algorithm is the memory access
latency, and not the computations themselves. If we therefore mergeW1 and W2 into one array with
it the output array alternating between elements fromW1 and elements fromW2, we might achieve
the exact same memory access latency, which halving the number of memory accesses, therefore
running the list ranking algorithm twice in the time it takes to run it once.
Sadly, the third instance of the list ranking algorithm depends on the output of the previous two,
so it cannot be combined with the other two.
Even if we implement all these optimisations we are unlikely to get much more than a 20%-30%
(based on the timeline in fig. 12 which is obtained while collecting profiling information, so it is very
Page 53 of 57

11.4 Optimisationss184009 25th June 2025
List ranking List ranking List ranking
Figure 12: A screenshot of part of the output when profiling the GPU RMQ algorithm using the
RadeonGPUProfiler software. The horizontal axis represents the time spent, and the vertical axis
represents how â€œbusyâ€ the GPU is measured as the number of workgroups actively running on the
hardware. The marked parts are where kernels 9, 10, and 11 are dispatched to run a list ranking
algorithm.
inaccurate) or maybe a 50% speed-up if we are very lucky. While this is a large speed-up, if we look
at the data in table 8, we see that the RMQ algorithm would still be much slower than th ANSV
algorithm. And it also wouldnâ€™t be much faster than a hypothetical parallelised CPU algorithm.
Our best guess of why the GPU algorithm performs so poorly,is because of memory access latency.
What makes GPUs fast is that they can perform many calculations in parallel and have the SIMDs
swap between workgroups in order to hide memory access latency. The list ranking algorithm is the
antithesis of this. It performs a random memory access to get a single element, after which it adds
two numbers and performs another random memory access. This means that the SIMD cannot keep
progressing workgroups, as all the workgroups in the cache are waiting for memory accesses to finish.
The best way to remedy this is to have the entire linked list in the cache, but a GPU has much
less cache space per thread than a CPU, so when the input data is large enough that a GPU becomes
relevant, it will have no chance of keeping the linked list in the cache.
Page 54 of 57

s184009 25th June 2025
Part III
Conclusion
In this thesis we have optimised a CPU ANSV algorithm and designed and implemented a GPU
version of both an ANSV algorithm and a RMQ algorithm.
Our optimised CPU ANSV algorithm enables calculating the ANSV of input with duplicated
values by running it once. The original algorithm is designed for input with distinct values, and only
supports duplicated values by running it twice. Our algorithm is therefore twice as fast on inputs
with duplicated values.
Additionally, our changes remove the need for running a prefix- and suffix minima algorithm as
part of the ANSV algorithm, which means that in our testing, our algorithm runs faster than the
original.
Our algorithm was, however, still slower than the BSZ algorithm implementation found at (Shun
& Zhao, 2013a).
We designed and implemented a GPU version of the ANSV algorithm. We found that it is
performs O(n)work and has O(log2(n))span on n
lg(n) threads on an CREW PRAM, making it only
slightly worse than theO(n)work and O(log(n))span on n
lg(n) threads of the original CPU ANSV
algorithm that we based it on. In return, our version supports running on a GPU on which we
achieved 4-5 times better performance than the CPU algorithm.
We then designed and implemented a GPU version of an algorithm that preprocesses the ANSV
of an array in order to answer RMQ queries in constant time. Following the constraints mentioned
in (Helman & JÃ¡JÃ¡, 2001, Lemma 1, p. 270), we analysed our algorithm to performO(n)work with
O(
p
nlog(n) )span on
q
n
ln(n) âˆ’ 1threads on an CREW PRAM. In our tests, however, we found
that despite us breaching the constraints by having too many threads, the worst case scenario of
one thread traversing a large part of the linked list never happened, so we suspect that the limit on
the number of threads might not be relevant in practice, turning it into aO(n)work and O(log2(n))
span algorithm on n
lg(n) threads.
Page 55 of 57

s184009 25th June 2025
12 Future work
The three main issues we couldnâ€™t solve were:
â€¢The GPU ANSV algorithm being limited by cache.
â€¢The GPU RMQ algorithm being bottlenecked by the list ranking algorithm.
â€¢ Analysing the Helman-JÃ¡JÃ¡ list ranking algorithm in such a way that it no longer has a
theoretical upper limit on the number of threads.
To improve on the GPU ANSV algorithm, it would likely require finding a way to replace the
kernel that calculates the local ANSV of a subset, and the kernel that runs the merging procedure
with other kernels that perform the same work without relying on having large amounts of data in
the cache at a time.
Likewise, due to the recursive Helman-JÃ¡JÃ¡ list ranking algorithm relying so heavily on random
memory accesses, it would likely require finding another list ranking algorithm in order to improve
significantly on our GPU RMQ algorithm.
13 Thanks
ThankstoProfessorIngeLiGÃ¸rtzandProfessorPhilipBilleforsupportandgoodguidancethroughout
the project.
Thanks to Albert Sidenius Garde for helping me figure out proc macros in Rust, which made
optimising GPU kernels easier.
Page 56 of 57

REFERENCESs184009 25th June 2025
References
Berkman, O., Schieber, B., & Vishkin, U. (1993). Optimal doubly logarithmic parallel algorithms
based on finding all nearest smaller values.Journal of Algorithms,14(3), 344â€“370. https:
//doi.org/https://doi.org/10.1006/jagm.1993.1018
Helman, D. R., & JÃ¡JÃ¡, J. (2001). Prefix computations on symmetric multiprocessors.Journal of
Parallel and Distributed Computing,61(2), 265â€“278. https://doi.org/https://doi.org/10.
1006/jpdc.2000.1678
Petri, M. (2024).Range_minimum_query. Retrieved June 25, 2025, from https://docs.rs/range_
minimum_query/latest/range_minimum_query/
Rehman, M. S., Kothapalli, K., & Narayanan, P. J. (2009). Fast and scalable list ranking on the
gpu.Proceedings of the 23rd International Conference on Supercomputing, 235â€“243. https:
//doi.org/10.1145/1542275.1542311
Schieber, B., & Vishkin, U. (1988).On finding lowest common ancestors: Simplification and paral-
lelization. In J. H. Reif (Ed.),Vlsi algorithms and architectures(pp. 111â€“123). Springer New
York.
Shun, J., & Zhao, F. (2013a).Practical parallel lempel-ziv factorization. Retrieved June 24, 2025,
from https://github.com/zfy0701/Parallel-LZ77
Shun, J., & Zhao, F. (2013b). Practical parallel lempel-ziv factorization.2013 Data Compression
Conference, 123â€“132. https://doi.org/10.1109/DCC.2013.20
Sitchinava, N., & Svenning, R. (2024). The all nearest smaller values problem revisited in practice,
parallel and external memory.Proceedings of the 36th ACM Symposium on Parallelism in
Algorithms and Architectures, 259â€“268. https://doi.org/10.1145/3626183.3659979
Page 57 of 57

s184009 25th June 2025
A Data points from the ANSV benchmarks
Hardware\Threads 1 6 12 16 24 32
Ryzen 5 7600x 8314ms 1640ms 1350ms
Xeon E5-2650 v4 23422ms 4122ms 2383ms 2424ms 2495ms
Xeon Gold 6226R 22283ms 4044ms 2382ms 2324ms 2422ms 2515ms
Table 1: Benchmarks of the original CPU ANSV algorithm on the random dataset. Red data points
designate when we utilise multithreading.
Hardware\Threads 1 6 12 16 24 32
Ryzen 5 7600x 6607ms 1281ms 907ms
Xeon E5-2650 v4 17670ms 3095ms 1645ms 1654ms 1665ms
Xeon Gold 6226R 17172ms 3111ms 1588ms 1250ms 1254ms 1267ms
Table 2: Benchmarks of our modified CPU ANSV algorithm on the random dataset. Red data points
designate when we utilise multithreading.
Hardware\Threads 1 6 12 16 24 32
Ryzen 5 7600x 6484ms 1242ms 873ms
Xeon E5-2650 v4 15899ms 2681ms 1363ms 1043ms 727ms
Xeon Gold 6226R 14621ms 2463ms 1235ms 940ms 646ms 510ms
Table 3: Benchmarks of the BSZ algorithm on the random dataset. Red data points designate when
we utilise multithreading.
Hardware\Threads 1 6 12 16 24 32
Ryzen 5 7600x 4809ms 1032ms 1050ms
Xeon E5-2650 v4 14872ms 2803ms 1794ms 1839ms 1895ms
Xeon Gold 6226R 14207ms 2760ms 1839ms 2198ms 2326ms 2449ms
Table 4: Benchmarks of the original CPU ANSV algorithm on the worst case dataset. Red data
points designate when we utilise multithreading.
Hardware\Threads 1 6 12 16 24 32
Ryzen 5 7600x 4666ms 908ms 871ms
Xeon E5-2650 v4 13657ms 2414ms 1317ms 1315ms 1328ms
Xeon Gold 6226R 13237ms 2387ms 1280ms 1021ms 1029ms 1050ms
Table 5: Benchmarks of our modified CPU ANSV algorithm on the worst case dataset. Red data
points designate when we utilise multithreading.
Appendix page I of II

s184009 25th June 2025
Hardware\Threads 1 6 12 16 24 32
Ryzen 5 7600x 3560ms 803ms 687ms
Xeon E5-2650 v4 11486ms 2276ms 1155ms 898ms 642ms
Xeon Gold 6226R 11702ms 2205ms 1105ms 843ms 589ms 466ms
Table 6: Benchmarks of the BSZ algorithm on the worst case dataset. Red data points designate
when we utilise multithreading.
Hardware\Dataset random worst case
AMD Radeon RX 7900XTX (RADV) 98ms 118ms
AMD Radeon RX 7900XTX (AMDGPU Pro) 495ms 382ms
Nvidia Tesla V100 429ms 439ms
Nvidia Tesla A100 169ms 196ms
Table 7: Benchmarks of the GPU ANSV algorithm.
B Data points from the RMQ benchmarks
Hardware\Dataset random ANSV RMQ worst case ANSV RMQ
AMD Radeon RX 7900XTX (RADV) 12ms 114ms 11ms 125ms
AMD Radeon RX 7900XTX (AMDGPU Pro) 50ms 119ms 35ms 122ms
Nvidia Tesla V100 38ms 429ms 37ms 439ms
Nvidia Tesla A100 24ms 169ms 26ms 196ms
Ryzen 5 7600x (sequential) 716ms 360ms
Table 8: Benchmarks of the GPU RMQ algorithm, split into the time to run the ANSV algorithm
and the RMQ algorithm. The CPU test has the two algorithms combined into one datapoint.
Appendix page II of II",,,,
Approximation Algorithms for Replenishment Problems with Fixed Turnover Times,"Approximation Algorithms for Replenishment Problems with Fixed T urnover
Times
Master Thesis
June, 2025
By
Hans Henrik Hermansen
Supervisors
Inge Li GÃ¸rtz & Philip Bille
Cover photo: Vibeke Hempler, 2012",,,,"9 Conclusion
In this thesis, we have studied Replenishment Problem with Fixed Turnover Times (RFTT),
first defined in [1], under both Min-A vg and Min-Max metrics. For Min-Max RFTT we
have covered the 6-approximation for trees given by [1], given a O(n) algorithm for cycles
and a simple 6 + NC approximation for cycle-trees. Here NC denotes the number cycles
in a graph, and a cycle-tree is a graph where all cycles are disjoint. For Min-A vg we
covered the 2-approximation for trees and gave a 4-approximation for cycles. These we
combined to also give a 4-approximation for cycle-trees. Additionally we gave insight into
how a pseudo-polynomial algorithm on cycles might be achieved and gave an algorithm
for finding optimal routes on cycle-trees. Finally on the basis of our result we pointed out
several clear paths towards achieving new results and improving the ones presented in this
thesis.
Approximation Algorithms for Replenishment Problems with Fixed Turnover Times 30

Bibliography
[1] Bosman et al. â€œApproximation Algorithms for Replenishment Problems with Fixed
Turnover Timesâ€ . In: Algorithmica (2022).
[2] Bosman T. N. â€œRelax, Round, Reformulate: Near-Optimal Algorithms for Planning
Problems in Network Design and Scheduling. â€ In: PhD-Thesis - Research and gradu-
ation internal, Vrije Universiteit Amsterdam (2019).
Approximation Algorithms for Replenishment Problems with Fixed Turnover Times 31"
Assessment of solar energy in Greenland Analyzing potential and resource availability,"1 Introduction 1
1.1 Current status of solar power in Greenland . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3",,"4 Results
Simulations were run following the structure described in Section 3.3, and below are reported the results,
scenario by scenario, beginning from tilted bifacial systems, followed by vertical bifacial systems, to
simulate ground-mounted PV plants, and finally tilted monofacial systems, that reproduce roof-mounted
panels.
4.1 Bifacial panels, tilted by 60 Â°
The first reported results are those relative to a PV plant of ground-mounted bifacial solar panels, tilted
by 60Â° with respect to the horizontal.
Figure 17: Growth trend of the displaced energy in Ilimanaq with increasing PV capacity.
Bifacial panels, tilt = 60Â°
Table 12: Annual displaced energy [MWh] for different system sizes and orientations.
Scenario 100 kW 200 kW 300 kW 400 kW 500 kW
South 136.1 272.3 369.3 413.5 427.1
East 125.9 251.9 333.7 383.4 401.8
West 125.5 250.9 331.6 382.1 400.1
Looking at Figure 17 and Table 12, it is possible to observe a decreasing growth pace of the total
displaced energy due to the adoption of a 60Â° tilted PV system when the size of it increases, for all the
tested orientations, and Figure 17 also reports the total yearly energy consumption of Ilimanaq, that
amounts to 591.6 MWh, as a dashed black horizontal line.
For all the tested azimuths, the displaced energy value doubles with the initial doubling of the
27


4.1 Bifacial panels, tilted by 60Â°
photovoltaic capacity from 100 to 200 kW, and the corresponding 0% of curtailed energy percentage
testify that there is no waste of solar energy.
The trend changes when the capacity grows to 300 kW, an the displaced energy just goes from a 100%
growth of the previous step, to a displaced energy growth of around 35% with a 50% increase in capacity.
When the PV capacity grows further, from 300 to 400 and subsequently 500 kW, the curve flattens due
to a very small increased of total displaced energy.
Figure 18: Growth trend of curtailed energy and solar contribution in Ilimanaq with increasing PV
capacity.
Bifacial panels, tilt = 60Â°
Table 13: Curtailed Energy Percentage [%] by System Size [kW] and Orientation
Bifacial, 60Â° tilt
Scenario 100 kW 200 kW 300 kW 400 kW 500 kW
South 0.0 0.0 9.6 24.1 37.3
East 0.0 0.0 11.7 23.9 36.2
West 0.0 0.0 11.9 23.9 36.2
Figure 18 and Table 13 instead report the growth of the percentage of the generated solar energy that
gets curtailed, meaning not consumed by the Ilimanaq community - and consequently wasted given
the lack of planned storage technologies in this project - when the PV capacity grows; alongside it,
there is the solar contribution percentage, defined as the ratio between non-curtailed energy and total
consumption, also growing with the trend thatâ€™s been already detected for displaced energy in Figure 17.
For this case, curtailment starts when the solar share is approximately 46% for South-facing modules,
and approximately 42% for the other two azimuth angles.
28


4.1 Bifacial panels, tilted by 60Â°
In this scenario, thereâ€™s no curtailed energy for the 100 and 200 kW cases, meaning that all the
PV-produced energy is consumed. When the capacity reaches 300 kW, the percentage starts growing
and it is interesting to highlight how it is lower when the panels face South, despite this being the
orientation resulting into the highest production, as shown in Appendix H.1.
It could be hypothesized that bigger energy production leads to greater waste, but the distribution of
the produced energy throughout the months plays a crucial role in this calculation. Figures 19, 20 and
21 below provide a better understanding of the results.
When the azimuth is 180Â° (for reference, see Figure 19 and Figure 47), i.e the panels are facing South,
the overall production is the highest but less energy is curtailed because an important production
growth is visible in low-radiation months like March, September and October, and such an increase in
production does not overcome the consumption value and hence does not result in any curtailment.
Such a growth in those months balances the curtailed energy of the months between April and June,
when the consumption is overtaken by the production for all three orientations.
When the PV capacity increases again, to 400 and 500 kW, this effect is lost due to the bigger gaps
created by a much higher production, and all the three orientations have similar percentages.
Looking at the dashed line, representing the solar contribution. This indicator is very strictly linked to
the energy curtailment: it is clear how the growth rate is linear at the beginning of the simulation, when
the curtailed percentage is null, for low capacities, and starts slowing down as soon as energy starts
being unused. This is because non-curtailed energy is utilized in the calculations of solar contribution,
being divided by the total consumption, it is naturally consequential that a rising waste of solar energy
gives a growth in production with decreasing pace.
In this case, the solar contribution flattens around a value of 70% and reaching higher values would
require a massive effort in terms of extension of the capacity, that would however bring low benefits.
This upper limit allocates so distant from the 100% maximum value because of the impossibility of
filling the consumption/production gap in winter times, when the solar radiation is almost constantly
equal to zero, hence creating an insurmountable obstacle to the achievement of higher values.
29


4.1 Bifacial panels, tilted by 60Â°
Figure 19: Monthly comparison of PV energy production and the Ilimanaq energy consumption.
Bifacial panels, tilt = 60Â°, azimuth = 180Â°
Figure 20: Monthly comparison of PV energy production and the Ilimanaq energy consumption.
Bifacial panels, tilt = 60Â°, azimuth = 90Â°
30


4.2 Vertical bifacial panels
Figure 21: Monthly comparison of PV energy production and the Ilimanaq energy consumption.
Bifacial panels, tilt = 60Â°, azimuth = 270Â°
Appendix H.2 reports an analysis for 45Â° tilted bifacial panels.
4.2 Vertical bifacial panels
The following, and last, analyzed set of bifacial panels are the vertical.
Figure 22: Growth trend of the displaced energy in Ilimanaq with increasing PV capacity.
Bifacial panels, vertical
31


4.2 Vertical bifacial panels
Table 14: Annual displaced energy [MWh] for different system sizes and orientations.
Bifacial panels, vertical
Scenario 100 kW 200 kW 300 kW 400 kW 500 kW
South 128.6 257.1 352.9 406.9 427.1
East 128.3 255.6 335.6 384.5 402.9
West 128.2 255.2 334.8 384.0 402.5
The displaced energy values of a system of vertically ground-mounted panels have the same trend of
the cases above. If the panels are oriented towards South, the resulting values are slightly lower than
the corresponding ones of the previous analyses for capacities up to 300 kW, when the overproduction
starts leveling the different scenarios.
Vertical West- and East-facing systems have, on the other hand, higher values than the ones of 45Â° and
60Â° from a 4-5% increase in the low capacity simulations to 1% in the higher ones. It is also interesting
to point that, with the adoption of vertical panels, the gap between the displaced energy of South and
East/West facing panels is reduced: from a 8 and 9% average difference for 60Â° and 45Â° tilts respectively,
this gap shrinks to an average 3% difference, and is even lower for the low capacity options. The reason
this happens is to be found in Figures 55, 56 and 57 (Appendix H.3): the AC energy production is very
similar for the three orientations, around 128 kWh yearly for a basic 100 kW plant, testifying that the
azimuth angle loses relevance with such high tilt values because of of the sunâ€™s low path in the sky, the
high proportion of diffuse radiation, and the balanced exposure to reflected light across all directions.
[23]
32


4.2 Vertical bifacial panels
Figure 23: Growth trend of curtailed energy and solar contribution in Ilimanaq with increasing PV
capacity.
Bifacial panels, vertical
Table 15: Curtailed Energy Percentage [%] by System Size [kW] and Orientation
Scenario 100 kW 200 kW 300 kW 400 kW 500 kW
South 0.0 0.0 8.5 20.9 33.6
East 0.0 0.4 12.8 25.1 37.2
West 0.0 0.5 12.9 25.1 37.2
The curtailed energy percentage graph shown in Figure 23 differs significantly from those previously
reported. Notably, at a capacity of 200 kW, the curtailed energy percentage becomes slightly greater
than zero. This is due to elevated production levels in May for the East- and West-facing systems,
which marginally exceed local consumption during that month.
As illustrated in Figures 24, 25, 26, and in the figures provided in Appendix H.3, East- and West-
oriented systems demonstrate higher energy output during the spring and summer months. In contrast,
South-facing systems yield more in February, March, September, and October. However, this seasonal
advantage does not lead to additional curtailment, as the generation in those months remains below
the consumption threshold.
Combining this insight with the relatively lower output of South-facing systems during the summer
monthsâ€”when consumption is consistently exceeded for all orientationsâ€”results in a lower overall
curtailed energy percentage for the South-facing configuration across all capacities.
33


4.2 Vertical bifacial panels
The trend in solar contribution mirrors the pattern discussed earlier in Section 4.1, with an initial
linear increase followed by a slowdown as curtailment rises. Despite the nearly identical total energy
production across orientations (see Appendix H.3), the higher excess generation in East- and West-facing
systems leads to reduced total displaced energy (Figure 22) and a lower solar share.
Figure 24: Monthly comparison of PV energy production and the Ilimanaq energy consumption.
Bifacial panels, vertical, azimuth = 180Â°
Figure 25: Monthly comparison of PV energy production and the Ilimanaq energy consumption.
Bifacial panels, vertical, azimuth = 90Â°
34


4.3 Monofacial panels
Figure 26: Monthly comparison of PV energy production and the Ilimanaq energy consumption.
Bifacial panels, vertical, azimuth = 270Â°
4.3 Monofacial panels
After the production analysis of bifacial plants, the focus moves to roof-mounted monofacial panels.
This scenario is needed to simulate the installation of roof-mounted panels instead of ground-mounted
systems.
Figure 27: Growth trend of the displaced energy in Ilimanaq with increasing PV capacity.
Monofacial panels, tilt = 40Â°
35


4.3 Monofacial panels
Table 16: Annual displaced energy [MWh] for different system sizes and orientations.
Monofacial panels, tilt = 40Â°
Scenario 100 kW 200 kW 300 kW 400 kW 500 kW
South 113.1 226.3 325.7 393.9 419.3
East 85.2 170.4 255.5 316.8 358.3
West 84.0 168.1 252.0 309.7 353.9
The growth in this case keeps a linear trend for longer than for bifacial panels, because of the lower global
production (shown in Appendix H.4) which needs a bigger capacity to get close to the consumption
rate and consequently resulting into curtailment.
This also is the cause of a higher displaced energy increase from step to step, and then the curves tend
to flatten later than in the previous cases, even more evidently for the East and West-facing scenarios.
Figure 28: Growth trend of the displaced energy in Ilimanaq with increasing PV capacity.
Monofacial panels, tilt = 40Â°
Table 17: Curtailed Energy Percentage [%] by System Size [kW] and Scenario
Scenario 100 kW 200 kW 300 kW 400 kW 500 kW
South 0.0 0.0 4.0 13.0 25.9
East 0.0 0.0 0.0 7.0 15.9
West 0.0 0.0 0.0 7.9 15.8
Figure 28 and Table 17 confirm the observations discussed above: as expected, the curtailed energy
36


4.3 Monofacial panels
percentages are lower for all three monofacial scenarios compared to the previously analyzed bifacial
ones.
Another key difference is that the curtailed energy shares are consistently higher for the 180Â° azimuth
compared to the 90Â°and 270Â°orientations. While South-facing systems (azimuth of 180Â°) still benefit from
a more favorable monthly production distributionâ€”particularly in March, September, and Octoberâ€”the
high curtailment observed during peak-irradiance months cannot be offset by this seasonal balance.
Moreover, unlike in the bifacial scenarios, the intermediate capacitiesâ€”where the 180Â° azimuth previously
benefited from a more even production profileâ€”now result in relatively low energy outputs for the 90Â°
and 270Â° orientations, which are insufficient to outperform the South-facing setups in overall energy
production. This behavior can be observed in Figures 29, 30 and 31 below.
The solar contribution continues to follow the trend of the displaced energy. In this case as well, the
curve begins to flatten at around 70% for the South-oriented system, while it stabilizes at approximately
60% for the East- and West-facing scenarios.
It is worth noting that the yellow and green dashed linesâ€”representing East and West orientations,
respectivelyâ€”exhibit a steeper slope compared to the South-facing line. This is due to their lower
overall production and curtailed energy shares, which makes the smoothing effect less pronounced.
Figure 29: Monthly comparison of PV energy production and the Ilimanaq energy consumption.
Monofacial panels, tilt = 40Â°, azimuth = 180Â°
37",,"5 Conclusion
The objective of this thesis was to study the impact that a large-scale implementation of solar power
technologies could have on the decarbonisation of the Greenlandic electric energy generation and carbon
emissions
Before proceeding with the conclusion, it is important to highlight a major limitation of this study,
namely its focus being exclusively on electricity generation, without considering other major energy
demands in Greenland such as heating. Since heating constitutes a significant share of the countryâ€™s
overall energy consumption and associated emissions, future work should aim to integrate both electricity
and thermal energy sectors for a more comprehensive decarbonisation assessment.
The motivations behind this choice reside in the high reliance the island has on diesel generators to
produce electricity, specially in the settlements located far from one of the five hydropower plants built
on the territory and supply just a few of the many towns and villages situated all along the coast and
whose electric power production totally depend on the combustion of fossil fuels. For this reason, the
study moves from simulations and models tailored for scenarios of PV adoption by small settlements,
with the future auspice to scale them to a wider scale.
The starting point of the project was ground-measured data, obtained via DMI, of the GHI of six
different weather stations in Greenland. After being analyzed, cleaned and filtered thanks to a thorough
quality control process aiming to empty the dataset from imperfections and errors due to instrumental
malfunctioning due to electronic or weather-related issues, the clean dataset was then used as a base
for the comparison of two reanalysis datasets, ERA5 and NASA POWER, both available online. The
reanalysis datasets of all the six stations were confronted with the DMI one, and the resulting RMSEs
and biases have indicated ERA5 as the better fitting among the two.
The ERA5 dataset of Ilimanaq, a 53 inhabitants settlement located on the Greenlandic West coast
whose consumption data for 2023 was provided by Nukissiorfiit, is the base of the next step of the
analysis, consisting in modeling and simulating the performance of a PV plant in the supra cited
location, in the light of a yearly electricity demand amounting to 591.6 MWh.
Different variants of the model were created: ground-mounted bifacial panels tilted by 45Â° or 60Â°,
ground-mounted bifacial vertical panels, and monofacial roof-mounted panels tilted by 40Â°.
The setupsâ€™ performances have been simulated for three different orientations, South, East and West,
and multiple plant capacities, the selected ones being all the multiples of 50 kW, from a minimum
of 100 kW to a maximum of 500 kW, and the results have been compared to the consumption data
mentioned above to compute displaced energy values and curtailed energy and solar contribution
percentages, to gain insights about the usefulness of increasing the plant capacity as opposed to a
constant consumption habitude.
The simulations proved that bifacial panels, tilted and vertical, have a linear growth trend until the
size of 200 kW, when curtailed energy becomes higher than zero for the first time because of some of
the monthly consumption values being lower than the AC output of the plant in the same month. A
200 kW capacity corresponds to a solar fraction of 46% for South facing systems, and 42.5% for the
two other orientations when it comes to 60Â° tilted bifacial solutions. The percentages are between 43
and 43.5% for all the three azimuths in the vertical case. This behavior becomes more influent with
43


capacity increase, as testified by the deceleration faced by the growth speed of the displaced energy
values, whose curve tends, in both cases, to flatten reaching approximately 430 MWh/y for a capacity
of 500 kW, in a South-facing scenario, and approximately 400 MWh/y in East/West facing scenarios.
Solar contribution is also similarly affected, assuming a similar curve trend and assessing itself around a
share of 72%, for modules oriented toward South, and 68%, for the remaining orientations. This means
that once 200 kW is exceeded, further increases in PV capacity result in rapidly diminishing returns in
terms of solar contribution.
Saved emissions are computed multiplying the displaced energy values by an emission factor of
0.807
kgCO2
kW h , specific for Ilimanaq and obtained from the Nukissiorfiitâ€™s annual report for year 2023. [10]
This calculation structure makes the avoided emissions have the same shape and trend as the displaced
energy, being the factor a constant, hence they have a fast initial growth followed by a subsequent
flattening when the capacity exceeds 200 kW.
In the absence of a storage technology, the approach of this project is to scale the plant up to a
maximum of approximately 200 kW, to avoid excess production that canâ€™t be repurposed.
Before proceeding with further discussions, it is important to note that curtailment is not always
negative. In fact, oversizing a solar plant can offer economic advantages: if the cost of solar electricity is
lower than that of diesel-generated power, it may be economically justified to accept a certain level of
curtailment. This allows for greater displacement of diesel consumption during sunny periods, even
if some excess energy must occasionally be curtailed. However, the economic aspect in this project
is secondary to the purely energy-related perspective and the curtailment-minimizing approach was
preferred.
In the case of South facing panels, 60Â° tilted panels have a better output for this capacity, while East
and West facing solutions find a better fit in the vertically-installed modules.
Without retaining the surplus electricity, oversizing the plant would require higher investments which
would result in a lower efficiency, and this would add up with more economic losses related to the
unsold curtailments.
On average, these solutions grant approximately 260 MWh of displaced energy yearly, corresponding to
210 saved tons of CO2 and a 44% reduction of carbon emissions compared to the base scenario, without
any renewable solutions integration. The reduction share grows until 46% if the adopted solution is the
best case scenario, a 200 kW plant of South-facing 60Â° tilted panels.
Monofacial roof-mounted systems have, on the other hand, a higher curtailment limit (300 kW) compared
to their counterpart, and lower installation costs linked to the usage of roofs as pre-existing mounting
structures, but encounter the major disadvantage of having a much lower available mounting area
compare to the bifacial alternative.
In Ilimanaq particularly, being a small settlement, the roof surface availability is too low to allow a
large scale development of this technology, which is advised as a side solution for the few buildings of
the village.
Nevertheless, it canâ€™t reach its potential, which is still high despite the lower energy production values
which make it less affected by curtailments and allows the final displaced energy values to at least
partially bridge the gap with its alternatives for a South facing 500 kW plant - 419 MWh/y against
427 MWh/y, curtailed shares of 26% against an average of 35% and 338 against 345 tons of saved
44


carbon emissions.
These answers confirm the good potential of a technology that is however necessarily limited in term of
spacial availability, in a place like Greenland which is the least densely populated territory on Earth.
CurtailmentcouldbeavoidedbyrepurposingsurplusPVelectricityforheatinginGreenlandichouseholds,
where heating demand is high and largely met by diesel. In 2023, household diesel use reached 1299 TJ
(360 GWh), contributing to 95 kton of CO2 emissions.
Although precise usage breakdowns are unavailable, it is likely that most of this energy supported
space heating due to the Arctic climate. Oversizing the Ilimanaq bifacial plant, which could exceed
200 kW, could allow excess generation to power electric boilers. This shift would reduce diesel reliance
and emissions but requires not only technological upgrades, but also changes in consumer habits,
infrastructure, and awareness.
Energy storage can be implemented as a repository for the excess generation occurring in spring or
summer. Some potential solutions that have been investigated include battery energy storage (BES),
mostly for diesel-off operations during the summertime, to garner surplus energy to be then released
in low-radiation hours or days, for lightning, or heating purposes, as just mentioned in the paragraph
above.
Integration of hydrogen to BES technologies has also been object of studies with the purpose to make
batteries more suitable for seasonal storage because this implementation could enable opportunities for
repurposing linked to the broader needs of the whole settlement, like desalinization or district heating.
[26]
Compared to hourly or daily balancing strategies, redirecting curtailed energy toward seasonal appli-
cations offers a greater potential for emission reductions and energy system optimization, although
bigger investments and operational costs must be faced in this scenario. The better potential is because
surplus generation in high-irradiance months â€” if stored or repurposed for long-term uses such as space
heating or water desalination â€” directly offsets significant diesel consumption during the extended
Arctic heating season.
A large scale implementation of storage technologies could then allow the expansion of the hypothe-
sized ground-mounted plants, allowing both the figures of displaced energy and solar contribution to
increase. The hope is of Nukissiorfiit to opt for investments towards more renewable energy solutions in
disconnected settlements and to accompany this choice with storage technologies that could improve
their effectiveness.
Some last considerations must be made regarding the scalability of this project on a wider size, including
all Greenland.
Given the vast land availability on the islandâ€™s territory, and assuming the feasibility of retrieving
consumption data for other towns and settlements, similar calculations and simulations to the ones
made in this thesis can be replicated for other locations.
A scenario including all the Greenlandic settlements not connected to already existing hydropower
plants, would entail variable consumptions and production profiles, due to the demand variability for
each location and for the different solar resource availability between the southern and northern extreme
latitudes of whatâ€™s the biggest island in the world. Taking also into consideration the different emission
factors that Nukissiorfiit provide on their annual report [10], and that will maybe be updated in the
45


next publications, it can be absolutely possible to replicate what done in this study on a larger scale
and make more accurate estimations of the emissions reduction impact for the whole Greenland.
The continuation of studies related to storage technologies and their integration into remote microgrids
will be essential to ensure year-round reliability and maximize the use of solar energy. Future work should
focus on evaluating the technical and economic feasibility of hybrid storage systemsâ€”such as battery
and hydrogen-based solutionsâ€”for seasonal and daily balancing. Additionally, incorporating spatially
resolved consumption data and refining solar resource assessments across the country would enhance
the accuracy of PV system design and emissions impact evaluation. Altogether, these developments
could pave the way for a more sustainable, decentralized energy transition across Greenland, tailored to
the specific needs and conditions of each settlement.
46


References
References
[1] World Meterological Organisation, â€œGlobal temperature is likely to exceed 1.5Â°c above pre-industrial
level temporarily in next 5 years,â€wmo.int, 2024.
[2] NASA Earth Observatory, â€œWorld of change: Global temperatures,â€earthobservatory.nasa.gov,
2024.
[3] M. Rantanen, â€œThe arctic has warmed nearly four times faster than the globe since 1979,â€Commu-
nications Earth & Environment , 2022.
[4] S. Jiang, A. Ye, and C. Xiao, â€œThe temperature increase in greenland has accelerated in the past
five years,â€Global and Planetary Change , 2020.
[5] K. Goto-Azuma, T. Homma, T. Saruya, F. Nakazawa, Y. Komuro, N. Nagatsuka, M. Hirabayashi,
Y. Kondo, M. Koike, T. Aoki, R. Greve, and J. Okuno, â€œStudies on the variability of the greenland
ice sheet and climate,â€Polar Science, 2021.
[6] T. Galimova, R. Satymov, D. Keiner, and C. Breyer, â€œSustainable energy transition of greenland
and its prospects as a potential arctic e-fuel and e-chemical export hub for europe and east asia,â€
Energy, 2024.
[7] Naalakkersuisut - Government of Greenland, â€œGreenland hydropower resources,â€ 2024. Accessed:
2025-04-25.
[8] A. Mustayen, M. Rasul, X. Wang, M. Negnevitsky, and J. Hamilton, â€œRemote areas and islands
power generation: A review on diesel engine performance and emission improvement techniques,â€
Energy Conversion and Management , vol. 260, 2022.
[9] GrÃ¸nlands Statistik, â€œEnergiforbrug 2023,â€ December 2024.
[10] Nukissiorfiit, â€œAnnual report 2023.â€https://nukissiorfiit.gl/da/om/Aarsregnskaber, 2023.
Appendix 2, Pages 66-67-68.
[11] C. J. Cox and S. Morris, â€œThe de-icing comparison experiment (d-ice): A campaign for improving
data retention of radiometric measurements under icing conditions in cold regions: Science and
implementation plan,â€ tech. rep., NOAA Earth System Research Laboratory (ESRL) Physical
Sciences Division and CIRES, July 2017. Project Leads, CIRES and NOAA ESRL PSD.
[12] J. T. Peterson, E. C. Flowers, and J. H. Rudisill, â€œDew and frost deposition on pyranometers,â€
Journal of Applied Meteorology , vol. 12, no. 7, pp. 1231â€“1233, 1973.
[13] A. R. Jensen and Y.-M. Saint-Drenan, â€œQuality assessment of solar irradiance data,â€ 2025. Accessed:
2025-04-27.
[14] Weather Spark, â€œClimate and Average Weather Year Round at Danmarkshavn, Greenland,â€ 2025.
Accessed: 2025-04-27.
[15] C. E. Hartvig and M. GrÃ¸nbek Bager, â€œEnergilagring til Ilimanaq,â€ Masterâ€™s thesis, MARTEC
(Maritime and Polytechnic University College) - in collaboration with Nukissiorfiit, 2023.
47


References
[16] E. M. Tonita, A. C. Russell, C. E. Valdivia, and K. Hinzer, â€œOptimal ground coverage ratios
for tracked, fixed-tilt, and vertical photovoltaic systems for latitudes up to 75Â°n,â€Solar Energy,
vol. 258, pp. 8â€“15, 2023.
[17] N. Riedel-LyngskÃ¦r, M. Ribaconka, M. PÃ³, A. Thorseth, S. Thorsteinsson, C. Dam-Hansen, and
M. L. Jakobsen, â€œThe effect of spectral albedo in bifacial photovoltaic performance,â€Solar Energy,
vol. 231, pp. 921â€“935, 2021.
[18] J. Eggeling, â€œModeling albedo in the dry snow zone of the greenland ice sheet,â€ technical report,
Lund University, Department of Physical Sciences, June 2014.
[19] Weather Spark, â€œClimate and Average Weather Year Round in Nuuk, Greenland,â€ 2025. Accessed:
2025-05-18.
[20] Weather Spark, â€œClimate and Average Weather Year Round in Ilulissat, Greenland,â€ 2025. Accessed:
2025-05-18.
[21] N. Janotte, S. Wilbert, F. Sallaberry, M. Schroedter-Homscheidt, and L. Ramirez, â€œPrinciples of
csp performance assessment,â€ inThe Performance of Concentrated Solar Power (CSP) Systems
(P. Heller, ed.), pp. 31â€“64, Woodhead Publishing, 2017.
[22] R. E. Pawluk, Y. Chen, and Y. She, â€œPhotovoltaic electricity generation loss due to snow â€“ a
literature review on influence factors, estimation, and mitigation,â€Renewable and Sustainable
Energy Reviews, vol. 107, pp. 171â€“182, 2019.
[23] S. JouttijÃ¤rvi, J. Thorning, M. Manni, H. Huerta, S. Ranta, M. D. Sabatino, G. Lobaccaro,
and K. Miettunen, â€œA comprehensive methodological workflow to maximize solar energy in low-
voltage grids: A case study of vertical bifacial panels in nordic conditions,â€Solar Energy, vol. 262,
pp. 1004â€“1012, 2023.
[24] B. Samaila and J. M. Garba, â€œComparative study on ground and roof-mounted solar pv systems,â€
Journal of Energy Engineering and Thermodynamics , vol. 4, Octâ€“Nov 2024.
[25] Statistics Greenland, â€œCo2 emissions by industry (1990â€“2022),â€ 2024. Accessed: 2025-06-22.
[26] A. Pantaleo, M. R. Albert, H. T. Snyder, S. Doig, T. Oshima, and N. E. Hagelqvist, â€œModeling
a sustainable energy transition in northern greenland: Qaanaaq case study,â€Sustainable Energy
Technologies and Assessments, vol. 54, p. 102774, 2022. Published under CC BY license.
48


Appendix
A Hydropower plants
Figure 32: Map of the hydropower plants in Greenland
I


B Example of time step distribution for the Nuuk station
Figure 33: Time steps distribution for the Nuuk DMI station
C 2-D visualizations
Figure 34: 2-D visualization of the raw data for Aasiaat
II


Figure 35: 2-D visualization of the raw data for Danmarkshavn
Figure 36: 2-D visualization of the raw data for Narsarsuaq
Figure 37: 2-D visualization of the raw data for Tasiilaq
III


D Additional data filtering
Figure 38: The three filtering steps shown for the month of January for the Ittoqqortoormiit station.
IV


E Scatter plots
Figure 39: Scatter plot for Aasiaat
V


Figure 40: Scatter plot for Danmarkshavn
VI


Figure 41: Scatter plot for Ittoqqortoormiit
VII


Figure 42: Scatter plot for Narsarsuaq
VIII


Figure 43: Scatter plot for Tasiilaq
IX


F Location of the settlements of interest
Figure 44: Location of the settlements of interest
X


G Energy consumption in Ilimanaq and Qeqerarsuatsiaat
Figure 45: Monthly energy consumption in Ilimanaq
Figure 46: Monthly energy consumption in Qeqerarsuatsiaat
XI


H PV Production
H.1 Bifacial panels, 60 Â° tilt
Figure 47: PV production in Ilimanaq in 2023, bifacial 100 kW plant
Tilt = 60Â°, South facing
Figure 48: PV production in Ilimanaq in 2023, bifacial 100 kW plant
Tilt = 60Â°, East facing
XII


H.2 Bifacial panels, 45 Â° tilt
Figure 49: PV production in Ilimanaq in 2023, bifacial 100 kW plant
Tilt = 60Â°, West facing
H.2 Bifacial panels, 45 Â° tilt
Below is reported the analysis conducted for 45Â° bifacial panels, that was initially meant to be part of
the analysis but was consequently discarded for not adding much value to the work compared to the 60Â°
tilted case.
It is, however, still reported for the interesting behavior retraceable in the curtailed energy at 200 kW,
whose analogies and differences with the 60Â° tilt case are reported.
XIII


H.2 Bifacial panels, 45 Â° tilt
Figure 50: Growth trend of the displaced energy in Ilimanaq with increasing PV capacity.
Bifacial panels, tilt = 45Â°
Table 21: Annual energy output [MWh] for different system sizes and orientations.
Bifacial, 45Â° tilt
Scenario 100 kW 200 kW 300 kW 400 kW 500 kW
South 136.4 272.9 368.1 410.7 424.8
East 122.4 244.9 327.8 380.6 398.7
West 121.8 243.6 325.4 379.1 397.0
The behavior of the displaced energy described in Section 4.1 for 60Â° tilted bifacial panels is also
traceable for 45Â° tilted bifacial panels: an initial linear increase when the capacity first doubles from
100 to 200 kW is followed by a decrease of the speed of growth, due to the reasons explained in the
previous section.
The energy production plots are reported in Figures 52, 53 and 54 below.
XIV


H.2 Bifacial panels, 45 Â° tilt
Figure 51: Growth trend of the curtailed energy in Ilimanaq with increasing PV capacity.
Bifacial panels, tilt = 45Â°
Table 22: Curtailed Energy Percentage [%] by System Size [kW] and Orientation
Scenario 100 kW 200 kW 300 kW 400 kW 500 kW
South 0.0 0.0 10.1 24.8 37.7
East 0.0 0.0 10.8 22.3 34.9
West 0.0 0.0 11.0 22.2 34.8
Figure 51 and Table 22 also confirm the similarities between the 60Â° and the 45Â° cases. Also in this
scenario, South facing panels have the smallest curtailed percentage despite the biggest production,
when the plant is scaled up to 300 kW. This is again attributable to a greater share of the produced
energy that gets distributed to the months with low irradiance.
The gap between West/East and South oriented panels is in this case lower than in the 60Â° one (1% vs.
2% difference): this small change is ascribable to the fact that the higher tilt of the previous scenario
makes the panels more fit to be hit by solar radiation when the sun is low in the horizon.
XV


H.2 Bifacial panels, 45 Â° tilt
Figure 52: PV production in Ilimanaq in 2023, bifacial 100 kW plant
Tilt = 45Â°, South facing
Figure 53: PV production in Ilimanaq in 2023, bifacial 100 kW plant
Tilt = 45Â°, East facing
XVI


H.3 Vertical bifacial panels
Figure 54: PV production in Ilimanaq in 2023, bifacial 100 kW plant
Tilt = 45Â°, West facing
H.3 Vertical bifacial panels
Figure 55: PV production in Ilimanaq in 2023, bifacial 100 kW plant
Vertical, South facing
XVII


H.3 Vertical bifacial panels
Figure 56: PV production in Ilimanaq in 2023, bifacial 100 kW plant
Vertical, East facing
Figure 57: PV production in Ilimanaq in 2023, bifacial 100 kW plant
Vertical, West facing
XVIII


H.4 Monofacial panels, 40 Â° tilt
H.4 Monofacial panels, 40 Â° tilt
Figure 58: PV production in Ilimanaq in 2023, monofacial 100 kW plant
Tilt = 40Â°, South facing
Figure 59: PV production in Ilimanaq in 2023, monofacial 100 kW plant
Tilt = 40Â°, East facing
XIX


H.4 Monofacial panels, 40 Â° tilt
Figure 60: PV production in Ilimanaq in 2023, monofacial 100 kW plant
Tilt = 40Â°, West facing
XX"
Frequency-based Substructuring using Scanning Laser Doppler Vibrometry,"1 Introduction  
Frequency-Based Substructuring (FBS)  is a practical method for predicting  the dynamic 
behavior of an assembled mechanical system by coupling the frequency response functions 
(FRFs) of its individual components. This process relies on accurate FRFs, which in turn depend 
on precise and reliable data acquisition.  Accelerometers are traditionally  used to collect 
experimental vibration data for FBS. While these sensors are widespread and offer many 
advantages, they are considered intrusive because attaching them to a structure alters the 
dynamic behavior. Consequently, the accuracy of the data collected for dynamic analyses is 
impaired. To overcome such limitations, this thesis investigates an alternative approach for 
FBS: Scanning Laser Doppler Vibrometry (SLDV). Unlike accelerometers, SLDV is a n on-
intrusive measurement technique that employs a laser to scan the surface and capture full-field 
vibration data without impacting the structureâ€™s dynamics. The research will evaluate the 
applicability of SLDV in FBS and explore its potential advantages o ver accelerometer-based 
measurements.  
Dynamic Substructuring (DS) is a modular approach to structural dynamic systems, which can 
be analyzed in physical, modal or frequency domains. This study focuses on frequency domain 
substructuring methods, which offer advantages over modal and, especially, physical domain 
modeling in experimental environments . In the physical domain, it is impossible to obtain a 
complete experimental description of a structure, while in the modal domain, high damping or 
complex frequency behaviors pose significant challenges. Frequency domain methods like FBS 
can provide accurate results in such challenging experimental settings  [1]. These methods  
characterize structures using their FRFs, which describe dynamic behavior as the linear ratio of 
responses (outputs) and excitations (inputs). 
DS in the frequency domain is also being referred to as Frequency-Based Substructuring (FBS). 
In general, FBS can predict an assemblyâ€™s frequency response by coupling the measured or 
simulated FRFs of its individual subsystems. Regardless of the coupling technique, two 
conditions must always be fulfilled in FBS [1]: 
1. Compatibility condition: Establishes the compatibility of displacements at the common 
interface. 
2. Equilibrium condition: Establishes the equilibrium of forces at the common interface. 

 5 
 
In a discretized system, these boundary conditions are easily enforced by coupling 
displacements and forces at each collocated interface node [2]. In real structures however, such 
a collocation does not exist, and different solutions have to be considered, one of which is the 
virtual point transformation (VPT). The VPT is based on the coupling of multiple translational 
responses close to the interface and projecting them into one common  subspace under the 
rigidity assumption [2]. In this way, rotational degrees of freedom ( DoFs) are implicitly 
accounted for1, making the VPT a widely-used method for experimental coupling. 
In both academic and industrial settings, experiments require significantly more time and 
resources than numerical simulations or digital models. Therefore, minimizing the number or 
scope of experiments is always a key objective.  One major advantage of FBS is that it allows 
for re-measurement or simulation of only the replaced component, instead of the entire system. 
The part can then be coupled with the main system again, no matter if it consists of 
experimentally or numerically generated data. The benefit s of this approach are evident in 
industrial applications where noise and vibration reduction are critical, allowing for fewer 
experiments and reduced costs. 
A successful application of FBS techniques depends on an accurate data generation method. 
Classically, 3D piezo accelerometers are used for experimental response measurement due to 
their advantages such as wide variety of sensor types, simple mounting and operation, and a 
broad dynamic and frequency range [3]. Despite their advantages, accelerometers have certain 
limitations. Any physical attachment to a structure alters its dynamic properties , causing each 
accelerometer to bias the measured response. This effect is particularly significant in 
experiments involving lightweight structures [4]. Given  the additional drawbacks of 
accelerometers [3], laser technology offers a non -intrusive alternative to these physically 
attached sensors. 
In a recent study 2, Trainotti et al. [3] investigated the applicability of laser technology for 
experimental LM-FBS coupling, including the VPT. Using a laser Doppler vibrometry (LDV) 
instrument, the study highlighted the potential of non -intrusive vibration measurement 
techniques, as the LDV responses closely matched the accelerometer measurements and the 
 
1 Directly measuring rotational acceleration is not common in practice since it is more difficult than measuring 
translational responses [32]. 
2 As of 2025. 

 6 
 
validation. In fact, the LDV  instrument proved to be more accurate over a broader frequency 
range than the accelerometers, leading to a strong recommendation for using LDV technology 
in FBS. 
Given the promising findings of LDV, this thesis will conduct further analysis on the potential 
of laser technology in FBS using a scanning laser Doppler vibrometer (SLDV).  The key 
difference between LDV and SLDV is that an SLDV scans an oscillating surface automatically 
over a predetermined grid of points. Therefore, the device allows for the recording of full-field 
surface vibration data, with the level of detail determined  by the grid resolution. This is 
particularly beneficial for FBS coupling because m ultiple response measurements can be 
captured simultaneously, saving significant time.  Additionally, with the right software, mode 
shapes can be efficiently estimated after measuring a fine laser grid over the entire surface of 
the structure in a single pass. 
However, since the SLDV scanning process is not instantaneous, classical modal hammer 
excitation is unsuitable because it would require many repeated hammer hits during the 
scanning process. Hence, a new experimental setup must be designed that uses a continuous 
excitation method, such as a piezoelectric actuator.  In addition, unlike accelerometers, the 
SLDV measures vibrations only perpendicular to the surface â€“ yet another technical challenge 
that must be addressed. 
This thesis represents a first investigation into the applicability of SLDV in FBS. Its practical 
part comprises a physical and a simulated numerical experiment on a two -component 
benchmark structure: 
â€¢ Experiment 1: Simulated numerical experiment  with FBS coupling , including a 
reference experiment on the assembled structure. 
â€¢ Experiment 2: SLDV experiment with FBS coupling, including a reference experiment 
on the assembled structure. 
As the first study of its kind, the discussion section will specifically document both practical 
and data -processing challenges encountered during the project. This will assist future 
researchers working with SLDV-based FBS in identifying and overcoming similar challenges. 
The thesis will be organized according to the following structure: Chapter 2 provides a literature 
review of FBS and VPT, placing the SLDV outcomes within a wider context. Furthermore, a",,,"5 Discussion  
This chapter discusses the results of the thesis. Since Experiments 1 and 2 were presented 
separately, they are also discussed in dedicated sections ( 5.1 and 5.2). Section 5.3 then 
compares the outcomes of both experiments. Lastly, section 5.4 directly evaluates the 
applicability of SLDV for FBS  based on the experimental findings . As one of the main goals 
of the thesis, this section offers important insights into using SLDV in FBS, aimed at supporting 
future research in the field. 
5.1 Discussion of Experiment 1 (Simulated Numerical Experiment) 
The numerical analysis indicated 8 modeshapes  on the assembled AM structure with 
distinguishable movement in the Z -direction in the frequency range 0 to 2 kHz. While the 
structureâ€™s cross section resembles a beam â€“ rectangular on A and a mix of rectangular and U-
shaped on B â€“ its overall geometry is quite complex. As a result, many mode shapes become 
rather intricate and often cannot be described using simple deformation modes.  
Nevertheless, some individual modes still exhibit beam-like deformation characteristics  such 
as pure bending or torsion in certain sections (see Figure 14): bending occurs twice in mode 1 
(both on A and B), and once in mode 2 (on A). Additionally, mode 7 displays significant torsion 
on B. Besides modes 2 and 7, mode 6 is one of the few where mainly one part of the structure 
moves while the other remains still.  However, unlike modes 2 and 7, mode 6 involves a more 
complex combination of bending and torsion on A. The remaining mode shapes affect the entire 
AM Structure. Mode 3 primarily exhibits bending with a uniform curvature throughout, while 
modes 4 and 5 have a complex combination of  bending and torsion.  
An observation regarding frequency: as it increases, the mode shapes become increasingly 
complex in their deformation patterns. For instance, in mode 1, part A experiences simple 
bending with maximum deflection at the end, while in mode 8, the same region shows peak 
deformation near the center, creating a bending node close to the branch at the armâ€™s end. This 
trend continues in higher modes  (not shown), where rising frequency leads to more intricate 
vibration patterns and multiple bending and torsional nodes along the structureâ€™s arms. 
The presence of eight  resonance frequencies â€“ each exciting modes with motion in the Z 
direction â€“ can also be identified in Figure 15, which displays the overlaid FRFs for all available 
force-response DoF combinations (LM-FBS coupled vs. reference). Since natural frequencies 

 53 
 
are inherent structure properties, the resonance peaks in the reference magnitude of Figure 15 
appear at the se natural frequencies (shown in Figure 14), regardless of the specific force -
response combination31. What varies between combinations is not the frequency of the peaks, 
but their amplitude, which depends on how strongly each mode is excited and observed at the 
respective DoF. 
Unlike the magnitudes, several phase curves do exhibit a variation in frequency in the reference 
plot of Figure 15. Before explaining this difference, some background on resonances and 
antiresonances is necessary. Antiresonances appear as dips or minima in the magnitude and 
correspond to the zeros of the FRF [27], arising from destructive interference between vibration 
modes [17]. In contrast to resonances, antiresonances are not inherent structural properties [27]; 
their frequency locations can vary with the selected force -response combination [28]. At 
antiresonances, the phase typically undergoes a shift of Ï€, while at resonances it shifts by 
approximately -Ï€ [27] [29]32.  
Returning to the question of why the overlaid phase plots differ in frequency across force-
response combinations: the difference cannot be linked to the occurrence of resonances in the 
magnitudes, since their (natural) frequencies are the same across all combinations. The variation 
must therefore be linked to the antiresonances, with two main factors being responsible. First, 
the number of antiresonances can vary between FRFs depending on the specific force-response 
pairing [27]: some FRFs may exhibit an antiresonance (and thus a phase shift of Ï€) near a certain 
(antiresonance) frequency, while others do not. Such a variation means that the phases of some 
combinations will shift at a certain frequency , while the phases of other combinations wonâ€™t. 
Second, even if an antiresonance is present near a such an (antiresonance) frequency, the exact 
frequency of it may differ between FRFs, as antiresonances are not invariant structural 
properties [28]. In turn, variations in (antiresonance) frequencies in the magnitude result  in 
variations of where the phase shift of Ï€ takes place between different phase curves. 
With the overlaid reference plot of Figure 15 now discussed, we proceed to the comparison 
between the reference and LM -FBS simulations. The overlaid reference and LM -FBS FRFs 
reveal no significant visual difference. However, for a more meaningful comparison, the FRFs 
 
31 With small damping, the difference between damped (resonance) frequencies and undamped natural frequencies 
is insignificant [26] (damping ratio Î¶ = 0.3 % assumed for FRF synthetization in pyFBS). 
32 The extent of these phase shifts may vary in figures due to the influence of damping and specified frequency 
resolution. 

 54 
 
of individual force-response combinations (such as those in Figure 16 and Figure 17) should be 
compared directly. 
First, some general observations from Figure 16 and Figure 17: The previously discussed phase 
shifts of Â±Ï€ at resonances and antiresonances are clearly visible in all plots, with the transitions 
smoothed due to the light damping introduced during the FRF synthetization. A characteristic 
feature of driving-point FRFs is that each antiresonance occurs between a pair of resonances 
[27]. This pattern is evident in Figure 16 (driving-point FRFs), but not consistently in Figure 
17 (non-driving-point FRFs) , where multiple resonances or antiresonances appear 
consecutively without the alteration observed for driving-points.  
Three out of the four plots in Figure 16 and Figure 17 display all eight resonance peaks, 
corresponding to the eight mode shapes presented in Figure 14. However, the first driving-point 
FRF shown (the left plot in Figure 16) exhibits only four visible resonance peaks, indicating a 
potential outlier. This suspicion is confirmed upon revisiting the overlaid FRFs in Figure 15: 
while most curves clearly show all eight resonance peaks, the first driving-point FRF stands out 
as it remains nearly horizontal across most of the  frequency range. The reason behind this can 
be explained by looking at the position of the driving point (force DoF 1, response DoF 1; see 
DoF naming in Figure 12) throughout the mode shapes in Figure 14: At 216 (turquoise; 1 st 
mode) and 294 Hz (green, 2 nd mode), the colors indicate rather high vibration amplitudes. At 
408 (light blue; 3 rd mode) and 918 Hz (light blue, 5 th mode), the colors indicate moderate  to 
low vibration amplitudes. These observations are reflected in the FRF magnitude of Figure 16 
(left plot), which displays two larger and two smaller resonance peaks.  However, for all other 
modes (4, 6, 7, and 8), the driving point remains in the dark blue color range, indicating very 
low vibration amplitudes. Low vibration amplitudes correspond to low response levels ; and 
since an FRF is the ratio of response to force, no resonance peaks are visible for these modes. 
Figure 16 and Figure 17 demonstrate a strong visual agreement between LM-FBS coupled and 
corresponding reference curves, indicating successful coupling. This is supported by the high 
average coherence values, which are all above 0.9. However, noticeable discrepancies appear 
at two specific frequency regions, where mismatches are observed in both frequency and 
amplitude. Since these discrepancies consistently occur across all four plots, they appear to be 
independent of the specific force-response DoF combinations. This points to the coupling as a 
possible cause of the differences.  

 55 
 
The first, main discrepancy occurs near 900 Hz, exhibiting an approximate 40 Hz frequency 
shift. The observed shift is likely linked to the VPT, whose validity may be compromised when 
the interface experiences high loading. Such high loading at the interface occurs at 918 Hz, as 
deduced from the 5th mode shape shown in Figure 14: although the interface region itself does 
not vibrate significantly (indicated by the dark blue color), it is precisely this region that 
accommodates the opposing nature of movement of A and B. 
At the 5th mode shapeâ€™s snapshot in Figure 14, the two arms of A (near the interface) and the 
arm of B (near the interface) have inertia in opposite Z -directions. Unlike other modes, where 
the regions on either side of the interface often move in an aligned manner, mode 5 is different. 
Parts A and B exhibit movement along opposing paths and the interface region acts as the 
separation boundary between these conflicting motion paths.  This results in heavy loading of 
the interface, which is the presumable cause of the frequency shift around 900 Hz. Two 
underlying effects can explain the shift: First, the heavy interface loading causes increased local 
deformation at the interface, likely invalidating the VPT rigidity assumption near this 
frequency. Second, the complex torsion and bending in the 5th mode may cause shifts in X and 
Y and rotation about Z that the 1D VPT cannot capture. 
The second, minor discrepancy in the plots of Figure 16 and Figure 17 occurs near 400 Hz. This 
frequency is associated with the 3 rd mode, which primarily shows bending with a uniform 
curvature throughout  the structure . In bending, one side of the neutral plane undergoes 
compression while the opposite side experiences elongation. At the interface, this can introduce 
flexibility since the adjacent regions of parts A and B are not constrained in the X and Y 
translational di rections. As a result, slight relative movement may occur at the interface, 
potentially contributing to the observed discrepancy.  
 
 
 
 
 
 

 56 
 
5.2 Discussion of Experiment 2 (SLDV Experiment) 
It is obvious that  conditions in physical laboratory experiments are less controlled than in 
numerical simulations. Quantifying the impact of this variability is essential for a scientific 
evaluation of the final results. This was done using the measurement signal coherence shown 
in Figure 31, which provides  a measure of how consistent and repeatable the SLDV 
measurements are when testing the same sample in the same position33.  
At first glance, the plots show a lot of inconsistency in the coherence curve s. Overall, all the 
plots display very low coherence below 600 Hz, followed by a sharp increase  at around 600 
Hz, where coherence remains high up to 2000 Hz.  Interestingly, below 600 Hz, the coherence 
spikes three times â€“ corresponding to the first three resonance frequencies. This behavior 
suggests that at these resonance points, the system response is strong and linear enough to 
produce clear, repeatable measurements, resulting i n high coherence. Such a pattern  is in 
contrast with common observations where coherence often drops at resonances due to nonlinear 
effects, noise, or modal coupling [26]. Additionally, coherence may decrease at antiresonances 
because the response is very low, causing the sensor to pick up mostly noise [26] â€“ the drops in 
coherence at higher frequencies are therefore likely linked to resonances and antiresonances.  
While the coherence curves vary visually and quantitatively (Ã˜ values 0.627 - 0.717) across the 
four plots in Figure 31, meaningful interpretation of these differences is limited, as each plot 
shows averaged coherence over all response DoFs for each excitation DoF.  
Overall, Figure 31 shows low measurement repeatability below 600 Hz, along with the presence 
of noise throughout the signal  â€“ likely related to the limited performance of the piezoelectric 
actuator. At lower frequencies, the actuator fails to excite with the same magnitude as at higher 
frequencies (a constant excitation magnitude is intended, as the power amplification stays 
constant). Since the actuator lacks a load cell (unlike a modal hammer), this magnitude 
discrepancy cannot be compensated for.  Consequently, because coherence is low below 600 
Hz, the FRFs in that range are not reliable. 
Figure 32 shows eight experimentally estimated mode shapes within the 0 to 2 kHz frequency 
range. While they appear flat (the SLDV measures only the surface layer), they exhibit the same 
 
33 Note that the plots of Figure 31 represent averaged coherences; see section Error! Reference source not found. 
for clarification.  

 57 
 
vibration patterns as the numerical mode shapes presented in section 3.2.134. Each mode shape 
corresponds to an estimated damped natural frequency and damping ratio. The damping ratios 
range from 0.0121  % to 0.439  %, confirming the initial assumption of very low damping in 
aluminum. Regarding the variation in the damping ratios: a higher ratio means vibration in that 
mode decays faster with broader FRF peaks; a lower ratio means it lasts longer and the peaks 
appear sharper.  
Such a characteristic is evident in the overlaid FRF reference plot of Figure 33, where the 6th 
resonance frequency â€“ which corresponds to by far the highest damping ratio in Figure 32 
(0.439 %) â€“ displays noticeably rounder resonance peaks compared to the other modes. 
Another observation regarding the overlaid reference FRFs is that (unlike the estimated modal 
properties), some experimental curves exhibit an additional 9th resonance peak near 1650 Hz. 
This extra peak appears mainly for force-response combinations involving D oF 1, which is 
located near a free end of subsystem A. One possible explanation is that the bolt connection 
introduces interface flexibility (i.e., it is not perfectly rigid), leading to mode localization, where 
deformation is concentrated in a small region  of the structure  [30]. For modal propert ies 
estimation, a dedicated test run (see section 4.1.4) was conducted with excitation applied only 
at DoF 2. Because DoF 1 was not excited, the localized mode there may have went undetected 
by MEâ€™Scope, resulting in only eight identified mode shapes. 
The overlaid LM-FBS plot of Figure 33 shows significantly more noise than the reference, 
especially for frequencies below 600 Hz. This is expected, as Figure 31 shows poor force-
response correlation below 600 Hz35. In general, while multiple common resonance peaks are 
present, not all of them are clearly visible and distinguishable when they are overlaid . The 
observation is likely linked to noise amplification during the VPT and LM -FBS, as the matrix 
inversions involved tend to amplify noise introduced by the piezoelectric actuator. Additionally, 
inherent noise in the measurements contributes to this effect.  
Compared to the reference, the curves above 1250 Hz appear considerably damped and 
inconsistent â€“ likely caused by noise effects and the VPT (limited DoF coupling; unfulfilled 
rigidity assumption). For a more meaningful comparison of the coupled and reference FRF 
 
34 For a detailed analysis of the vibration patterns, refer to the numerical discussion  in section 5.1. 
35 Although Figure 31 only presents coherence for AB (not for A and B separately), it reveals a clear trend of low 
measurement repeatability at low frequencies. 

 58 
 
results, individual D oF combinations  (such as those in Figure 34 and Figure 35) should be 
directly compared. 
Figure 34 and Figure 35 demonstrate sound agreement between the LM -FBS coupled and 
corresponding reference curves, supported by average coherence values ranging from 0.738 to 
0.932. In all plots, the reference exhibits less noise than the coupled FRF, which shows 
numerous smaller (and some larger ) spikes between the actual resonance peaks, causing the 
coherence to drop at those frequencies. Revisiting the observation from the previous paragraph, 
coupled FRFs above 1250 Hz appear significantly damped and inconsistent . In contrast, the 
reference FRFs show a single, well-defined peak around 1440 Hz36, causing a drop in coherence 
above 1250 Hz in the respective plots. Two reasons might contribute to this deviation, the first 
one being the influence of the localized 9th mode around 1650 Hz for some DoF combinations 
(see discussion of Figure 33). Secondly, in the mode shape at 1440 Hz, the structureâ€™s interface 
experiences large vibration amplitudes. This may influence the accuracy of the one-dimensional 
VPT, which allows relative displacements in X and Y and rotations around Z. 
Near 900â€¯Hz, the LM -FBS curves in the overlaid FRF plot exhibit two closely spaced peaks 
instead of a single resonance peak. Figure 34 and Figure 35 further reveal a frequency shift 
between the LM -FBS and reference curves in this region.  This behavior occurs near the 5 th 
mode, which has a complex vibration pattern, with the two arms of A and the arm of B having 
inertia in opposite Z -directions near the interface (see Figure 32). The interface region forms 
the boundary between these opposing motions, resulting in significant interface loading, which 
may explain the irregularit ies observed near 900â€¯Hz . As this behavior was also observed in 
Experiment 1, a detailed analysis is provided in 5.1. 
 
 
 
 
 
36 Not relevant for the DoF 1 driving-point FRF outlier (left plot in Figure 34; see section 5.1 for discussion). 

 59 
 
5.3 Experiment 1 vs. Experiment 2  
The findings from Experiments 1 and 2 are compared in this section. Although the goal of the 
thesis is not to perfectly match the numerical and experimental results , a comparison helps to 
identify experimental limitations and validate the numerical model.  
Starting with the modal properties, the mode shapes in Figure 14 and Figure 32 show consistent 
vibration patterns across the frequency range, indicating that the rubber band suspension in 
Experiment 2 successfully approximated free -free boundary conditions.  The natural 
frequencies indicate good overall agreement as well: e xcept for the third, fifth, and eighth 
natural frequencies (showing differences of 19, 55 and 101 Hz , respectively ), all other 
frequencies differ by less than 7 Hz. Although Figure 14 shows undamped and Figure 32 shows 
damped natural frequencies, good agreement is expected due to the low damping in Experiment 
237. The discrepancies in the third, fifth, and eighth modes are likely linked to the interface 
connection. In Experiment 2, the reference consists of two parts connected by a physical bolt 
(introducing added mass and flexibility), whereas in Experiment 1 it is modeled as a single part. 
Since the  mentioned modes involve increased vibration or loading at the interface, the 
difference in connection likely contributes to the observed deviations. 
Figure 36 presents the overlaid FRFs of Experiments 1 and 2 for one selected DoF combination 
(force DoF 1 and response DoF 4), serving as a representative example of the effects discussed. 
Overall, the resonance peaks are visible for most of the curves, with their peaks agreeing in 
frequency. Below 600 Hz, the curves from Experiment 2 show a noticeable decrease in 
magnitude compared to those from Experiment 1. This difference is expected, as poor 
measurement signal coherence s (shown in Figure 31) in this frequency range were already 
observed and discussed i n section 5.2. Nevertheless, the resonance peaks of the curves below 
600 Hz are largely aligned in frequency. 
For modes 4, 6 and 7, the natural frequencies agree well, although the magnitudes of 
Experiment 2 are much higher for mode 7. A possible explanation for the difference is  that 
mode 7 has a very low damping ratio (Î¶â‚‡ = 0.03 %), while the numerical model assumes a 
uniform damping of 0.3 %. Since higher damping tends to round resonance peaks, this 
difference in damping may contribute to the observed discrepancy.  Regarding mode 5, a  
 
37 Note the classical relationship between damped (fd) and undamped (fn) natural frequencies: fd = fn (1 - Î¶2)0.5. 

 60 
 
variation in natural frequency can be observed between the FRFs. As discussed in sections 5.1 
and 5.2, this variation is linked to heavy interface loading caused by the distinct vibration 
pattern of mode 5. 
Although the shape of the 8th  resonance peak is similar between Experiment 1 and the reference 
of Experiment 2, there is a significant frequency shift. This is likely because the reference in 
Experiment 2 is connected with a physical bolt assembly, which adds mass and introduces 
residual interface flexibility  (unlike the reference in Experiment 1 ). Since the interface 
experiences high vibration amplitudes in the 8 th mode, this effect becomes significant.  For 
further information regarding this characteristi c, refer to section 5.2, where it is discussed in 
detail. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 36: Overlaid FRFs of Experiment 1 (num) and Experiment 2 (exp) 
for force DoF 2 and response DoF 4. 

 61 
 
5.4 Applicability of SLDV in FBS 
The experiments conducted in this thesis successfully demonstrated the feasibility of using 
SLDV for response measurement in FBS. Its advantages over conventional techniques  â€“
particularly in the FBS context â€“ will be detailed in section 5.4.1. 
The study also encountered several challenges, both due to the SLDVâ€™s fundamentally different 
characteristics compared to traditional accelerometers  and the influence of the excitation 
method (since FRFs inherently depend on both response  and excitation). In Experiment 2, for 
example, the use of a piezoelectric actuator â€“ while providing certain advantages â€“ contributed 
to the low measurement signal coherence, especially at lower frequencies, and thereby affected 
the quality of the final results. These and other experimental limitations are discussed in detail 
in section 5.4.2. 
5.4.1 Strengths of the Experimental Framework  for Experiment 2  
One big advantage of the  Experiment 2  framework is that it allows for  measuring many 
response DoFs at once with in a single SLDV scan. In practice, this means that an arbitrary 
number of points  on the structureâ€™s surface facing the SLDV can  be recorded for every 
excitation position.  These points can then serve as internal or interface response DoFs.  
Without the SLDVâ€™s scanning feature , gathering FRFs for 16 different forceâ€response 
combinations would have taken significantly longer. As the SLDV moved from one response 
DoF to another, it averaged 50 samples for each DoF  combination â€“ equivalent to 50 separate 
modal hammer impacts (+ accelerometer measurements) for just that single response point. The 
main time-consuming step was re-mounting the piezoelectric actuator between tests. Once the 
actuator was in place , the rest of the experimental execution (steps 1 and 2 in Figure 27) 
proceeded quickly since the actuator provided continuous excitation throughout each scan . 
Another benefit of the actuator was its small size , allowing for quick  repositioning between 
tests. In contrast, larger continuous excitation methods like shakers are bulkier and more 
difficult to move. 
Since the VPT requires multiple response DoFs in proximity to the coupling interface, space 
constraints may become an issue with conventional sensors such as accelerometers. No such 
limitations are encountered with the SLDV equipment  since the PSV Acquisition software 
allows for setting a dense grid of scan points. Note that an even denser interface grid was tested 

 62 
 
in the numerical study (16 interface responses instead of 4), but it was discarded as it yielded 
virtually the same numerical coupling results as the configuration with 4 interface responses. 
This is mentioned to clarify that such a denser setup would have also been easily possible with 
the SLDV. In fact, a fairly dense grid (see Figure 29 â€“ which could be made even denser) was 
used to estimate modal properties with MEâ€™Scope. This allowed for a clear and detailed 2D 
visualization of the mode shapes.  Such fast and detailed mode shape estimation in two 
dimensions (along with clear visualization) is difficult to achieve with conventional tools and 
further demonstrates the practical strengths of the SLDV system. 
Another benefit of using the SLDV equipment is its straightforward operation â€“ provided that 
good user instructions are available (such as in [26]), the SLDV and its software s are easy to 
use. When paired with a piezoelectric actuator, common vibration testing  issues like â€œdouble 
hitsâ€, which can occur when using a  modal hammer for excitation, are completely avoided. 
Overall, the SLDV system offers a user-friendly and reliable setup. 
The 1D VPT performed better than expected with the chosen AM structure. Not coupling all 
DoFs did not significantly affect the FRFs along the Z-axis, however, this may not be generally 
valid. In this particular case, t he geometry of the AM structure, combined with the specific 
choice of which DoFs were coupled and which were not, happened to be well-suited. 
5.4.2 Limitations of the Experimental Framework for Experiment 2  
One limitation of Experiment 2 is the rather poor measurement signal coherence, which gives 
insight into how well the piezoelectric actuator and the SLDV response measurements are 
linearly related. Below around 600 Hz, the coherence is very low, so the FRFs (both coupled 
and reference) do not carry meaningful information in that range. In other words, at those low 
frequencies, only a small fraction of the measured response actually comes from the actuator. 
Another problem is measurement noise, which affects higher frequencies  as well . For the 
reference, this noise can be filtered out easily â€“ so the residual noise left in the reference FRFs 
is barely noticeable. However, for the coupled setup, the same filtering does not fully solve the 
problem. Due to the matrix inversions used in the VPT and LM -FBS methods, the residual 
noise left after filtering gets amplified. That is why the coupled FRFs often look irregular or 
â€œnoisyâ€ compared to the smoother reference FRFs.","6 Conclusion  
The conclusion section summarizes the main findings from the previous sections. Additionally, 
recommendations for future work are provided. 
6.1 Summary of Main Findings 
As a first attempt at using SLDV for response measurement in FBS, this study yielded 
promising results. Experiment 1 demonstrated that FBS can perform well when only three DoFs 
are coupled after being reduced into the VP using a one-dimensional VPT. Consequently, the 
coherence between the numerically coupled system and the reference was very high (> 0.9) 
across all examined force -response combinations.  Nevertheless, FBS introduced small 
frequency shifts, likely due to high interface loading  in certain modes  (which may affect the 
FBS rigidity assumption) and relative motion resulting from not coupling all DoFs. 
The geometry of the AM structure and the choice of which DoFs to couple  (especially the 
direction) are expected to influence the quality of the 1D VPT results. A 1D VPT allows relative 
interface motion in the DoFs that are not coupled, which can affect the validity of the FBS 
coupling results . In this study, the geometry of the AM Structure naturally lent itself to 
investigation in the Z -direction. As a result, translation in Z was coupled directly, while 
rotations in X and Y were implicitly included due to the planar distribution of  interface 
responses.  
Since the number, placement, and orientation of DoFs were carefully defined in Experiment 1 
with regard to Experiment 2, Experiment 2 proceeded without major practical issues. In 
particular, placing the force and response DoFs on opposite sides of the AM s tructureâ€™s main 
flat surface greatly facilitated the experimental procedure. 
The scanning feature of the SLDV allowed for the generation of FRFs with many force -
response combinations. Additionally, it enabled the creation of fine -grid mode shapes, which 
would have required significantly more time using conventional response measurement 
equipment. Overall, the coupling results (although not as good as numerical), were decent with 
coherence between the experimentally coupled system and the reference above 0.7 across all 
examined force-response combinations."
